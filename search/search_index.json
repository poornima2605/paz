{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>PAZ is a hierarchical perception library in Python.</p>"},{"location":"#selected-examples","title":"Selected examples:","text":"<p>PAZ is used in the following examples (links to real-time demos and training scripts):</p> Probabilistic 2D keypoints 6D head-pose estimation Object detection Emotion classifier 2D keypoint estimation Mask-RCNN (in-progress) 3D keypoint discovery Haar Cascade detector 6D pose estimation Implicit orientation Attention (STNs) <p>All models can be re-trained with your own data (except for Mask-RCNN, we are working on it here).</p>"},{"location":"#hierarchical-apis","title":"Hierarchical APIs","text":"<p>PAZ can be used with three diferent API levels which are there to be helpful for the user's specific application.</p>"},{"location":"#high-level","title":"High-level","text":"<p>Easy out-of-the-box prediction. For example, for detecting objects we can call the following pipeline:</p> <pre><code>from paz.pipelines import SSD512COCO\n\ndetect = SSD512COCO()\n\n# apply directly to an image (numpy-array)\ninferences = detect(image)\n</code></pre> <p>There are multiple high-level functions a.k.a. <code>pipelines</code> already implemented in PAZ here. Those functions are build using our mid-level API described now below.</p> <p> </p>"},{"location":"#mid-level","title":"Mid-level","text":"<p>While the high-level API is useful for quick applications, it might not be flexible enough for your specific purporse. Therefore, in PAZ we can build high-level functions using our a mid-level API.</p>"},{"location":"#mid-level-sequential","title":"Mid-level: Sequential","text":"<p>If your function is sequential you can construct a sequential function using <code>SequentialProcessor</code>. In the example below we create a data-augmentation pipeline:</p> <pre><code>from paz.abstract import SequentialProcessor\nfrom paz import processors as pr\n\naugment = SequentialProcessor()\naugment.add(pr.RandomContrast())\naugment.add(pr.RandomBrightness())\naugment.add(pr.RandomSaturation())\naugment.add(pr.RandomHue())\n\n# you can now use this now as a normal function\nimage = augment(image)\n</code></pre> <p> </p> <p>You can also add any function not only those found in <code>processors</code>. For example we can pass a numpy function to our original data-augmentation pipeline:</p> <pre><code>augment.add(np.mean)\n</code></pre> <p>There are multiple functions a.k.a. <code>Processors</code> already implemented in PAZ here.</p> <p>Using these processors we can build more complex pipelines e.g. data augmentation for object detection: <code>pr.AugmentDetection</code></p> <p> </p>"},{"location":"#mid-level-explicit","title":"Mid-level: Explicit","text":"<p>Non-sequential pipelines can be also build by abstracting <code>Processor</code>. In the example below we build a emotion classifier from scratch using our high-level and mid-level functions.</p> <pre><code>from paz.applications import HaarCascadeFrontalFace, MiniXceptionFER\nimport paz.processors as pr\n\nclass EmotionDetector(pr.Processor):\n    def __init__(self):\n        super(EmotionDetector, self).__init__()\n        self.detect = HaarCascadeFrontalFace(draw=False)\n        self.crop = pr.CropBoxes2D()\n        self.classify = MiniXceptionFER()\n        self.draw = pr.DrawBoxes2D(self.classify.class_names)\n\n    def call(self, image):\n        boxes2D = self.detect(image)['boxes2D']\n        cropped_images = self.crop(image, boxes2D)\n        for cropped_image, box2D in zip(cropped_images, boxes2D):\n            box2D.class_name = self.classify(cropped_image)['class_name']\n        return self.draw(image, boxes2D)\n\ndetect = EmotionDetector()\n# you can now apply it to an image (numpy array)\npredictions = detect(image)\n</code></pre> <p> </p> <p><code>Processors</code> allow us to easily compose, compress and extract away parameters of functions. However, most processors are build using our low-level API (backend) shown next.</p>"},{"location":"#low-level","title":"Low-level","text":"<p>Mid-level processors are mostly built from small backend functions found in: boxes, cameras, images, keypoints and quaternions.</p> <p>These functions can found in <code>paz.backend</code>:</p> <pre><code>from paz.backend import boxes, camera, image, keypoints, quaternion\n</code></pre> <p>For example, you can use them in your scripts to load or show images:</p> <pre><code>from paz.backend.image import load_image, show_image\n\nimage = load_image('my_image.png')\nshow_image(image)\n</code></pre>"},{"location":"#additional-functionality","title":"Additional functionality","text":"<ul> <li> <p>PAZ has built-in messages e.g. <code>Pose6D</code> for an easier data exchange with other libraries or frameworks such as ROS.</p> </li> <li> <p>There are custom callbacks e.g. MAP evaluation for object detectors while training</p> </li> <li> <p>PAZ comes with data loaders for the multiple datasets:     OpenImages, VOC, YCB-Video, FAT, FERPlus, FER2013.</p> </li> <li> <p>We have an automatic batch creation and dispatching wrappers for an easy connection between you <code>pipelines</code> and tensorflow generators. Please look at the examples and the processor <code>pr.SequenceWrapper</code> for more information.</p> </li> </ul>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#pull-requests","title":"Pull requests","text":"<ol> <li> <p>Always use full english names for variable names</p> <ul> <li>Only the following exceptions are allowed:<ul> <li>\"number\" should be \"num\"</li> <li>\"argument\" should be \"arg\"</li> <li>\"width\" can be \"W\" if the context is clear.</li> <li>\"height\" can be \"H\" if the context is clear.</li> </ul> </li> </ul> </li> <li> <p>Functions should be small, approximately ~6 lines:</p> <ul> <li>Functions should only do one thing.</li> <li>Certain aspects of the code base don't reflect this but we are working on changing this.</li> </ul> </li> <li> <p>Use PEP8 syntax conventions:</p> <ul> <li>This can be easily achieved when you install a linter e.g. flake8</li> </ul> </li> <li> <p>If new functionality is added please include unit-tests for it.</p> </li> <li> <p>Please make sure that all unit-tests are passing before your make your PR.</p> </li> <li> <p>Commits should try to have the following structure:</p> <ul> <li>Commits are titles:<ul> <li>Start with a capital letter</li> <li>Don't end the commit with a period</li> </ul> </li> <li>Commits should be written to answer:     If applied, this commit will      A good commit would then look like:         \"Remove deprecated backend function\" <li>Find more information about how to write good commits here.</li> <li> <p>Provide documentation of new features:</p> <ul> <li>Use the documentation syntax of the repository</li> <li>If new functionality is added please add your function to paz/docs/structure.py</li> </ul> </li> <li> <p>After looking into the points here discussed, please submit your PR such that we can start a discussion about it and check that all tests are passing.</p> </li>"},{"location":"datasets/","title":"Datasets","text":"<p>[source]</p>"},{"location":"datasets/#voc","title":"VOC","text":"<pre><code>paz.datasets.voc.VOC(path=None, split='train', class_names='all', name='VOC2007', with_difficult_samples=True, evaluate=False)\n</code></pre> <p>Dataset loader for the falling things dataset (FAT).</p> <p>Arguments</p> <ul> <li>data_path: Data path to VOC2007 annotations</li> <li>split: String determining the data split to load.     e.g. <code>train</code>, <code>val</code> or <code>test</code></li> <li>class_names: <code>all</code> or list. If list it should contain as elements     strings indicating each class name.</li> <li>name: String or list indicating with dataset or datasets to load.     e.g. <code>VOC2007</code> or <code>[''VOC2007'', VOC2012]</code>.</li> <li>with_difficult_samples: Boolean. If <code>True</code> flagged difficult boxes     will be added to the returned data.</li> <li>evaluate: Boolean. If <code>True</code> returned data will be loaded without     normalization for a direct evaluation.</li> </ul> <p>Return</p> <ul> <li>data: List of dictionaries with keys corresponding to the image paths</li> </ul> <p>and values numpy arrays of shape <code>[num_objects, 4 + 1]</code> where the <code>+ 1</code> contains the <code>class_arg</code> and <code>num_objects</code> refers to the amount of boxes in the image.</p> <p>[source]</p>"},{"location":"datasets/#fat","title":"FAT","text":"<pre><code>paz.datasets.fat.FAT(path, split='train', class_names='all')\n</code></pre> <p>Dataset loader for the falling things dataset (FAT).</p> <p>Arguments</p> <ul> <li>path: String indicating full path to dataset     e.g. /home/user/fat/</li> <li>split: String determining the data split to load.     e.g. <code>train</code>, <code>val</code> or <code>test</code></li> <li>class_names: <code>all</code> or list. If list it should contain as elements     strings indicating each class name.</li> </ul> <p>References</p> <ul> <li>Deep Object Pose     Estimation (DOPE)</li> </ul> <p>[source]</p>"},{"location":"datasets/#fer","title":"FER","text":"<pre><code>paz.datasets.fer.FER(path, split='train', class_names='all', image_size=(48, 48))\n</code></pre> <p>Class for loading FER2013 emotion classification dataset. Arguments</p> <ul> <li>path: String. Full path to fer2013.csv file.</li> <li>split: String. Valid option contain 'train', 'val' or 'test'.</li> <li>class_names: String or list: If 'all' then it loads all default     class names.</li> <li>image_size: List of length two. Indicates the shape in which     the image will be resized.</li> </ul> <p>References</p> <p>-FER2013 Dataset and Challenge</p> <p>[source]</p>"},{"location":"datasets/#ferplus","title":"FERPlus","text":"<pre><code>paz.datasets.ferplus.FERPlus(path, split='train', class_names='all', image_size=(48, 48))\n</code></pre> <p>Class for loading FER2013 emotion classification dataset. with FERPlus labels. Arguments</p> <ul> <li>path: String. Path to directory that has inside the files:     <code>fer2013.csv</code> and  <code>fer2013new.csv</code></li> <li>split: String. Valid option contain 'train', 'val' or 'test'.</li> <li>class_names: String or list: If 'all' then it loads all default     class names.</li> <li>image_size: List of length two. Indicates the shape in which     the image will be resized.</li> </ul> <p>References</p> <ul> <li>FerPlus</li> <li>FER2013</li> </ul> <p>[source]</p>"},{"location":"datasets/#openimages","title":"OpenImages","text":"<pre><code>paz.datasets.open_images.OpenImages(path, split='train', class_names='all')\n</code></pre> <p>Dataset loader for the OpenImagesV4 dataset.</p> <p>Arguments</p> <ul> <li>path: String indicating full path to dataset     e.g. /home/user/open_images/</li> <li>split: String determining the data split to load.     e.g. <code>train</code>, <code>val</code> or <code>test</code></li> <li>class_names: <code>all</code> or list. If list it should contain as elements     the strings of the class names.</li> </ul> <p>[source]</p>"},{"location":"datasets/#cityscapes","title":"CityScapes","text":"<pre><code>paz.datasets.cityscapes.CityScapes(image_path, label_path, split, class_names='all')\n</code></pre> <p>CityScapes data manager for loading the paths of the RGB and segmentation masks.</p> <p>Arguments</p> <ul> <li>image_path: String. Path to RGB images e.g. '/home/user/leftImg8bit/'</li> <li>label_path: String. Path to label masks e.g. '/home/user/gtFine/'</li> <li>split: String. Valid option contain 'train', 'val' or 'test'.</li> <li>class_names: String or list: If 'all' then it loads all default     class names.</li> </ul> <p>References</p> <p>-The Cityscapes Dataset for Semantic Urban Scene Understanding</p> <p>[source]</p>"},{"location":"datasets/#shapes","title":"Shapes","text":"<pre><code>paz.datasets.shapes.Shapes(num_samples, image_size, split='train', class_names='all', iou_thresh=0.3, max_num_shapes=3)\n</code></pre> <p>Loader for shapes synthetic dataset.</p> <p>Arguments</p> <ul> <li>num_samples: Int indicating number of samples to load.</li> <li>image_size: (height, width) of input image to load.</li> <li>split: String determining the data split to load.     e.g. <code>train</code>, <code>val</code> or <code>test</code></li> <li>class_names: List of strings or <code>all</code>.</li> <li>iou_thresh: Float intersection over union.</li> <li>max_num_shapes: Int. maximum number of shapes in the image.</li> </ul> <p>Returns</p> <p>List of dictionaries with keys <code>image</code>, <code>mask</code>, <code>box_data</code>     containing</p> <p>[source]</p>"},{"location":"datasets/#omniglot","title":"Omniglot","text":"<pre><code>paz.datasets.omniglot.Omniglot(split, shape, flat=True)\n</code></pre> <p>Loads omniglot dataset for in between and within alphabet sampling.</p> <p>Arguments</p> <ul> <li>split: String. Either <code>train</code> or <code>test</code>. Indicates which split to load.</li> <li>shape: List of two integers indicating resize shape <code>(H, W)</code>.</li> <li>flat: Boolean. If <code>True</code> the returned data dictionary is organized     using each possible character as a class, with each key being a     number having as value an image array.     If <code>False</code> the returned data dictionary is organized using as keys     the language names and as value another dictionary with keys being     the character number, and as value the image array.     This is to perform either sampling between alpahabet (<code>flat=True</code>)     or to perform sampling within alphabet (<code>flat=False</code>).     Usually, neural few-shot learning algorithms have been tested using     in between alphabet sampling, but the original authors tested using     the more challenging within alphabet sampling.</li> </ul> <p>Returns</p> <p>dictionary with class names as keys and image numpy arrays as values.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation","title":"Installation","text":"<p>PAZ has only three dependencies: Tensorflow2.0, OpenCV and NumPy.</p> <p>To install PAZ with pypi run:</p> <p><code>pip install pypaz --user</code></p>"},{"location":"abstract/loader/","title":"Loader","text":"<p>[source]</p>"},{"location":"abstract/loader/#loader-class","title":"Loader class","text":"<pre><code>paz.abstract.loader.Loader(path, split, class_names, name)\n</code></pre> <p>Abstract class for loading a dataset.</p> <p>Arguments</p> <ul> <li>path: String. Path to data.</li> <li>split: String. Dataset split e.g. traing, val, test.</li> <li>class_names: List of strings. Label names of the classes.</li> <li>name: String. Dataset name.</li> </ul> <p>Properties</p> <ul> <li>name: Str.</li> <li>path: Str.</li> <li>split: Str or Flag.</li> <li>class_names: List of strings.</li> <li>num_classes: Int.</li> </ul> <p>Methods</p> <p>load_data()</p>"},{"location":"abstract/loader/#loader-methods","title":"Loader methods","text":"<p>[source]</p>"},{"location":"abstract/loader/#load_data","title":"load_data","text":"<pre><code>load_data()\n</code></pre> <p>Abstract method for loading dataset.</p> <p>Returns</p> <p>dictionary containing absolute image paths as keys, and ground truth vectors as values.</p>"},{"location":"abstract/messages/","title":"Messages","text":"<p>[source]</p>"},{"location":"abstract/messages/#box2d-class","title":"Box2D class","text":"<pre><code>paz.abstract.messages.Box2D(coordinates, score, class_name=None)\n</code></pre> <p>Bounding box 2D coordinates with class label and score.</p> <p>Properties</p> <ul> <li>coordinates: List of float/integers indicating the     <code>[x_min, y_min, x_max, y_max]</code> coordinates.</li> <li>score: Float. Indicates the score of label associated to the box.</li> <li>class_name: String indicating the class label name of the object.</li> </ul> <p>Methods</p> <p>contains()</p>"},{"location":"abstract/messages/#box2d-methods","title":"Box2D methods","text":"<p>[source]</p>"},{"location":"abstract/messages/#contains","title":"contains","text":"<pre><code>contains(point)\n</code></pre> <p>Checks if point is inside bounding box.</p> <p>Arguments</p> <ul> <li>point: Numpy array of size 2.</li> </ul> <p>Returns</p> <p>Boolean. 'True' if 'point' is inside bounding box.     'False' otherwise.</p> <p>[source]</p>"},{"location":"abstract/messages/#pose6d","title":"Pose6D","text":"<pre><code>paz.abstract.messages.Pose6D(quaternion, translation, class_name=None)\n</code></pre> <p>Pose estimation results with 6D coordinates.</p> <p>Properties</p> <ul> <li>quaternion: List of 4 floats indicating (w, x, y, z) components.</li> <li>translation: List of 3 floats indicating (x, y, z)     translation components.</li> <li>class_name: String or <code>None</code> indicating the class label name of     the object.</li> </ul> <p>Class Methods</p> <ul> <li>from_rotation_vector: Instantiates a <code>Pose6D</code> object using a     rotation and a translation vector.</li> </ul>"},{"location":"abstract/processor/","title":"Processor","text":"<p>[source]</p>"},{"location":"abstract/processor/#processor-class","title":"Processor class","text":"<pre><code>paz.abstract.processor.Processor(name=None)\n</code></pre> <p>Abstract class for creating a processor unit.</p> <p>Arguments</p> <ul> <li>name: String indicating name of the processing unit.</li> </ul> <p>Methods</p> <p>call()</p> <p>Example</p> <pre><code>class NormalizeImage(Processor):\ndef __init__(self):\n    super(NormalizeImage, self).__init__()\n\ndef call(self, image):\n    return image / 255.0\n</code></pre> <p>Why this name?</p> <p>Originally PAZ was only meant for pre-processing pipelines that included data-augmentation, normalization, etc. However, I found out that we could use the same API for post-processing; therefore, I thought at the time that <code>Processor</code> would be adequate to describe the capacity of both pre-processing and post-processing. Names that I also thought could have worked were: <code>Function</code>, <code>Functor</code> but I didn't want to use those since I thought they could also cause confusion. Similarly, in Keras this abstraction is interpreted as a <code>Layer</code> but here I don't think that abstraction is adequate. A layer of computation maybe? So after having this thoughts swirling around I decided to go with <code>Processor</code> and try to be explicit about my mental jugglery hoping the name doesn't cause much mental overhead.</p>"},{"location":"abstract/processor/#processor-methods","title":"Processor methods","text":"<p>[source]</p>"},{"location":"abstract/processor/#call","title":"call","text":"<pre><code>call(X)\n</code></pre> <p>Custom user's logic should be implemented here.</p> <p>[source]</p>"},{"location":"abstract/processor/#sequentialprocessor-class","title":"SequentialProcessor class","text":"<pre><code>paz.abstract.processor.SequentialProcessor(processors=None, name=None)\n</code></pre> <p>Abstract class for creating a sequential pipeline of processors.</p> <p>Arguments</p> <ul> <li>processors: List of instantiated child classes of <code>Processor</code>     classes.</li> <li>name: String indicating name of the processing unit.</li> </ul> <p>Methods</p> <p>add() remove() pop() insert() get_processor()</p> <p>Example</p> <pre><code>AugmentImage = SequentialProcessor()\nAugmentImage.add(pr.RandomContrast())\nAugmentImage.add(pr.RandomBrightness())\naugment_image = AugmentImage()\n\ntransformed_image = augment_image(image)\n</code></pre>"},{"location":"abstract/processor/#sequentialprocessor-methods","title":"SequentialProcessor methods","text":"<p>[source]</p>"},{"location":"abstract/processor/#add","title":"add","text":"<pre><code>add(processor)\n</code></pre> <p>Adds a process to the sequence of processes to be applied to input.</p> <p>Arguments</p> <ul> <li>processor: An instantiated child class of of <code>Processor</code>.</li> </ul> <p>[source]</p>"},{"location":"abstract/processor/#remove","title":"remove","text":"<pre><code>remove(name)\n</code></pre> <p>Removes processor from sequence</p> <p>Arguments</p> <ul> <li>name: String indicating the process name</li> </ul> <p>[source]</p>"},{"location":"abstract/processor/#pop","title":"pop","text":"<pre><code>pop(index=-1)\n</code></pre> <p>Pops processor in given index from sequence</p> <p>Arguments</p> <ul> <li>index: Int.</li> </ul> <p>[source]</p>"},{"location":"abstract/processor/#insert","title":"insert","text":"<pre><code>insert(index, processor)\n</code></pre> <p>Inserts <code>processor</code> to self.processors queue at <code>index</code></p> <p>Argument</p> <ul> <li>index: Int.</li> <li>processor: An instantiated child class of of <code>Processor</code>.</li> </ul> <p>[source]</p>"},{"location":"abstract/processor/#get_processor","title":"get_processor","text":"<pre><code>get_processor(name)\n</code></pre> <p>Gets processor from sequencer</p> <p>Arguments</p> <ul> <li>name: String indicating the process name</li> </ul>"},{"location":"abstract/sequence/","title":"Sequence","text":"<p>[source]</p>"},{"location":"abstract/sequence/#processingsequence","title":"ProcessingSequence","text":"<pre><code>paz.abstract.sequence.ProcessingSequence(processor, batch_size, data, as_list=False)\n</code></pre> <p>Sequence generator used for processing samples given in <code>data</code>.</p> <p>Arguments</p> <ul> <li>processor: Function, used for processing elements of <code>data</code>.</li> <li>batch_size: Int.</li> <li>data: List. Each element of the list is processed by <code>processor</code>.</li> <li>as_list: Bool, if True <code>inputs</code> and <code>labels</code> are dispatched as     lists. If false <code>inputs</code> and <code>labels</code> are dispatched as     dictionaries.</li> </ul> <p>[source]</p>"},{"location":"abstract/sequence/#generatingsequence","title":"GeneratingSequence","text":"<pre><code>paz.abstract.sequence.GeneratingSequence(processor, batch_size, num_steps, as_list=False)\n</code></pre> <p>Sequence generator used for generating samples.</p> <p>Arguments</p> <ul> <li>processor: Function used for generating and processing <code>samples</code>.</li> <li>batch_size: Int.</li> <li>num_steps: Int. Number of steps for each epoch.</li> <li>as_list: Bool, if True <code>inputs</code> and <code>labels</code> are dispatched as     lists. If false <code>inputs</code> and <code>labels</code> are dispatched as     dictionaries.</li> </ul>"},{"location":"backend/anchors/","title":"Anchors","text":"<p>[source]</p>"},{"location":"backend/anchors/#build_anchors","title":"build_anchors","text":"<pre><code>paz.backend.anchors.build_anchors(image_shape, branches, num_scales, aspect_ratios, scale)\n</code></pre> <p>Builds anchor boxes in centre form for given model. Anchor boxes a.k.a prior boxes are reference boxes built with various scales and aspect ratio centered over every pixel in the input image and branch tensors. They can be strided. Anchor boxes define regions of image where objects are likely to be found. They help object detector to accurately localize and classify objects at the same time handling variations in object size and shape.</p> <p>Arguments</p> <ul> <li>image_shape: List, input image shape.</li> <li>branches: List, EfficientNet branch tensors.</li> <li>num_scales: Int, number of anchor scales.</li> <li>aspect_ratios: List, anchor box aspect ratios.</li> <li>scale: Float, anchor box scale.</li> </ul> <p>Returns</p> <ul> <li>anchor_boxes: Array of shape <code>(num_boxes, 4)</code>.</li> </ul> <p>[source]</p>"},{"location":"backend/anchors/#build_octaves","title":"build_octaves","text":"<pre><code>paz.backend.anchors.build_octaves(num_scales, aspect_ratios)\n</code></pre> <p>Builds branch-wise EfficientNet anchor box octaves. Octaves are values that differ from each other by a multiplicative factor of 2. In case of EfficienDet the scales of anchor box, which are integers raised to the power of 2 are normalized. This makes the values differ from each other by a multiplicative factor of approximately 1.2599. Therefore in this case it is not a perfect octave however is an approximation of octave. The following shows an example visualization of anchor boxes each with same aspect ratio and scale but with different octaves.</p> <p>+--------+       +---------------+       +----------------------+ |        |       |               |       |                      | |  0.0   |       |               |       |                      | |        |       |     0.33      |       |                      | +--------+       |               |       |         0.67         | |               |       |                      | +---------------+       |                      | |                      | |                      | +----------------------+</p> <p>Arguments</p> <ul> <li>num_scales: Int, number of anchor scales.</li> <li>aspect_ratios: List, anchor box aspect ratios.</li> </ul> <p>Returns</p> <ul> <li>octave_normalized: Array of shape <code>(num_scale_aspect,)</code>.</li> </ul> <p>[source]</p>"},{"location":"backend/anchors/#build_aspect","title":"build_aspect","text":"<pre><code>paz.backend.anchors.build_aspect(num_scales, aspect_ratios)\n</code></pre> <p>Builds branch-wise EfficientNet anchor box aspect ratios. The aspect ratio of an anchor box refers to the ratio of its width to its height. They define the shape of the object that the object detector is trying to detect. If aspect ratio is 1, the anchor box is a square. If it is greater than 1, the box is wider than it is tall. If it is less than 1, the box is taller than its is wide. The following shows visualization of anchor boxes each with same octave and scale but different aspect ratios.</p> <p>+--------+       +---------------+       +--------+ |        |       |               |       |        | |  1.0   |       |      2.0      |       |        | |        |       |               |       |  0.5   | +--------+       +---------------+       |        | |        | |        | +--------+</p> <p>Arguments</p> <ul> <li>num_scales: Int, number of anchor scales.</li> <li>aspect_ratios: List, anchor box aspect ratios.</li> </ul> <p>Returns</p> <p>Array of shape <code>(num_scale_aspect,)</code>.</p> <p>[source]</p>"},{"location":"backend/anchors/#build_scales","title":"build_scales","text":"<pre><code>paz.backend.anchors.build_scales(scale, num_scale_aspect)\n</code></pre> <p>Builds branch-wise EfficientNet anchor box scales. Anchor box scale refers to the size of the anchor box. The scale of the anchor box determines how large the box is in relation of the object it is trying to detect. If the object detector is trying to detect smaller objects, anchor box with smaller scales may be more effective. If the object detector is trying to detect larger objects, anchor box with larger scales my be more effective. The following shows an example visualization of anchor boxes each with same octave and aspect ratio but with different scales.</p> <p>+--------+       +----------------+       +------------------------+ |        |       |                |       |                        | |  1.0   |       |                |       |                        | |        |       |                |       |                        | +--------+       |       2.0      |       |                        | |                |       |                        | |                |       |           3.0          | |                |       |                        | +----------------+       |                        | |                        | |                        | |                        | +------------------------+</p> <p>Arguments</p> <ul> <li>scale: Float, anchor box scale.</li> <li>num_scale_aspect: Int, number of scale and aspect combinations.</li> </ul> <p>Returns</p> <p>Array of shape <code>(num_scale_aspect,)</code>.</p> <p>[source]</p>"},{"location":"backend/anchors/#build_strides","title":"build_strides","text":"<pre><code>paz.backend.anchors.build_strides(branch_arg, image_shape, branches, num_scale_aspect)\n</code></pre> <p>Builds branch-wise EfficientNet anchor box strides. The stride of an anchor box determines how densely the anchor boxes are placed in the image. A smaller stride means that the anchor boxes are more densely packed and cover a larger area of the image, while a larger stride means that the anchor boxes are less densely packed and cover a smaller area of the image. In general, a smaller stride is more effective at detecting smaller objects, while a larger stride is more effective at detecting larger objects. The optimal stride for a particular object detection system will depend on the sizes of the objects that it is trying to detect and the resolution of the input images. The following shows an example visualization of anchor box's centre marked by + each with same octave and aspect ratio and scale but with different strides.</p> <p>8.0                     16.0                     32.0 +-----------------+      +-----------------+     +-----------------+ | + + + + + + + + |      |                 |     |   +    +    +   | | + + + + + + + + |      |  +  +  +  +  +  |     |                 | | + + + + + + + + |      |                 |     |                 | | + + + + + + + + |      |  +  +  +  +  +  |     |   +    +    +   | | + + + + + + + + |      |                 |     |                 | | + + + + + + + + |      |  +  +  +  +  +  |     |                 | | + + + + + + + + |      |                 |     |   +    +    +   | +-----------------+      +-----------------+     +-----------------+</p> <p>Arguments</p> <ul> <li>branch_arg: Int, branch index.</li> <li>image_shape: List, input image shape.</li> <li>branches: List, EfficientNet branch tensors.</li> <li>num_scale_aspect: Int, count of scale aspect ratio combinations.</li> </ul> <p>Returns</p> <ul> <li>Tuple: Containing strides in y and x direction.</li> </ul> <p>[source]</p>"},{"location":"backend/anchors/#make_branch_boxes","title":"make_branch_boxes","text":"<pre><code>paz.backend.anchors.make_branch_boxes(stride_y, stride_x, octave, aspect, scales, image_shape)\n</code></pre> <p>Builds branch-wise EfficientNet anchor boxes.</p> <p>Arguments</p> <ul> <li>stride_y: Array of shape <code>(num_scale_aspect,)</code> y-axis stride.</li> <li>stride_x: Array of shape <code>(num_scale_aspect,)</code> x-axis stride.</li> <li>octave: Array of shape <code>(num_scale_aspect,)</code> octave scale.</li> <li>aspect: Array of shape <code>(num_scale_aspect,)</code> aspect ratio.</li> <li>scales: Array of shape <code>(num_scale_aspect,)</code> anchor box scales.</li> <li>image_shape: List, input image shape.</li> </ul> <p>Returns</p> <ul> <li>branch_boxes: Array of shape <code>(num_boxes,num_scale_aspect,4)</code>.</li> </ul> <p>[source]</p>"},{"location":"backend/anchors/#compute_box_coordinates","title":"compute_box_coordinates","text":"<pre><code>paz.backend.anchors.compute_box_coordinates(image_shape, stride_y, stride_x, scale, octave_scale, aspect)\n</code></pre> <p>Computes anchor box coordinates in corner form.</p> <p>Arguments</p> <ul> <li>image_shape: List, input image shape.</li> <li>stride_y: Array of shape <code>(num_scale_aspect,)</code> y-axis stride.</li> <li>stride_x: Array of shape <code>(num_scale_aspect,)</code> x-axis stride.</li> <li>scale: Array of shape <code>()</code>, anchor box scales.</li> <li>octave_scale: Array of shape <code>()</code>, anchor box octave scale.</li> <li>aspect: Array of shape <code>()</code>, anchor box aspect ratio.</li> </ul> <p>Returns</p> <ul> <li>Tuple: Box coordinates in corner form.</li> </ul> <p>[source]</p>"},{"location":"backend/anchors/#build_base_anchor","title":"build_base_anchor","text":"<pre><code>paz.backend.anchors.build_base_anchor(stride_y, stride_x, scale, octave_scale)\n</code></pre> <p>Builds base anchor's width and height.</p> <p>Arguments</p> <ul> <li>stride_y: Array of shape <code>(num_scale_aspect,)</code> y-axis stride.</li> <li>stride_x: Array of shape <code>(num_scale_aspect,)</code> x-axis stride.</li> <li>scale: Float, anchor box scale.</li> <li>octave_scale: Array of shape <code>()</code>, anchor box octave scale.</li> </ul> <p>Returns</p> <ul> <li>Tuple: Base anchor width and height.</li> </ul> <p>[source]</p>"},{"location":"backend/anchors/#compute_aspect_size","title":"compute_aspect_size","text":"<pre><code>paz.backend.anchors.compute_aspect_size(aspect)\n</code></pre> <p>Computes aspect width and height.</p> <p>Arguments</p> <ul> <li>aspect: Array of shape <code>()</code>, anchor box aspect ratio.</li> </ul> <p>Returns</p> <ul> <li>Tuple: Aspect width and height.</li> </ul> <p>[source]</p>"},{"location":"backend/anchors/#compute_anchor_dims","title":"compute_anchor_dims","text":"<pre><code>paz.backend.anchors.compute_anchor_dims(base_anchor_W, base_anchor_H, aspect_W, aspect_H, image_shape)\n</code></pre> <p>Compute anchor's half width and half height.</p> <p>Arguments</p> <ul> <li>base_anchor_W: Array of shape (), base anchor width.</li> <li>base_anchor_H: Array of shape (), base anchor height.</li> <li>aspect_W: Array of shape (), aspect width.</li> <li>aspect_H: Array of shape (), aspect height.</li> <li>image_shape: List, input image shape.</li> </ul> <p>Returns</p> <ul> <li>Tuple: Anchor's half width and height.</li> </ul> <p>[source]</p>"},{"location":"backend/anchors/#compute_anchor_centres","title":"compute_anchor_centres","text":"<pre><code>paz.backend.anchors.compute_anchor_centres(stride_y, stride_x, image_shape)\n</code></pre> <p>Compute anchor centres normalized to image size.</p> <p>Arguments</p> <ul> <li>stride_y: Array of shape <code>(num_scale_aspect,)</code> y-axis stride.</li> <li>stride_x: Array of shape <code>(num_scale_aspect,)</code> x-axis stride.</li> <li>image_shape: List, input image shape.</li> </ul> <p>Returns</p> <ul> <li>Tuple: Normalized anchor centres.</li> </ul>"},{"location":"backend/angles/","title":"Angles","text":"<p>[source]</p>"},{"location":"backend/angles/#calculate_relative_angle","title":"calculate_relative_angle","text":"<pre><code>paz.backend.angles.calculate_relative_angle(absolute_rotation, links_origin_transform, parents=[None, 0, 1, 2, 0, 4, 5, 0, 7, 8, 0, 10, 11, 0, 13, 14, 3, 6, 9, 12, 15])\n</code></pre> <p>Calculate the realtive joint rotation for the minimal hand joints.</p> <p>Arguments</p> <p>absolute_angles : Array [num_joints, 4]. Absolute joint angle rotation for the minimal hand joints in Euler representation.</p> <p>Returns</p> <ul> <li>relative_angles: Array [num_joints, 3].</li> </ul> <p>Relative joint rotation of the minimal hand joints in compact axis angle representation.</p> <p>[source]</p>"},{"location":"backend/angles/#reorder_relative_angles","title":"reorder_relative_angles","text":"<pre><code>paz.backend.angles.reorder_relative_angles(relative_angles, root_angle, children, root_joints=[1, 4, 7, 10, 13])\n</code></pre> <p>Reorder the relative angles according to the kinematic chain</p> <p>Arguments</p> <ul> <li>relative_angles: Array</li> <li>root_angle: Array. root joint angle for the minimal hand</li> <li>children: List, Indexes of the children in the kinematic chain.</li> </ul> <p>Returns</p> <ul> <li>angles: Array. Reordered relative angles</li> </ul> <p>[source]</p>"},{"location":"backend/angles/#change_link_order","title":"change_link_order","text":"<pre><code>paz.backend.angles.change_link_order(joints, config1_labels, config2_labels)\n</code></pre> <p>Map data from config1_labels to config2_labels.</p> <p>Arguments</p> <ul> <li>joints: Array</li> <li>config1_labels: input joint configuration</li> <li>config2_labels: output joint configuration</li> </ul> <p>Returns</p> <ul> <li>Array: joints maped to the config2_labels</li> </ul> <p>[source]</p>"},{"location":"backend/angles/#is_hand_open","title":"is_hand_open","text":"<pre><code>paz.backend.angles.is_hand_open(relative_angles, joint_name_to_arg, thresh)\n</code></pre> <p>Check is the hand is open by calculating relative pip joint angle norm.</p> <p>[(theta * ex), (theta * ey), (theta * ez)] = compact axis angle ex, ey, ez = normalized_axis</p> <p>norm =    / (theta2) * [(ex2) + (ey2) + (ez2)] \\/</p> <p>=&gt; norm is directly proportional to the theta if axis is normalized. If hand is open the relative angle of the pip joint will be less as compared to the closed hand.</p> <p>Arguments</p> <ul> <li>relative_angle: Array</li> <li>joint_name_to_arg: Dictionary for the joints</li> <li>thresh: Float. Threshold value for theta</li> </ul> <p>Returns</p> <ul> <li>Boolean: Hand is open or closed.</li> </ul>"},{"location":"backend/boxes/","title":"Boxes","text":"<p>Backend functionality for 2D bounding boxes</p> <p>[source]</p>"},{"location":"backend/boxes/#apply_non_max_suppression","title":"apply_non_max_suppression","text":"<pre><code>paz.backend.boxes.apply_non_max_suppression(boxes, scores, iou_thresh=0.45, top_k=200)\n</code></pre> <p>Apply non maximum suppression.</p> <p>Arguments</p> <ul> <li>boxes: Numpy array, box coordinates of shape <code>(num_boxes, 4)</code>     where each columns corresponds to x_min, y_min, x_max, y_max.</li> <li>scores: Numpy array, of scores given for each box in <code>boxes</code>.</li> <li>iou_thresh: float, intersection over union threshold for removing     boxes.</li> <li>top_k: int, number of maximum objects per class.</li> </ul> <p>Returns</p> <ul> <li>selected_indices: Numpy array, selected indices of kept boxes.</li> <li>num_selected_boxes: int, number of selected boxes.</li> </ul> <p>[source]</p>"},{"location":"backend/boxes/#nms_per_class","title":"nms_per_class","text":"<pre><code>paz.backend.boxes.nms_per_class(box_data, nms_thresh=0.45, epsilon=0.01, top_k=200)\n</code></pre> <p>Applies non maximum suppression per class. This function takes all the detections from the detector which consists of boxes and their corresponding class scores to which it applies non maximum suppression for every class independently and then combines the result.</p> <p>Arguments</p> <ul> <li>box_data: Array of shape <code>(num_nms_boxes, 4 + num_classes)</code>     containing the box coordinates as well as the predicted     scores of all the classes for all non suppressed boxes.</li> <li>nms_thresh: Float, Non-maximum suppression threshold.</li> <li>epsilon: Float, Filter scores with a lower confidence     value before performing non-maximum supression.</li> <li>top_k: Int, Maximum number of boxes per class outputted by nms.</li> </ul> <p>Returns</p> <ul> <li>Tuple: Containing an array non suppressed boxes of shape     <code>(num_nms_boxes, 4 + num_classes)</code> and an array     of corresponding class labels of shape <code>(num_nms_boxes, )</code>.</li> </ul> <p>[source]</p>"},{"location":"backend/boxes/#_nms_per_class","title":"_nms_per_class","text":"<pre><code>paz.backend.boxes._nms_per_class(nms_boxes, class_labels, class_arg, decoded_boxes, class_predictions, epsilon, nms_thresh, top_k)\n</code></pre> <p>Applies non maximum suppression for a given class. This function takes all the detections that belong only to the given single class and applies non maximum suppression for that class alone and returns the resulting non suppressed boxes.</p> <p>Arguments</p> <ul> <li>nms_boxes: Array of shape <code>(num_boxes, 4 + num_classes)</code>.</li> <li>class_labels: Array of shape <code>(num_boxes, )</code>.</li> <li>class_arg: Int, class index.</li> <li>decoded_boxes: Array of shape <code>(num_prior_boxes, 4)</code>     containing the box coordinates of all the     non suppressed boxes.</li> <li>class_predictions: Array of shape     <code>(num_nms_boxes, num_classes)</code> containing the predicted     scores of all the classes for all the non suppressed boxes.</li> <li>epsilon: Float, Filter scores with a lower confidence     value before performing non-maximum supression.</li> <li>nms_thresh: Float, Non-maximum suppression threshold.</li> <li>top_k: Int, Maximum number of boxes per class outputted by nms.</li> </ul> <p>Returns</p> <ul> <li>Tuple: Containing an array non suppressed boxes per class of     shape <code>(num_nms_boxes_per_class, 4 + num_classes) and an     array corresponding class labels of shape</code>(num_nms_boxes_per_class, )`.</li> </ul> <p>[source]</p>"},{"location":"backend/boxes/#pre_filter_nms","title":"pre_filter_nms","text":"<pre><code>paz.backend.boxes.pre_filter_nms(class_arg, class_predictions, epsilon)\n</code></pre> <p>Applies score filtering. This function takes all the predicted scores of a given class and filters out all the predictions less than the given <code>epsilon</code> value.</p> <p>Arguments</p> <ul> <li>class_arg: Int, class index.</li> <li>class_predictions: Array of shape     <code>(num_nms_boxes, num_classes)</code> containing the predicted     scores of all the classes for all the non suppressed boxes.</li> <li>epsilon: Float, threshold value for score filtering.</li> </ul> <p>Returns</p> <ul> <li>Tuple: Containing an array filtered scores of shape     <code>(num_pre_filtered_boxes, )</code> and an array filter mask of     shape <code>(num_prior_boxes, )</code>.</li> </ul> <p>[source]</p>"},{"location":"backend/boxes/#merge_nms_box_with_class","title":"merge_nms_box_with_class","text":"<pre><code>paz.backend.boxes.merge_nms_box_with_class(box_data, class_labels)\n</code></pre> <p>Merges box coordinates with their corresponding class defined by <code>class_labels</code> which is decided by best box geometry by non maximum suppression (and not by the best scoring class) into a single output. This function retains only the predicted score of the class to which the box belongs to and sets the scores of all the remaining classes to zero, thereby combining box and class information in a single variable.</p> <p>Arguments</p> <ul> <li>box_data: Array of shape <code>(num_nms_boxes, 4 + num_classes)</code>     containing the box coordinates as well as the predicted     scores of all the classes for all non suppressed boxes.</li> <li>class_labels: Array of shape <code>(num_nms_boxes, )</code> that contains     the indices of the class whose score is to be retained.</li> </ul> <p>Returns</p> <ul> <li>boxes: Array of shape <code>(num_nms_boxes, 4 + num_classes)</code>,     containing coordinates of non supressed boxes along with     scores of the class to which the box belongs. The scores of     the other classes are zeros.</li> </ul> <p>[source]</p>"},{"location":"backend/boxes/#suppress_other_class_scores","title":"suppress_other_class_scores","text":"<pre><code>paz.backend.boxes.suppress_other_class_scores(class_predictions, class_labels)\n</code></pre> <p>Retains the score of class in <code>class_labels</code> and sets other class scores to zero.</p> <p>Arguments</p> <ul> <li>class_predictions: Array of shape     <code>(num_nms_boxes, num_classes)</code> containing the predicted     scores of all the classes for all the non suppressed boxes.</li> <li>class_labels: Array of shape <code>(num_nms_boxes, )</code> that contains     the indices of the class whose score is to be retained.</li> </ul> <p>Returns</p> <ul> <li>retained_class_score: Array of shape     <code>(num_nms_boxes, num_classes)</code> that consists of score at     only those location specified by 'class_labels' and zero     at other class locations.</li> </ul> <p>Note</p> <p>This approach retains the scores of that class in <code>class_predictions</code> defined by <code>class_labels</code> by generating a boolean mask <code>score_suppress_mask</code> with elements True at the locations where the score in <code>class_predictions</code> is to be retained and False wherever the class score is to be suppressed. This approach of retaining/suppressing scores does not make use of for loop, if-else condition and direct value assignment to arrays.</p> <p>[source]</p>"},{"location":"backend/boxes/#offset","title":"offset","text":"<pre><code>paz.backend.boxes.offset(coordinates, offset_scales)\n</code></pre> <p>Apply offsets to box coordinates</p> <p>Arguments</p> <ul> <li>coordinates: List of floats containing coordinates in point form.</li> <li>offset_scales: List of floats having x and y scales respectively.</li> </ul> <p>Returns</p> <ul> <li>coordinates: List of floats containing coordinates in point form.     i.e. [x_min, y_min, x_max, y_max].</li> </ul> <p>[source]</p>"},{"location":"backend/boxes/#clip","title":"clip","text":"<pre><code>paz.backend.boxes.clip(coordinates, image_shape)\n</code></pre> <p>Clip box to valid image coordinates Arguments</p> <ul> <li>coordinates: List of floats containing coordinates in point form     i.e. [x_min, y_min, x_max, y_max].</li> <li>image_shape: List of two integers indicating height and width of image     respectively.</li> </ul> <p>Returns</p> <p>List of clipped coordinates.</p> <p>[source]</p>"},{"location":"backend/boxes/#compute_iou","title":"compute_iou","text":"<pre><code>paz.backend.boxes.compute_iou(box, boxes)\n</code></pre> <p>Calculates the intersection over union between 'box' and all 'boxes'. Both <code>box</code> and <code>boxes</code> are in corner coordinates.</p> <p>Arguments</p> <ul> <li>box: Numpy array with length at least of 4.</li> <li>boxes: Numpy array with shape <code>(num_boxes, 4)</code>.</li> </ul> <p>Returns</p> <p>Numpy array of shape <code>(num_boxes, 1)</code>.</p> <p>[source]</p>"},{"location":"backend/boxes/#compute_ious","title":"compute_ious","text":"<pre><code>paz.backend.boxes.compute_ious(boxes_A, boxes_B)\n</code></pre> <p>Calculates the intersection over union between <code>boxes_A</code> and <code>boxes_B</code>. For each box present in the rows of <code>boxes_A</code> it calculates the intersection over union with respect to all boxes in <code>boxes_B</code>. The variables <code>boxes_A</code> and <code>boxes_B</code> contain the corner coordinates of the left-top corner <code>(x_min, y_min)</code> and the right-bottom <code>(x_max, y_max)</code> corner.</p> <p>Arguments</p> <ul> <li>boxes_A: Numpy array with shape <code>(num_boxes_A, 4)</code>.</li> <li>boxes_B: Numpy array with shape <code>(num_boxes_B, 4)</code>.</li> </ul> <p>Returns</p> <p>Numpy array of shape <code>(num_boxes_A, num_boxes_B)</code>.</p> <p>[source]</p>"},{"location":"backend/boxes/#decode","title":"decode","text":"<pre><code>paz.backend.boxes.decode(predictions, priors, variances=[0.1, 0.1, 0.2, 0.2])\n</code></pre> <p>Decode default boxes into the ground truth boxes</p> <p>Arguments</p> <ul> <li>loc: Numpy array of shape <code>(num_priors, 4)</code>.</li> <li>priors: Numpy array of shape <code>(num_priors, 4)</code>.</li> <li>variances: List of two floats. Variances of prior boxes.</li> </ul> <p>Returns</p> <p>decoded boxes: Numpy array of shape <code>(num_priors, 4)</code>.</p> <p>[source]</p>"},{"location":"backend/boxes/#denormalize_box","title":"denormalize_box","text":"<pre><code>paz.backend.boxes.denormalize_box(box, image_shape)\n</code></pre> <p>Scales corner box coordinates from normalized values to image dimensions</p> <p>Arguments</p> <ul> <li>box: Numpy array containing corner box coordinates.</li> <li>image_shape: List of integers with (height, width).</li> </ul> <p>Returns</p> <ul> <li>returns: box corner coordinates in image dimensions</li> </ul> <p>[source]</p>"},{"location":"backend/boxes/#encode","title":"encode","text":"<pre><code>paz.backend.boxes.encode(matched, priors, variances=[0.1, 0.1, 0.2, 0.2])\n</code></pre> <p>Encode the variances from the priorbox layers into the ground truth boxes we have matched (based on jaccard overlap) with the prior boxes.</p> <p>Arguments</p> <ul> <li>matched: Numpy array of shape <code>(num_priors, 4)</code> with boxes in     point-form.</li> <li>priors: Numpy array of shape <code>(num_priors, 4)</code> with boxes in     center-form.</li> <li>variances: (list[float]) Variances of priorboxes</li> </ul> <p>Returns</p> <p>encoded boxes: Numpy array of shape <code>(num_priors, 4)</code>.</p> <p>[source]</p>"},{"location":"backend/boxes/#flip_left_right","title":"flip_left_right","text":"<pre><code>paz.backend.boxes.flip_left_right(boxes, width)\n</code></pre> <p>Flips box coordinates from left-to-right and vice-versa. Arguments</p> <ul> <li>boxes: Numpy array of shape <code>[num_boxes, 4]</code>. Returns</li> </ul> <p>Numpy array of shape <code>[num_boxes, 4]</code>.</p> <p>[source]</p>"},{"location":"backend/boxes/#make_box_square","title":"make_box_square","text":"<pre><code>paz.backend.boxes.make_box_square(box)\n</code></pre> <p>Makes box coordinates square with sides equal to the longest original side.</p> <p>Arguments</p> <ul> <li>box: Numpy array with shape <code>(4)</code> with point corner coordinates.</li> </ul> <p>Returns</p> <ul> <li>returns: List of box coordinates ints.</li> </ul> <p>[source]</p>"},{"location":"backend/boxes/#match","title":"match","text":"<pre><code>paz.backend.boxes.match(boxes, prior_boxes, iou_threshold=0.5)\n</code></pre> <p>Matches each prior box with a ground truth box (box from <code>boxes</code>). It then selects which matched box will be considered positive e.g. iou &gt; .5 and returns for each prior box a ground truth box that is either positive (with a class argument different than 0) or negative.</p> <p>Arguments</p> <ul> <li>boxes: Numpy array of shape <code>(num_ground_truh_boxes, 4 + 1)</code>,     where the first the first four coordinates correspond to     box coordinates and the last coordinates is the class     argument. This boxes should be the ground truth boxes.</li> <li>prior_boxes: Numpy array of shape <code>(num_prior_boxes, 4)</code>.     where the four coordinates are in center form coordinates.</li> <li>iou_threshold: Float between [0, 1]. Intersection over union     used to determine which box is considered a positive box.</li> </ul> <p>Returns</p> <p>numpy array of shape <code>(num_prior_boxes, 4 + 1)</code>.     where the first the first four coordinates correspond to point     form box coordinates and the last coordinates is the class     argument.</p> <p>[source]</p>"},{"location":"backend/boxes/#nms_per_class_1","title":"nms_per_class","text":"<pre><code>paz.backend.boxes.nms_per_class(box_data, nms_thresh=0.45, epsilon=0.01, top_k=200)\n</code></pre> <p>Applies non maximum suppression per class. This function takes all the detections from the detector which consists of boxes and their corresponding class scores to which it applies non maximum suppression for every class independently and then combines the result.</p> <p>Arguments</p> <ul> <li>box_data: Array of shape <code>(num_nms_boxes, 4 + num_classes)</code>     containing the box coordinates as well as the predicted     scores of all the classes for all non suppressed boxes.</li> <li>nms_thresh: Float, Non-maximum suppression threshold.</li> <li>epsilon: Float, Filter scores with a lower confidence     value before performing non-maximum supression.</li> <li>top_k: Int, Maximum number of boxes per class outputted by nms.</li> </ul> <p>Returns</p> <ul> <li>Tuple: Containing an array non suppressed boxes of shape     <code>(num_nms_boxes, 4 + num_classes)</code> and an array     of corresponding class labels of shape <code>(num_nms_boxes, )</code>.</li> </ul> <p>[source]</p>"},{"location":"backend/boxes/#to_image_coordinates","title":"to_image_coordinates","text":"<pre><code>paz.backend.boxes.to_image_coordinates(boxes, image)\n</code></pre> <p>Transforms normalized box coordinates into image coordinates. Arguments</p> <ul> <li>image: Numpy array.</li> <li>boxes: Numpy array of shape <code>[num_boxes, N]</code> where N &gt;= 4. Returns</li> </ul> <p>Numpy array of shape <code>[num_boxes, N]</code>.</p> <p>[source]</p>"},{"location":"backend/boxes/#to_center_form","title":"to_center_form","text":"<pre><code>paz.backend.boxes.to_center_form(boxes)\n</code></pre> <p>Transform from corner coordinates to center coordinates.</p> <p>Arguments</p> <ul> <li>boxes: Numpy array with shape <code>(num_boxes, 4)</code>.</li> </ul> <p>Returns</p> <p>Numpy array with shape <code>(num_boxes, 4)</code>.</p> <p>[source]</p>"},{"location":"backend/boxes/#to_one_hot","title":"to_one_hot","text":"<pre><code>paz.backend.boxes.to_one_hot(class_indices, num_classes)\n</code></pre> <p>Transform from class index to one-hot encoded vector.</p> <p>Arguments</p> <ul> <li>class_indices: Numpy array. One dimensional array specifying     the index argument of the class for each sample.</li> <li>num_classes: Integer. Total number of classes.</li> </ul> <p>Returns</p> <p>Numpy array with shape <code>(num_samples, num_classes)</code>.</p> <p>[source]</p>"},{"location":"backend/boxes/#to_normalized_coordinates","title":"to_normalized_coordinates","text":"<pre><code>paz.backend.boxes.to_normalized_coordinates(boxes, image)\n</code></pre> <p>Transforms coordinates in image dimensions to normalized coordinates. Arguments</p> <ul> <li>image: Numpy array.</li> <li>boxes: Numpy array of shape <code>[num_boxes, N]</code> where N &gt;= 4. Returns</li> </ul> <p>Numpy array of shape <code>[num_boxes, N]</code>.</p> <p>[source]</p>"},{"location":"backend/boxes/#to_corner_form","title":"to_corner_form","text":"<pre><code>paz.backend.boxes.to_corner_form(boxes)\n</code></pre> <p>Transform from center coordinates to corner coordinates.</p> <p>Arguments</p> <ul> <li>boxes: Numpy array with shape <code>(num_boxes, 4)</code>.</li> </ul> <p>Returns</p> <p>Numpy array with shape <code>(num_boxes, 4)</code>.</p> <p>[source]</p>"},{"location":"backend/boxes/#extract_bounding_box_corners","title":"extract_bounding_box_corners","text":"<pre><code>paz.backend.boxes.extract_bounding_box_corners(points3D)\n</code></pre> <p>Extracts the (x_min, y_min, z_min) and the (x_max, y_max, z_max) coordinates from an array of  points3D Arguments</p> <ul> <li>points3D: Array (num_points, 3)</li> </ul> <p>Returns</p> <p>Left-down-bottom corner (x_min, y_min, z_min) and right-up-top     (x_max, y_max, z_max) corner.</p> <p>[source]</p>"},{"location":"backend/boxes/#scale_box","title":"scale_box","text":"<pre><code>paz.backend.boxes.scale_box(predictions, image_scales)\n</code></pre> <p>Arguments</p> <ul> <li>predictions: Array of shape <code>(num_boxes, num_classes+N)</code>     model predictions.</li> <li>image_scales: Array of shape <code>()</code>, scale value of boxes.</li> </ul> <p>Returns</p> <ul> <li>predictions: Array of shape <code>(num_boxes, num_classes+N)</code>     model predictions.</li> </ul> <p>[source]</p>"},{"location":"backend/boxes/#change_box_coordinates","title":"change_box_coordinates","text":"<pre><code>paz.backend.boxes.change_box_coordinates(outputs)\n</code></pre> <p>Converts box coordinates format from (y_min, x_min, y_max, x_max) to (x_min, y_min, x_max, y_max).</p> <p>Arguments</p> <ul> <li>outputs: Tensor, model output.</li> </ul> <p>Returns</p> <ul> <li>outputs: Array, Processed outputs by merging the features     at all levels. Each row corresponds to box coordinate     offsets and sigmoid of the class logits.</li> </ul>"},{"location":"backend/camera/","title":"Camera","text":"<p>Backend functionality for cameras</p> <p>[source]</p>"},{"location":"backend/camera/#camera-class","title":"Camera class","text":"<pre><code>paz.backend.camera.Camera(device_id=0, name='Camera', intrinsics=None, distortion=None)\n</code></pre> <p>Camera abstract class. By default this camera uses the openCV functionality. It can be inherited to overwrite methods in case another camera API exists.</p>"},{"location":"backend/camera/#camera-methods","title":"Camera methods","text":"<p>[source]</p>"},{"location":"backend/camera/#is_open","title":"is_open","text":"<pre><code>is_open()\n</code></pre> <p>Checks if camera is open.</p> <p>Returns</p> <p>Boolean</p> <p>[source]</p>"},{"location":"backend/camera/#start","title":"start","text":"<pre><code>start()\n</code></pre> <p>Starts capturing device</p> <p>Returns</p> <p>Camera object.</p> <p>[source]</p>"},{"location":"backend/camera/#stop","title":"stop","text":"<pre><code>stop()\n</code></pre> <p>Stops capturing device.</p> <p>[source]</p>"},{"location":"backend/camera/#intrinsics_from_hfov","title":"intrinsics_from_HFOV","text":"<pre><code>intrinsics_from_HFOV(HFOV=70, image_shape=None)\n</code></pre> <p>Computes camera intrinsics using horizontal field of view (HFOV).</p> <p>Arguments</p> <ul> <li>HFOV: Angle in degrees of horizontal field of view.</li> <li>image_shape: List of two floats [height, width].</li> </ul> <p>Returns</p> <p>camera intrinsics array (3, 3).</p> <p>Notes:</p> <pre><code>       \\           /      ^\n        \\         /       |\n         \\ lens  /        | w/2\n</code></pre> <p>horizontal field  \\     / alpha/2 | of view (alpha)_( )/______ |      image                /( )\\          |      plane               /     &lt;-- f --&gt; |              /       \\        |             /         \\       |            /           \\      v</p> <p>Pinhole camera model</p> <p>From the image above we know that: tan(alpha/2) = w/2f -&gt; f = w/2 * (1/tan(alpha/2))</p> <p>alpha in webcams and phones is often between 50 and 70 degrees. -&gt; 0.7 w &lt;= f &lt;= w</p> <p>[source]</p>"},{"location":"backend/camera/#take_photo","title":"take_photo","text":"<pre><code>take_photo()\n</code></pre> <p>Starts camera, reads buffer and returns an image.</p> <p>[source]</p>"},{"location":"backend/camera/#videoplayer-class","title":"VideoPlayer class","text":"<pre><code>paz.backend.camera.VideoPlayer(image_size, pipeline, camera, topic='image')\n</code></pre> <p>Performs visualization inferences in a real-time video.</p> <p>Properties</p> <ul> <li>image_size: List of two integers. Output size of the displayed image.</li> <li>pipeline: Function. Should take RGB image as input and it should     output a dictionary with key 'image' containing a visualization     of the inferences. Built-in pipelines can be found in     <code>paz/processing/pipelines</code>.</li> </ul> <p>Methods</p> <p>run() record()</p>"},{"location":"backend/camera/#videoplayer-methods","title":"VideoPlayer methods","text":"<p>[source]</p>"},{"location":"backend/camera/#step","title":"step","text":"<pre><code>step()\n</code></pre> <p>Runs the pipeline process once</p> <p>Returns</p> <p>Inferences from <code>pipeline</code>.</p> <p>[source]</p>"},{"location":"backend/camera/#run","title":"run","text":"<pre><code>run()\n</code></pre> <p>Opens camera and starts continuous inference using <code>pipeline</code>, until the user presses <code>q</code> inside the opened window.</p> <p>[source]</p>"},{"location":"backend/camera/#record","title":"record","text":"<pre><code>record(name='video.avi', fps=20, fourCC='XVID')\n</code></pre> <p>Opens camera and records continuous inference using <code>pipeline</code>.</p> <p>Arguments</p> <ul> <li>name: String. Video name. Must include the postfix .avi.</li> <li>fps: Int. Frames per second.</li> <li>fourCC: String. Indicates the four character code of the video.</li> </ul> <p>e.g. XVID, MJPG, X264.</p> <p>[source]</p>"},{"location":"backend/camera/#record_from_file","title":"record_from_file","text":"<pre><code>record_from_file(video_file_path, name='video.avi', fps=20, fourCC='XVID')\n</code></pre> <p>Load video and records continuous inference using <code>pipeline</code>.</p> <p>Arguments</p> <ul> <li>video_file_path: String. Path to the video file.</li> <li>name: String. Output video name. Must include the postfix .avi.</li> <li>fps: Int. Frames per second.</li> <li>fourCC: String. Indicates the four character code of the video.</li> </ul> <p>e.g. XVID, MJPG, X264.</p>"},{"location":"backend/draw/","title":"Draw","text":"<p>[source]</p>"},{"location":"backend/draw/#draw_circle","title":"draw_circle","text":"<pre><code>paz.backend.image.draw.draw_circle(image, center, color=(0, 255, 0), radius=5)\n</code></pre> <p>Draw a circle in an image</p> <p>Arguments</p> <ul> <li>image: Array <code>(H, W, 3)</code></li> <li>center: List <code>(2)</code> with <code>(x, y)</code> values in openCV coordinates.</li> <li>radius: Float. Radius of circle.</li> <li>color: Tuple <code>(3)</code> indicating the RGB colors.</li> </ul> <p>Returns</p> <p>Array <code>(H, W, 3)</code> with circle.</p> <p>[source]</p>"},{"location":"backend/draw/#draw_square","title":"draw_square","text":"<pre><code>paz.backend.image.draw.draw_square(image, center, color, size)\n</code></pre> <p>Draw a square in an image</p> <p>Arguments</p> <ul> <li>image: Array <code>(H, W, 3)</code></li> <li>center: List <code>(2)</code> with <code>(x, y)</code> values in openCV coordinates.</li> <li>size: Float. Length of square size.</li> <li>color: List <code>(3)</code> indicating RGB colors.</li> </ul> <p>Returns</p> <p>Array <code>(H, W, 3)</code> with square.</p> <p>[source]</p>"},{"location":"backend/draw/#draw_triangle","title":"draw_triangle","text":"<pre><code>paz.backend.image.draw.draw_triangle(image, center, color, size)\n</code></pre> <p>Draw a triangle in an image</p> <p>Arguments</p> <ul> <li>image: Array <code>(H, W, 3)</code></li> <li>center: List <code>(2)</code> containing <code>(x_center, y_center)</code>.</li> <li>size: Float. Length of square size.</li> <li>color: Tuple <code>(3)</code> indicating the RGB colors.</li> </ul> <p>Returns</p> <p>Array <code>(H, W, 3)</code> with triangle.</p> <p>[source]</p>"},{"location":"backend/draw/#draw_keypoint","title":"draw_keypoint","text":"<pre><code>paz.backend.image.draw.draw_keypoint(image, point, color=(0, 255, 0), radius=5)\n</code></pre> <p>Draws a circle in image.</p> <p>Arguments</p> <ul> <li>image: Numpy array of shape <code>[H, W, 3]</code>.</li> <li>point: List of length two indicating <code>(y, x)</code>     openCV coordinates.</li> <li>color: List of length three indicating RGB color of point.</li> <li>radius: Integer indicating the radius of the point to be drawn.</li> </ul> <p>Returns</p> <p>Numpy array with shape <code>[H, W, 3]</code>. Image with circle.</p> <p>[source]</p>"},{"location":"backend/draw/#draw_cube","title":"draw_cube","text":"<pre><code>paz.backend.image.draw.draw_cube(image, points, color=(0, 255, 0), thickness=2, radius=5)\n</code></pre> <p>Draws a cube in image.</p> <p>Arguments</p> <ul> <li>image: Numpy array of shape (H, W, 3).</li> <li>points: List of length 8  having each element a list     of length two indicating (U, V) openCV coordinates.</li> <li>color: List of length three indicating RGB color of point.</li> <li>thickness: Integer indicating the thickness of the line to be drawn.</li> <li>radius: Integer indicating the radius of corner points to be drawn.</li> </ul> <p>Returns</p> <p>Numpy array with shape (H, W, 3). Image with cube.</p> <p>[source]</p>"},{"location":"backend/draw/#draw_dot","title":"draw_dot","text":"<pre><code>paz.backend.image.draw.draw_dot(image, point, color=(0, 255, 0), radius=5, filled=-1)\n</code></pre> <p>Draws a dot (small rectangle) in image.</p> <p>Arguments</p> <ul> <li>image: Numpy array of shape <code>[H, W, 3]</code>.</li> <li>point: List of length two indicating <code>(y, x)</code> openCV coordinates.</li> <li>color: List of length three indicating RGB color of point.</li> <li>radius: Integer indicating the radius of the point to be drawn.</li> <li>filled: Boolean. If <code>True</code> rectangle is filled with <code>color</code>.</li> </ul> <p>Returns</p> <p>Numpy array with shape <code>[H, W, 3]</code>. Image with dot.</p> <p>[source]</p>"},{"location":"backend/draw/#draw_filled_polygon","title":"draw_filled_polygon","text":"<pre><code>paz.backend.image.draw.draw_filled_polygon(image, vertices, color)\n</code></pre> <p>Draws filled polygon</p> <p>Arguments</p> <ul> <li>image: Numpy array.</li> <li>vertices: List of elements each having a list     of length two indicating <code>(y, x)</code> openCV coordinates.</li> <li>color: Numpy array specifying RGB color of the polygon.</li> </ul> <p>Returns</p> <p>Numpy array with shape <code>[H, W, 3]</code>. Image with polygon.</p> <p>[source]</p>"},{"location":"backend/draw/#draw_line","title":"draw_line","text":"<pre><code>paz.backend.image.draw.draw_line(image, point_A, point_B, color=(0, 255, 0), thickness=5)\n</code></pre> <p>Draws a line in image from <code>point_A</code> to <code>point_B</code>.</p> <p>Arguments</p> <ul> <li>image: Numpy array of shape <code>[H, W, 3]</code>.</li> <li>point_A: List of length two indicating <code>(y, x)</code> openCV coordinates.</li> <li>point_B: List of length two indicating <code>(y, x)</code> openCV coordinates.</li> <li>color: List of length three indicating RGB color of point.</li> <li>thickness: Integer indicating the thickness of the line to be drawn.</li> </ul> <p>Returns</p> <p>Numpy array with shape <code>[H, W, 3]</code>. Image with line.</p> <p>[source]</p>"},{"location":"backend/draw/#draw_random_polygon","title":"draw_random_polygon","text":"<pre><code>paz.backend.image.draw.draw_random_polygon(image, max_radius_scale=0.5)\n</code></pre> <p>Draw random polygon image.</p> <p>Arguments</p> <ul> <li>image: Numpy array with shape <code>[H, W, 3]</code>.</li> <li>max_radius_scale: Float between [0, 1].</li> </ul> <p>Returns</p> <p>Numpy array with shape <code>[H, W, 3]</code>. Image with polygon.</p> <p>[source]</p>"},{"location":"backend/draw/#draw_rectangle","title":"draw_rectangle","text":"<pre><code>paz.backend.image.draw.draw_rectangle(image, corner_A, corner_B, color, thickness)\n</code></pre> <p>Draws a filled rectangle from <code>corner_A</code> to <code>corner_B</code>.</p> <p>Arguments</p> <ul> <li>image: Numpy array of shape <code>[H, W, 3]</code>.</li> <li>corner_A: List of length two indicating <code>(y, x)</code> openCV coordinates.</li> <li>corner_B: List of length two indicating <code>(y, x)</code> openCV coordinates.</li> <li>color: List of length three indicating RGB color of point.</li> <li>thickness: Integer/openCV Flag. Thickness of rectangle line.     or for filled use cv2.FILLED flag.</li> </ul> <p>Returns</p> <p>Numpy array with shape <code>[H, W, 3]</code>. Image with rectangle.</p> <p>[source]</p>"},{"location":"backend/draw/#lincolor","title":"lincolor","text":"<pre><code>paz.backend.image.draw.lincolor(num_colors, saturation=1, value=1, normalized=False)\n</code></pre> <p>Creates a list of RGB colors linearly sampled from HSV space with randomised Saturation and Value.</p> <p>Arguments</p> <ul> <li>num_colors: Int.</li> <li>saturation: Float or <code>None</code>. If float indicates saturation.     If <code>None</code> it samples a random value.</li> <li>value: Float or <code>None</code>. If float indicates value.     If <code>None</code> it samples a random value.</li> <li>normalized: Bool. If True, RGB colors are returned between [0, 1]     if False, RGB colors are between [0, 255].</li> </ul> <p>Returns</p> <p>List, for which each element contains a list with RGB color</p> <p>[source]</p>"},{"location":"backend/draw/#put_text","title":"put_text","text":"<pre><code>paz.backend.image.draw.put_text(image, text, point, scale, color, thickness)\n</code></pre> <p>Draws text in image.</p> <p>Arguments</p> <ul> <li>image: Numpy array.</li> <li>text: String. Text to be drawn.</li> <li>point: Tuple of coordinates indicating the top corner of the text.</li> <li>scale: Float. Scale of text.</li> <li>color: Tuple of integers. RGB color coordinates.</li> <li>thickness: Integer. Thickness of the lines used for drawing text.</li> </ul> <p>Returns</p> <p>Numpy array with shape <code>[H, W, 3]</code>. Image with text.</p> <p>[source]</p>"},{"location":"backend/draw/#make_mosaic","title":"make_mosaic","text":"<pre><code>paz.backend.image.draw.make_mosaic(images, shape, border=0)\n</code></pre> <p>Creates an image mosaic.</p> <p>Arguments</p> <ul> <li>images: Numpy array of shape (num_images, height, width, num_channels)</li> <li>shape: List of two integers indicating the mosaic shape.</li> <li>border: Integer indicating the border per image.</li> </ul> <p>Returns</p> <p>A numpy array containing all images.</p> <p>Exceptions</p> <p>Shape must satisfy <code>len(images) &gt; shape[0] * shape[1]</code></p> <p>[source]</p>"},{"location":"backend/draw/#draw_points2d","title":"draw_points2D","text":"<pre><code>paz.backend.image.draw.draw_points2D(image, points2D, colors)\n</code></pre> <p>Draws a pixel for all points2D in UV space using only numpy.</p> <p>Arguments</p> <ul> <li>image: Array (H, W).</li> <li>keypoints: Array (num_points, U, V). Keypoints in image space</li> <li>colors: Array (num_points, 3). Colors in RGB space.</li> </ul> <p>Returns</p> <p>Array with drawn points.</p> <p>[source]</p>"},{"location":"backend/draw/#draw_keypoints_link","title":"draw_keypoints_link","text":"<pre><code>paz.backend.image.draw.draw_keypoints_link(image, keypoints, link_args, link_orders, link_colors, check_scores=False, link_width=2)\n</code></pre> <p>Draw link between the keypoints.</p> <p>Arguments</p> <ul> <li>images: Numpy array.</li> <li>keypoints: Keypoint(k0, k1, ...) locations in the image. Numpy array.</li> <li>link_args: Keypoint labels. Dictionary. {'k0':0, 'k1':1, ...}</li> <li>link_orders: List of tuple. [('k0', 'k1'),('kl', 'k2'), ...]</li> <li>link_colors: Color of each link. List of list</li> <li>check_scores: Condition to draw links. Boolean.</li> </ul> <p>Returns</p> <p>A numpy array containing drawn link between the keypoints.</p> <p>[source]</p>"},{"location":"backend/draw/#draw_keypoints","title":"draw_keypoints","text":"<pre><code>paz.backend.image.draw.draw_keypoints(image, keypoints, keypoint_colors, check_scores=False, keypoint_radius=6)\n</code></pre> <p>Draw a circle at keypoints.</p> <p>Arguments</p> <ul> <li>images: Numpy array.</li> <li>keypoints: Keypoint locations in the image. Numpy array.</li> <li>keypoint_colors: Color of each keypoint. List of list</li> <li>check_scores: Condition to draw keypoint. Boolean.</li> </ul> <p>Returns</p> <p>A numpy array containing circle at each keypoints.</p> <p>[source]</p>"},{"location":"backend/draw/#points3d_to_rgb","title":"points3D_to_RGB","text":"<pre><code>paz.backend.image.draw.points3D_to_RGB(points3D, object_sizes)\n</code></pre> <p>Transforms points3D in object frame to RGB color space. Arguments</p> <ul> <li>points3D: Array (num_points, 3). Points3D a</li> <li>object_sizes: Array (3) indicating the     (width, height, depth) of object.</li> </ul> <p>Returns</p> <p>Array of ints (num_points, 3) in RGB space.</p> <p>[source]</p>"},{"location":"backend/draw/#draw_rgb_mask","title":"draw_RGB_mask","text":"<pre><code>paz.backend.image.draw.draw_RGB_mask(image, points2D, points3D, object_sizes)\n</code></pre> <p>Draws RGB mask by transforming points3D to RGB space and putting in them in their 2D coordinates (points2D)</p> <p>Arguments</p> <ul> <li>image: Array (H, W, 3).</li> <li>points2D: Array (num_points, 2)</li> <li>points3D: Array (num_points, 3)</li> <li>object_sizes: Array (x_size, y_size, z_size)</li> </ul> <p>Returns</p> <p>Image array with drawn masks</p> <p>[source]</p>"},{"location":"backend/draw/#draw_rgb_masks","title":"draw_RGB_masks","text":"<pre><code>paz.backend.image.draw.draw_RGB_masks(image, points2D, points3D, object_sizes)\n</code></pre> <p>Draws RGB masks by transforming points3D to RGB space and putting in them in their 2D coordinates (points2D)</p> <p>Arguments</p> <ul> <li>image: Array (H, W, 3).</li> <li>points2D: Array (num_samples, num_points, 2)</li> <li>points3D: Array (num_samples, num_points, 3)</li> <li>object_sizes: Array (x_size, y_size, z_size)</li> </ul> <p>Returns</p> <p>Image array with drawn masks</p> <p>[source]</p>"},{"location":"backend/draw/#draw_human_pose6d","title":"draw_human_pose6D","text":"<pre><code>paz.backend.image.draw.draw_human_pose6D(image, rotation, translation, camaera_intrinsics)\n</code></pre> <p>Draw basis vectors for human pose 6D</p> <p>Arguments</p> <ul> <li>image: numpy array</li> <li>rotation: numpy array of size (3 x 3)</li> <li>translations: list of length 3</li> <li>camera_matrix: numpy array</li> </ul> <p>Returns</p> <ul> <li>image: numpy array        image with basis vectors of rotaion and translation.</li> </ul>"},{"location":"backend/groups/","title":"Groups","text":"<p>[source]</p>"},{"location":"backend/groups/#rotation_vector_to_quaternion","title":"rotation_vector_to_quaternion","text":"<pre><code>paz.backend.groups.quaternion.rotation_vector_to_quaternion(rotation_vector)\n</code></pre> <p>Transforms rotation vector into quaternion.</p> <p>Arguments</p> <ul> <li>rotation_vector: Numpy array of shape <code>[3]</code>.</li> </ul> <p>Returns</p> <p>Numpy array representing a quaternion having a shape <code>[4]</code>.</p> <p>[source]</p>"},{"location":"backend/groups/#homogenous_quaternion_to_rotation_matrix","title":"homogenous_quaternion_to_rotation_matrix","text":"<pre><code>paz.backend.groups.quaternion.homogenous_quaternion_to_rotation_matrix(quaternion)\n</code></pre> <p>Transforms quaternion to rotation matrix.</p> <p>Arguments</p> <ul> <li>quaternion: Array containing quaternion value [q1, q2, q3, w0].</li> </ul> <p>Returns</p> <p>Rotation matrix [3, 3].</p> <p>Note</p> <p>If quaternion is not a unit quaternion the rotation matrix is not unitary but still orthogonal i.e. the outputted rotation matrix is a scalar multiple of a rotation matrix.</p> <p>[source]</p>"},{"location":"backend/groups/#quaternion_to_rotation_matrix","title":"quaternion_to_rotation_matrix","text":"<pre><code>paz.backend.groups.quaternion.quaternion_to_rotation_matrix(quaternion)\n</code></pre> <p>Transforms quaternion to rotation matrix.</p> <p>Arguments</p> <ul> <li>quaternion: Array containing quaternion value [q1, q2, q3, w0].</li> </ul> <p>Returns</p> <p>Rotation matrix [3, 3].</p> <p>Note</p> <p>\"If the quaternion \"is not a unit quaternion then the homogeneous form is still a scalar multiple of a rotation matrix, while the inhomogeneous form is in general no longer an orthogonal matrix. This is why in numerical work the homogeneous form is to be preferred if distortion is to be avoided.\" wikipedia</p> <p>[source]</p>"},{"location":"backend/groups/#rotation_matrix_to_quaternion","title":"rotation_matrix_to_quaternion","text":"<pre><code>paz.backend.groups.quaternion.rotation_matrix_to_quaternion(rotation_matrix)\n</code></pre> <p>Transforms rotation matrix to quaternion.</p> <p>Arguments</p> <p>Rotation matrix [3, 3].</p> <p>Returns</p> <ul> <li>quaternion: Array containing quaternion value [q1, q2, q3, w0].</li> </ul> <p>[source]</p>"},{"location":"backend/groups/#get_quaternion_conjugate","title":"get_quaternion_conjugate","text":"<pre><code>paz.backend.groups.quaternion.get_quaternion_conjugate(quaternion)\n</code></pre> <p>Estimate conjugate of a quaternion.</p> <p>Arguments</p> <ul> <li>quaternion: Array containing quaternion value [q1, q2, q3, w0].</li> </ul> <p>Returns</p> <ul> <li>quaternion: Array containing quaternion value [-q1, -q2, -q3, w0].</li> </ul> <p>[source]</p>"},{"location":"backend/groups/#quaternions_to_rotation_matrices","title":"quaternions_to_rotation_matrices","text":"<pre><code>paz.backend.groups.quaternion.quaternions_to_rotation_matrices(quaternions)\n</code></pre> <p>Transform quaternion vectors to rotation matrix vector.</p> <p>Arguments</p> <p>quaternions [N, 4].</p> <p>Returns</p> <p>Rotated matrices [N, 3, 3]</p> <p>[source]</p>"},{"location":"backend/groups/#to_affine_matrix","title":"to_affine_matrix","text":"<pre><code>paz.backend.groups.SE3.to_affine_matrix(rotation_matrix, translation)\n</code></pre> <p>Builds affine matrix from rotation matrix and translation vector.</p> <p>Arguments</p> <ul> <li>rotation_matrix: Array (3, 3). Representing a rotation matrix.</li> <li>translation: Array (3). Translation vector.</li> </ul> <p>Returns</p> <p>Array (4, 4) representing an affine matrix.</p> <p>[source]</p>"},{"location":"backend/groups/#to_affine_matrices","title":"to_affine_matrices","text":"<pre><code>paz.backend.groups.SE3.to_affine_matrices(rotations, translations)\n</code></pre> <p>Construct affine matrices for rotation matrices vector and translation vector.</p> <p>Arguments</p> <ul> <li>ratations: Rotation matrix vector [N, 3, 3].</li> <li>translations: Translation vector [N, 3].</li> </ul> <p>Returns</p> <p>Transformation matrix [N, 4, 4]</p> <p>[source]</p>"},{"location":"backend/groups/#rotation_vector_to_rotation_matrix","title":"rotation_vector_to_rotation_matrix","text":"<pre><code>paz.backend.groups.SO3.rotation_vector_to_rotation_matrix(rotation_vector)\n</code></pre> <p>Transforms rotation vector (axis-angle) form to rotation matrix.</p> <p>Arguments</p> <ul> <li>rotation_vector: Array (3). Rotation vector in axis-angle form.</li> </ul> <p>Returns</p> <p>Array (3, 3) rotation matrix.</p> <p>[source]</p>"},{"location":"backend/groups/#build_rotation_matrix_x","title":"build_rotation_matrix_x","text":"<pre><code>paz.backend.groups.SO3.build_rotation_matrix_x(angle)\n</code></pre> <p>Builds rotation matrix in X axis.</p> <p>Arguments</p> <ul> <li>angle: Float. Angle in radians.</li> </ul> <p>Return</p> <p>Array (3, 3) rotation matrix in Z axis.</p> <p>[source]</p>"},{"location":"backend/groups/#build_rotation_matrix_y","title":"build_rotation_matrix_y","text":"<pre><code>paz.backend.groups.SO3.build_rotation_matrix_y(angle)\n</code></pre> <p>Builds rotation matrix in Y axis.</p> <p>Arguments</p> <ul> <li>angle: Float. Angle in radians.</li> </ul> <p>Return</p> <p>Array (3, 3) rotation matrix in Z axis.</p> <p>[source]</p>"},{"location":"backend/groups/#build_rotation_matrix_z","title":"build_rotation_matrix_z","text":"<pre><code>paz.backend.groups.SO3.build_rotation_matrix_z(angle)\n</code></pre> <p>Builds rotation matrix in Z axis.</p> <p>Arguments</p> <ul> <li>angle: Float. Angle in radians.</li> </ul> <p>Return</p> <p>Array (3, 3) rotation matrix in Z axis.</p> <p>[source]</p>"},{"location":"backend/groups/#compute_norm_so3","title":"compute_norm_SO3","text":"<pre><code>paz.backend.groups.SO3.compute_norm_SO3(rotation_mesh, rotation)\n</code></pre> <p>Computes norm between SO3 elements.</p> <p>Arguments</p> <ul> <li>rotation_mesh: Array (3, 3), rotation matrix.</li> <li>rotation: Array (3, 3), rotation matrix.</li> </ul> <p>Returns</p> <p>Scalar representing the distance between both rotation matrices.</p> <p>[source]</p>"},{"location":"backend/groups/#calculate_canonical_rotation","title":"calculate_canonical_rotation","text":"<pre><code>paz.backend.groups.SO3.calculate_canonical_rotation(rotation_mesh, rotations)\n</code></pre> <p>Returns the rotation matrix closest to rotation mesh.</p> <p>Arguments</p> <ul> <li>rotation_mesh: Array (3, 3), rotation matrix.</li> <li>rotations: List of array of (3, 3), rotation matrices.</li> </ul> <p>Returns</p> <p>Element of list closest to rotation mesh.</p> <p>[source]</p>"},{"location":"backend/groups/#rotation_matrix_to_axis_angle","title":"rotation_matrix_to_axis_angle","text":"<pre><code>paz.backend.groups.SO3.rotation_matrix_to_axis_angle(rotation_matrix)\n</code></pre> <p>Transforms rotation matrix to axis angle.</p> <p>Arguments</p> <p>Rotation matrix [3, 3].</p> <p>Returns</p> <ul> <li>axis_angle: Array containing axis angle represent [wx, wy, wz, theta].</li> </ul> <p>[source]</p>"},{"location":"backend/groups/#rotation_matrix_to_compact_axis_angle","title":"rotation_matrix_to_compact_axis_angle","text":"<pre><code>paz.backend.groups.SO3.rotation_matrix_to_compact_axis_angle(matrix)\n</code></pre> <p>Transforms rotation matrix to compact axis angle.</p> <p>Arguments</p> <p>Rotation matrix [3, 3].</p> <p>Returns</p> <p>compact axis_angle</p>"},{"location":"backend/heatmaps/","title":"Heatmaps","text":"<p>[source]</p>"},{"location":"backend/heatmaps/#get_keypoints_heatmap","title":"get_keypoints_heatmap","text":"<pre><code>paz.backend.heatmaps.get_keypoints_heatmap(heatmaps, num_keypoints, indices=None, axis=1)\n</code></pre> <p>Extract the heatmaps that only contains the keypoints.</p> <p>Arguments</p> <ul> <li>heatmaps: Numpy array of shape (1, 2*num_keypoints, H, W)</li> <li>num_keypoints: Int.</li> <li>indices: List. Indices of the heatmaps to extract.</li> <li>axis: Int.</li> </ul> <p>Returns</p> <ul> <li>keypoints: Numpy array of shape (1, num_keypoints, H, W)</li> </ul> <p>[source]</p>"},{"location":"backend/heatmaps/#get_tags_heatmap","title":"get_tags_heatmap","text":"<pre><code>paz.backend.heatmaps.get_tags_heatmap(heatmaps, num_keypoints, indices=None, axis=1)\n</code></pre> <p>Extract the heatmaps that only contains the tags.</p> <p>Arguments</p> <ul> <li>heatmaps: Numpy array of shape (1, 2*num_keypoints, H, W)</li> <li>num_keypoints: Int.</li> <li>indices: List. Indices of the heatmaps to extract.</li> <li>axis: Int.</li> </ul> <p>Returns</p> <ul> <li>tags: Numpy array of shape (1, num_keypoints, H, W)</li> </ul> <p>[source]</p>"},{"location":"backend/heatmaps/#get_keypoints_locations","title":"get_keypoints_locations","text":"<pre><code>paz.backend.heatmaps.get_keypoints_locations(indices, image_width)\n</code></pre> <p>Calculate the location of keypoints in an image.</p> <p>Arguments</p> <ul> <li>indices: Numpy array. Indices of the keypoints in the heatmap.</li> </ul> <p>Image width: Int.</p> <p>Returns</p> <ul> <li>coordinate: Numpy array. locations of keypoints</li> </ul> <p>[source]</p>"},{"location":"backend/heatmaps/#get_top_k_keypoints_numpy","title":"get_top_k_keypoints_numpy","text":"<pre><code>paz.backend.heatmaps.get_top_k_keypoints_numpy(heatmaps, k)\n</code></pre> <p>Numpy implementation of get_top_k_keypoints from heatmaps.</p> <p>Arguments</p> <ul> <li>heatmaps: Keypoints heatmaps. Numpy array of shape           (1, num_keypoints, H, W)</li> <li>k: Int. Maximum number of instances to return.</li> </ul> <p>Returns</p> <ul> <li>values: Numpy array. Value of heatmaps at top k keypoints</li> <li>indices: Numpy array. Indices of top k keypoints.</li> </ul> <p>[source]</p>"},{"location":"backend/heatmaps/#get_valid_detections","title":"get_valid_detections","text":"<pre><code>paz.backend.heatmaps.get_valid_detections(detection, detection_thresh)\n</code></pre> <p>Accept the keypoints whose score is greater than the detection threshold.</p> <p>Arguments</p> <ul> <li>detection: Numpy array. Contains the location, value, and</li> </ul> <p>tags of the keypoints</p> <ul> <li>detection_thresh: Float. Detection threshold for the keypoint</li> </ul>"},{"location":"backend/image/","title":"Image","text":"<p>[source]</p>"},{"location":"backend/image/#resize_image","title":"resize_image","text":"<pre><code>paz.backend.image.opencv_image.resize_image(image, size, method=1)\n</code></pre> <p>Resize image.</p> <p>Arguments</p> <ul> <li>image: Numpy array.</li> <li>size: List of two ints.</li> <li>method: Flag indicating interpolation method i.e.     paz.backend.image.CUBIC</li> </ul> <p>Returns</p> <p>Numpy array.</p> <p>[source]</p>"},{"location":"backend/image/#convert_color_space","title":"convert_color_space","text":"<pre><code>paz.backend.image.opencv_image.convert_color_space(image, flag)\n</code></pre> <p>Convert image to a different color space.</p> <p>Arguments</p> <ul> <li>image: Numpy array.</li> <li>flag: PAZ or openCV flag. e.g. paz.backend.image.RGB2BGR.</li> </ul> <p>Returns</p> <p>Numpy array.</p> <p>[source]</p>"},{"location":"backend/image/#load_image","title":"load_image","text":"<pre><code>paz.backend.image.opencv_image.load_image(filepath, num_channels=3)\n</code></pre> <p>Load image from a ''filepath''.</p> <p>Arguments</p> <ul> <li>filepath: String indicating full path to the image.</li> <li>num_channels: Int.</li> </ul> <p>Returns</p> <p>Numpy array.</p> <p>[source]</p>"},{"location":"backend/image/#show_image","title":"show_image","text":"<pre><code>paz.backend.image.opencv_image.show_image(image, name='image', wait=True)\n</code></pre> <p>Shows RGB image in an external window.</p> <p>Arguments</p> <ul> <li>image: Numpy array</li> <li>name: String indicating the window name.</li> <li>wait: Boolean. If ''True'' window stays open until user presses a key.     If ''False'' windows closes immediately.</li> </ul> <p>[source]</p>"},{"location":"backend/image/#warp_affine","title":"warp_affine","text":"<pre><code>paz.backend.image.opencv_image.warp_affine(image, matrix, fill_color=[0, 0, 0], size=None)\n</code></pre> <p>Transforms <code>image</code> using an affine <code>matrix</code> transformation.</p> <p>Arguments</p> <ul> <li>image: Numpy array.</li> <li>matrix: Numpy array of shape (2,3) indicating affine transformation.</li> <li>fill_color: List/tuple representing BGR use for filling empty space.</li> </ul> <p>[source]</p>"},{"location":"backend/image/#write_image","title":"write_image","text":"<pre><code>paz.backend.image.opencv_image.write_image(filepath, image)\n</code></pre> <p>Writes an image inside <code>filepath</code>. If <code>filepath</code> doesn't exist it makes a directory. If <code>image</code> has three channels the image is converted into BGR and then written. This is done such that this function compatible with <code>load_image</code>.</p> <p>Arguments</p> <ul> <li>filepath: String with image path. It should include postfix e.g. .png</li> <li>image: Numpy array.</li> </ul> <p>[source]</p>"},{"location":"backend/image/#gaussian_image_blur","title":"gaussian_image_blur","text":"<pre><code>paz.backend.image.opencv_image.gaussian_image_blur(image, kernel_size=(5, 5))\n</code></pre> <p>Applies Gaussian blur to an image.</p> <p>Arguments</p> <ul> <li>image: Numpy array of shape ''(H, W, 4)''.</li> <li>kernel_size: List of two ints e.g. ''(5, 5)''.</li> </ul> <p>Returns</p> <p>Numpy array</p> <p>[source]</p>"},{"location":"backend/image/#median_image_blur","title":"median_image_blur","text":"<pre><code>paz.backend.image.opencv_image.median_image_blur(image, apperture=5)\n</code></pre> <p>Applies median blur to an image.</p> <p>Arguments</p> <ul> <li>image: Numpy array of shape ''(H, W, 3)''.</li> </ul> <p>apperture. Int.</p> <p>Returns</p> <p>Numpy array.</p> <p>[source]</p>"},{"location":"backend/image/#get_rotation_matrix","title":"get_rotation_matrix","text":"<pre><code>paz.backend.image.opencv_image.get_rotation_matrix(center, degrees, scale=1.0)\n</code></pre> <p>Returns a 2D rotation matrix.</p> <p>Arguments</p> <ul> <li>center: List of two integer values.</li> <li>degrees: Float indicating the angle in degrees.</li> </ul> <p>Returns</p> <p>Numpy array</p> <p>[source]</p>"},{"location":"backend/image/#cast_image","title":"cast_image","text":"<pre><code>paz.backend.image.image.cast_image(image, dtype)\n</code></pre> <p>Casts an image into a different type</p> <p>Arguments</p> <ul> <li>image: Numpy array.</li> <li>dtype: String or np.dtype.</li> </ul> <p>Returns</p> <p>Numpy array.</p> <p>[source]</p>"},{"location":"backend/image/#random_saturation","title":"random_saturation","text":"<pre><code>paz.backend.image.image.random_saturation(image, lower=0.3, upper=1.5)\n</code></pre> <p>Applies random saturation to an RGB image.</p> <p>Arguments</p> <ul> <li>image: Numpy array representing an image RGB format.</li> <li>lower: Float.</li> <li>upper: Float.</li> </ul> <p>[source]</p>"},{"location":"backend/image/#random_brightness","title":"random_brightness","text":"<pre><code>paz.backend.image.image.random_brightness(image, delta=32)\n</code></pre> <p>Applies random brightness to an RGB image.</p> <p>Arguments</p> <ul> <li>image: Numpy array representing an image RGB format.</li> <li>delta: Int.</li> </ul> <p>[source]</p>"},{"location":"backend/image/#random_contrast","title":"random_contrast","text":"<pre><code>paz.backend.image.image.random_contrast(image, lower=0.5, upper=1.5)\n</code></pre> <p>Applies random contrast to an RGB image.</p> <p>Arguments</p> <ul> <li>image: Numpy array representing an image RGB format.</li> <li>lower: Float.</li> <li>upper: Float.</li> </ul> <p>[source]</p>"},{"location":"backend/image/#random_hue","title":"random_hue","text":"<pre><code>paz.backend.image.image.random_hue(image, delta=18)\n</code></pre> <p>Applies random hue to an RGB image.</p> <p>Arguments</p> <ul> <li>image: Numpy array representing an image RGB format.</li> <li>delta: Int.</li> </ul> <p>[source]</p>"},{"location":"backend/image/#flip_left_right","title":"flip_left_right","text":"<pre><code>paz.backend.image.image.flip_left_right(image)\n</code></pre> <p>Flips an image left and right.</p> <p>Arguments</p> <ul> <li>image: Numpy array.</li> </ul> <p>[source]</p>"},{"location":"backend/image/#random_flip_left_right","title":"random_flip_left_right","text":"<pre><code>paz.backend.image.image.random_flip_left_right(image)\n</code></pre> <p>Applies random left or right flip.</p> <p>Arguments</p> <ul> <li>image: Numpy array.</li> </ul> <p>[source]</p>"},{"location":"backend/image/#crop_image","title":"crop_image","text":"<pre><code>paz.backend.image.image.crop_image(image, crop_box)\n</code></pre> <p>Resize image.</p> <p>Arguments</p> <ul> <li>image: Numpy array.</li> <li>crop_box: List of four ints.</li> </ul> <p>Returns</p> <p>Numpy array.</p> <p>[source]</p>"},{"location":"backend/image/#image_to_normalized_device_coordinates","title":"image_to_normalized_device_coordinates","text":"<pre><code>paz.backend.image.image.image_to_normalized_device_coordinates(image)\n</code></pre> <p>Map image value from [0, 255] -&gt; [-1, 1].</p> <p>[source]</p>"},{"location":"backend/image/#normalized_device_coordinates_to_image","title":"normalized_device_coordinates_to_image","text":"<pre><code>paz.backend.image.image.normalized_device_coordinates_to_image(image)\n</code></pre> <p>Map normalized value from [-1, 1] -&gt; [0, 255].</p> <p>[source]</p>"},{"location":"backend/image/#random_shape_crop","title":"random_shape_crop","text":"<pre><code>paz.backend.image.image.random_shape_crop(image, shape)\n</code></pre> <p>Randomly crops an image of the given <code>shape</code>.</p> <p>Arguments</p> <ul> <li>image: Numpy array.</li> <li>shape: List of two ints ''(H, W)''.</li> </ul> <p>Returns</p> <p>Numpy array of cropped image.</p> <p>[source]</p>"},{"location":"backend/image/#make_random_plain_image","title":"make_random_plain_image","text":"<pre><code>paz.backend.image.image.make_random_plain_image(shape)\n</code></pre> <p>Makes random plain image by sampling three random values.</p> <p>Arguments</p> <ul> <li>shape: Image shape e.g. ''(H, W, 3)''.</li> </ul> <p>Returns</p> <p>Numpy array of shape ''(H, W, 3)''.</p> <p>[source]</p>"},{"location":"backend/image/#blend_alpha_channel","title":"blend_alpha_channel","text":"<pre><code>paz.backend.image.image.blend_alpha_channel(image, background)\n</code></pre> <p>Blends image with background using an alpha channel.</p> <p>Arguments</p> <ul> <li>image: Numpy array with alpha channel. Shape must be ''(H, W, 4)''</li> <li>background: Numpy array of shape ''(H, W, 3)''.</li> </ul> <p>[source]</p>"},{"location":"backend/image/#concatenate_alpha_mask","title":"concatenate_alpha_mask","text":"<pre><code>paz.backend.image.image.concatenate_alpha_mask(image, alpha_mask)\n</code></pre> <p>Concatenates alpha mask to image.</p> <p>Arguments</p> <ul> <li>image: Numpy array of shape ''(H, W, 3)''.</li> <li>alpha_mask: Numpy array array of shape ''(H, W)''.</li> </ul> <p>Returns</p> <p>Numpy array of shape ''(H, W, 4)''.</p> <p>[source]</p>"},{"location":"backend/image/#split_and_normalize_alpha_channel","title":"split_and_normalize_alpha_channel","text":"<pre><code>paz.backend.image.image.split_and_normalize_alpha_channel(image)\n</code></pre> <p>Splits alpha channel from an RGBA image and normalizes alpha channel.</p> <p>Arguments</p> <ul> <li>image: Numpy array of shape ''(H, W, 4)''.</li> </ul> <p>Returns</p> <p>List of two numpy arrays containing respectively the image and the     alpha channel.</p> <p>[source]</p>"},{"location":"backend/image/#random_image_blur","title":"random_image_blur","text":"<pre><code>paz.backend.image.image.random_image_blur(image)\n</code></pre> <p>Applies random choice blur.</p> <p>Arguments</p> <ul> <li>image: Numpy array of shape ''(H, W, 3)''.</li> </ul> <p>Returns</p> <p>Numpy array.</p> <p>[source]</p>"},{"location":"backend/image/#translate_image","title":"translate_image","text":"<pre><code>paz.backend.image.image.translate_image(image, translation, fill_color)\n</code></pre> <p>Translate image.</p> <p>Arguments</p> <ul> <li>image: Numpy array.</li> <li>translation: A list of length two indicating the x,y translation values</li> <li>fill_color: List of three floats representing a color.</li> </ul> <p>Returns</p> <p>Numpy array</p> <p>[source]</p>"},{"location":"backend/image/#sample_scaled_translation","title":"sample_scaled_translation","text":"<pre><code>paz.backend.image.image.sample_scaled_translation(delta_scale, image_shape)\n</code></pre> <p>Samples a scaled translation from a uniform distribution.</p> <p>Arguments</p> <ul> <li>delta_scale: List with two elements having the normalized deltas.     e.g. ''[.25, .25]''.</li> <li>image_shape: List containing the height and width of the image.</li> </ul> <p>[source]</p>"},{"location":"backend/image/#replace_lower_than_threshold","title":"replace_lower_than_threshold","text":"<pre><code>paz.backend.image.image.replace_lower_than_threshold(source, threshold=0.001, replacement=0.0)\n</code></pre> <p>Replace values from source that are lower than the given threshold. This function doesn't create a new array but does replacement in place.</p> <p>Arguments</p> <ul> <li>source: Array.</li> <li>threshold: Float. Values lower than this value will be replaced.</li> <li>replacement: Float. Value taken by elements lower than threshold.</li> </ul> <p>Returns</p> <p>Array of same shape as source.</p> <p>[source]</p>"},{"location":"backend/image/#normalize_min_max","title":"normalize_min_max","text":"<pre><code>paz.backend.image.image.normalize_min_max(x, x_min, x_max)\n</code></pre> <p>Normalized data using it's maximum and minimum values</p> <p>Arguments</p> <ul> <li>x: array</li> <li>x_min: minimum value of x</li> <li>x_max: maximum value of x</li> </ul> <p>Returns</p> <p>min-max normalized data</p> <p>[source]</p>"},{"location":"backend/image/#sample_scaled_translation_1","title":"sample_scaled_translation","text":"<pre><code>paz.backend.image.image.sample_scaled_translation(delta_scale, image_shape)\n</code></pre> <p>Samples a scaled translation from a uniform distribution.</p> <p>Arguments</p> <ul> <li>delta_scale: List with two elements having the normalized deltas.     e.g. ''[.25, .25]''.</li> <li>image_shape: List containing the height and width of the image.</li> </ul> <p>[source]</p>"},{"location":"backend/image/#get_rotation_matrix_1","title":"get_rotation_matrix","text":"<pre><code>paz.backend.image.opencv_image.get_rotation_matrix(center, degrees, scale=1.0)\n</code></pre> <p>Returns a 2D rotation matrix.</p> <p>Arguments</p> <ul> <li>center: List of two integer values.</li> <li>degrees: Float indicating the angle in degrees.</li> </ul> <p>Returns</p> <p>Numpy array</p> <p>[source]</p>"},{"location":"backend/image/#calculate_image_center","title":"calculate_image_center","text":"<pre><code>paz.backend.image.image.calculate_image_center(image)\n</code></pre> <p>Return image center.</p> <p>Arguments</p> <ul> <li>image: Numpy array.</li> </ul> <p>Returns</p> <p>image center.</p> <p>[source]</p>"},{"location":"backend/image/#get_affine_transform","title":"get_affine_transform","text":"<pre><code>paz.backend.image.opencv_image.get_affine_transform(source_points, destination_points)\n</code></pre> <p>Return the transformation matrix.</p> <p>Arguments</p> <ul> <li>source_points: Numpy array.</li> <li>destination_points: Numpy array.</li> </ul> <p>Returns</p> <p>Transformation matrix.</p> <p>[source]</p>"},{"location":"backend/image/#get_scaling_factor","title":"get_scaling_factor","text":"<pre><code>paz.backend.image.image.get_scaling_factor(image, scale=1, shape=(128, 128))\n</code></pre> <p>Return scaling factor for the image.</p> <p>Arguments</p> <ul> <li>image: Numpy array.</li> <li>scale: Int.</li> <li>shape: Tuple of integers. eg. (128, 128)</li> </ul> <p>Returns</p> <p>scaling factor: Numpy array of size 2</p> <p>[source]</p>"},{"location":"backend/image/#scale_resize","title":"scale_resize","text":"<pre><code>paz.backend.image.image.scale_resize(image, image_size)\n</code></pre> <p>Resizes and crops image by returning the scales to original image.</p> <p>Args: image: Numpy array, raw image. image_size: Int, size of the image.</p> <p>Returns: Tuple: output_image, image_scale.</p>"},{"location":"backend/keypoints/","title":"Keypoints","text":"<p>Backend functionality for 2D keypoints</p> <p>[source]</p>"},{"location":"backend/keypoints/#build_cube_points3d","title":"build_cube_points3D","text":"<pre><code>paz.backend.keypoints.build_cube_points3D(width, height, depth)\n</code></pre> <p>Build the 3D points of a cube in the openCV coordinate system: 4--------1 /|       /| / |      / | 3--------2  | |  8_|5 | /      | / |/       |/ 7--------6</p> <p>Z (depth) / /_____X (width) | | Y (height)</p> <p>Arguments</p> <ul> <li>height: float, height of the 3D box.</li> <li>width: float,  width of the 3D box.</li> <li>depth: float,  width of the 3D box.</li> </ul> <p>Returns</p> <p>Numpy array of shape ``(8, 3)'' corresponding to 3D keypoints of a cube</p> <p>[source]</p>"},{"location":"backend/keypoints/#normalize_keypoints2d","title":"normalize_keypoints2D","text":"<pre><code>paz.backend.keypoints.normalize_keypoints2D(points2D, height, width)\n</code></pre> <p>Transform points2D in image coordinates to normalized coordinates i.e. [U, V] -&gt; [-1, 1]. UV have maximum values of [W, H] respectively.</p> <p>Image plane</p> <p>width (0,0)--------&gt;  (U) | height | | v</p> <p>(V)</p> <p>Arguments</p> <ul> <li>points2D: Numpy array of shape (num_keypoints, 2).</li> <li>height: Int. Height of the image</li> <li>width: Int. Width of the image</li> </ul> <p>Returns</p> <p>Numpy array of shape (num_keypoints, 2).</p> <p>[source]</p>"},{"location":"backend/keypoints/#denormalize_keypoints2d","title":"denormalize_keypoints2D","text":"<pre><code>paz.backend.keypoints.denormalize_keypoints2D(points2D, height, width)\n</code></pre> <p>Transform nomralized points2D to image UV coordinates i.e. [-1, 1] -&gt; [U, V]. UV have maximum values of [W, H] respectively.</p> <p>Image plane</p> <p>(0,0)--------&gt;  (U) | | | v</p> <p>(V)</p> <p>Arguments</p> <ul> <li>points2D: Numpy array of shape (num_keypoints, 2).</li> <li>height: Int. Height of the image</li> <li>width: Int. Width of the image</li> </ul> <p>Returns</p> <p>Numpy array of shape (num_keypoints, 2).</p> <p>[source]</p>"},{"location":"backend/keypoints/#project_to_image","title":"project_to_image","text":"<pre><code>paz.backend.keypoints.project_to_image(rotation, translation, points3D, camera_intrinsics)\n</code></pre> <p>Project points3D to image plane using a perspective transformation.</p> <p>Image plane</p> <p>(0,0)--------&gt;  (U) | | | v</p> <p>(V)</p> <p>Arguments</p> <ul> <li>rotation: Array (3, 3). Rotation matrix (Rco).</li> <li>translation: Array (3). Translation (Tco).</li> <li>points3D: Array (num_points, 3). Points 3D in object frame.</li> <li>camera_intrinsics: Array of shape (3, 3). Diagonal elements represent     focal lenghts and last column the image center translation.</li> </ul> <p>Returns</p> <p>Array (num_points, 2) in UV image space.</p> <p>[source]</p>"},{"location":"backend/keypoints/#solve_pnp_ransac","title":"solve_PnP_RANSAC","text":"<pre><code>paz.backend.keypoints.solve_PnP_RANSAC(object_points3D, image_points2D, camera_intrinsics, inlier_threshold=5, num_iterations=100)\n</code></pre> <p>Returns rotation (Roc) and translation (Toc) vectors that transform 3D points in object frame to camera frame.</p> <p>O------------O /|           /| / |          / | O------------O  | |  |    z    |  | |  O_|_|__O |  /    |___y|  /   object | /    /     | /  coordinates |/    x      |/ O------------O</p> <p>Z                | /                 | Rco, Tco /_____X     &lt;------| | |    camera Y  coordinates</p> <p>Arguments</p> <ul> <li>object_points3D: Array (num_points, 3). Points 3D in object reference     frame. Represented as (0) in image above.</li> <li>image_points2D: Array (num_points, 2). Points in 2D in camera UV space.</li> <li>camera_intrinsics: Array of shape (3, 3). Diagonal elements represent     focal lenghts and last column the image center translation.</li> <li>inlier_threshold: Number of inliers for RANSAC method.</li> <li>num_iterations: Maximum number of iterations.</li> </ul> <p>Returns</p> <p>Rotation vector in axis-angle form (3) and translation vector (3).</p> <p>[source]</p>"},{"location":"backend/keypoints/#arguments_to_image_points2d","title":"arguments_to_image_points2D","text":"<pre><code>paz.backend.keypoints.arguments_to_image_points2D(row_args, col_args)\n</code></pre> <p>Convert array arguments into UV coordinates.</p> <p>Image plane</p> <p>(0,0)--------&gt;  (U) | | | v</p> <p>(V)</p> <p>Arguments</p> <ul> <li>row_args: Array (num_rows).</li> <li>col_args: Array (num_cols).</li> </ul> <p>Returns</p> <p>Array (num_cols, num_rows) representing points2D in UV space.</p> <p>Notes</p> <p>Arguments are row args (V) and col args (U). Image points are in UV     coordinates; thus, we concatenate them in that order     i.e. [col_args, row_args]</p> <p>[source]</p>"},{"location":"backend/keypoints/#cascade_classifier","title":"cascade_classifier","text":"<pre><code>paz.backend.keypoints.cascade_classifier(path)\n</code></pre> <p>OpenCV Cascade classifier.</p> <p>Arguments</p> <ul> <li>path: String. Path to default openCV XML format.</li> </ul> <p>Returns</p> <p>OpenCV classifier with <code>detectMultiScale</code> for inference..</p> <p>[source]</p>"},{"location":"backend/keypoints/#project_points3d","title":"project_points3D","text":"<pre><code>paz.backend.keypoints.project_points3D(points3D, pose6D, camera)\n</code></pre> <p>Projects 3D points into a specific pose.</p> <p>Arguments</p> <ul> <li>points3D: Numpy array of shape <code>(num_points, 3)</code>.</li> <li>pose6D: An instance of <code>paz.abstract.Pose6D</code>.</li> <li>camera: An instance of <code>paz.backend.Camera</code> object.</li> </ul> <p>Returns</p> <p>Numpy array of shape <code>(num_points, 2)</code></p> <p>[source]</p>"},{"location":"backend/keypoints/#solve_pnp","title":"solve_PNP","text":"<pre><code>paz.backend.keypoints.solve_PNP(points3D, points2D, camera, solver)\n</code></pre> <p>Calculates 6D pose from 3D points and 2D keypoints correspondences.</p> <p>Arguments</p> <ul> <li>points3D: Numpy array of shape <code>(num_points, 3)</code>.     3D points known in advance.</li> <li>points2D: Numpy array of shape <code>(num_points, 2)</code>.     Predicted 2D keypoints of object.</li> <li>camera: Instance of ''paz.backend.Camera'' containing as properties     the ''camera_intrinsics'' a Numpy array of shape ''(3, 3)''     usually calculated from the openCV ''calibrateCamera'' function,     and the ''distortion'' a Numpy array of shape ''(5)'' in which the     elements are usually obtained from the openCV     ''calibrateCamera'' function.</li> <li>solver: Flag from e.g openCV.SOLVEPNP_UPNP.</li> <li>distortion: Numpy array of shape of 5 elements calculated from     the openCV calibrateCamera function.</li> </ul> <p>Returns</p> <p>A list containing success flag, rotation and translation components of the 6D pose.</p> <p>[source]</p>"},{"location":"backend/keypoints/#translate_keypoints","title":"translate_keypoints","text":"<pre><code>paz.backend.keypoints.translate_keypoints(keypoints, translation)\n</code></pre> <p>Translate keypoints.</p> <p>Arguments</p> <ul> <li>kepoints: Numpy array of shape <code>(num_keypoints, 2)</code>.</li> <li>translation: A list of length two indicating the x,y translation values</li> </ul> <p>Returns</p> <p>Numpy array</p> <p>[source]</p>"},{"location":"backend/keypoints/#rotate_point2d","title":"rotate_point2D","text":"<pre><code>paz.backend.keypoints.rotate_point2D(point2D, rotation_angle)\n</code></pre> <p>Rotate keypoint.</p> <p>Arguments</p> <ul> <li>point2D: keypoint [x, y]</li> </ul> <p>rotation angle: Int. Angle of rotation.</p> <p>Returns</p> <p>List of x and y rotated points</p> <p>[source]</p>"},{"location":"backend/keypoints/#transform_keypoint","title":"transform_keypoint","text":"<pre><code>paz.backend.keypoints.transform_keypoint(keypoint, transform)\n</code></pre> <p>Transform keypoint.</p> <p>Arguments</p> <ul> <li>keypoint2D: keypoint [x, y]</li> <li>transform: Array. Transformation matrix</li> </ul> <p>[source]</p>"},{"location":"backend/keypoints/#add_offset_to_point","title":"add_offset_to_point","text":"<pre><code>paz.backend.keypoints.add_offset_to_point(keypoint_location, offset=0)\n</code></pre> <p>Add offset to keypoint location</p> <p>Arguments</p> <ul> <li>keypoint_location: keypoint [y, x]</li> <li>offset: Float.</li> </ul> <p>[source]</p>"},{"location":"backend/keypoints/#translate_points2d_origin","title":"translate_points2D_origin","text":"<pre><code>paz.backend.keypoints.translate_points2D_origin(points2D, coordinates)\n</code></pre> <p>Translates points2D to a different origin</p> <p>Arguments</p> <ul> <li>points2D: Array (num_points, 2)</li> <li>coordinates: Array (4) containing (x_min, y_min, x_max, y_max)</li> </ul> <p>Returns</p> <p>Translated points2D array (num_points, 2)</p> <p>[source]</p>"},{"location":"backend/keypoints/#flip_keypoints_left_right","title":"flip_keypoints_left_right","text":"<pre><code>paz.backend.keypoints.flip_keypoints_left_right(keypoints, image_size=(32, 32))\n</code></pre> <p>Flip the detected 2D keypoints left to right.</p> <p>Arguments</p> <ul> <li>keypoints: Array</li> <li>image_size: list/tuple</li> <li>axis: int</li> </ul> <p>Returns</p> <ul> <li>flipped_keypoints: Numpy array</li> </ul> <p>[source]</p>"},{"location":"backend/keypoints/#compute_orientation_vector","title":"compute_orientation_vector","text":"<pre><code>paz.backend.keypoints.compute_orientation_vector(keypoints3D, parents)\n</code></pre> <p>Compute bone orientations from joint coordinates (child joint - parent joint). The returned vectors are normalized. For the root joint, it will be a zero vector.</p> <p>Arguments</p> <p>keypoints3D : Numpy array [num_keypoints, 3]. Joint coordinates. - parents: Parents of the keypoints from kinematic chain</p> <p>Returns</p> <p>Array [num_keypoints, 3]. The unit vectors from each child joint to its parent joint. For the root joint, it's are zero vector.</p> <p>[source]</p>"},{"location":"backend/keypoints/#rotate_keypoints3d","title":"rotate_keypoints3D","text":"<pre><code>paz.backend.keypoints.rotate_keypoints3D(rotation_matrix, keypoints)\n</code></pre> <p>Rotatate the keypoints by using rotation matrix</p> <p>Arguments</p> <p>Rotation matrix [N, 3, 3]. keypoints [N, 3]</p> <p>Returns</p> <p>Rotated keypoints [N, 3]</p> <p>[source]</p>"},{"location":"backend/keypoints/#flip_along_x_axis","title":"flip_along_x_axis","text":"<pre><code>paz.backend.keypoints.flip_along_x_axis(keypoints, axis=0)\n</code></pre> <p>Flip the keypoints along the x axis.</p> <p>Arguments</p> <ul> <li>keypoints: Array</li> <li>axis: int/list</li> </ul> <p>Returns</p> <p>Flipped keypoints: Array</p>"},{"location":"backend/keypoints/#_1","title":"Keypoints","text":"<p>[source]</p>"},{"location":"backend/keypoints/#uv_to_vu","title":"uv_to_vu","text":"<pre><code>paz.backend.keypoints.uv_to_vu(keypoints)\n</code></pre> <p>Flips the uv coordinates to vu.</p> <p>Arguments</p> <ul> <li>keypoints: Array.</li> </ul> <p>[source]</p>"},{"location":"backend/keypoints/#standardize","title":"standardize","text":"<pre><code>paz.backend.keypoints.standardize(data, mean, scale)\n</code></pre> <p>It takes the data the mean and the standard deviation and returns the standardized data</p> <p>Arguments</p> <ul> <li>data: nxd matrix to normalize</li> <li>mean: Array of means</li> <li>scale: standard deviation</li> </ul> <p>Returns</p> <p>standardized poses2D</p>"},{"location":"backend/keypoints/#_2","title":"Keypoints","text":"<p>[source]</p>"},{"location":"backend/keypoints/#destandardize","title":"destandardize","text":"<pre><code>paz.backend.keypoints.destandardize(data, mean, scale)\n</code></pre> <p>It takes the standardized data the mean and the standard deviation and returns the destandardized data</p> <p>Arguments</p> <ul> <li>data: nxd matrix to unnormalize</li> <li>mean: Array of means</li> <li>scale: standard deviation</li> </ul> <p>Returns</p> <p>destandardized poses3D</p> <p>[source]</p>"},{"location":"backend/keypoints/#initialize_translation","title":"initialize_translation","text":"<pre><code>paz.backend.keypoints.initialize_translation(joints2D, camera_intrinsics, ratio)\n</code></pre> <p>Computes initial 3D translation of root joint</p> <p>Arguments</p> <ul> <li>joints2D: 2D root joint from HigherHRNet</li> <li>camera_intrinsics: camera intrinsic parameters</li> <li>ratio: ration of sum of 3D bones to 2D bones</li> </ul> <p>Returns</p> <p>Array of initial estimate of the global position of the root joint in 3D</p> <p>[source]</p>"},{"location":"backend/keypoints/#solve_least_squares","title":"solve_least_squares","text":"<pre><code>paz.backend.keypoints.solve_least_squares(solver, compute_joints_distance, initial_joints_translation, joints3D, poses2D, camera_intrinsics)\n</code></pre> <p>Solve the least squares</p> <p>Arguments</p> <ul> <li>solver: from scipy.optimize import least_squares</li> <li>compute_joints_distance: global_pose.compute_joints_distance</li> <li>initial_root_translation: initial 3D translation of root joint</li> <li>joints3D: 16 moving joints in 3D</li> <li>poses2d: 2D poses</li> <li>camera_intrinsics: camera intrinsic parameters</li> </ul> <p>Returns optimal translation of root joint for each person</p> <p>[source]</p>"},{"location":"backend/keypoints/#get_bones_length","title":"get_bones_length","text":"<pre><code>paz.backend.keypoints.get_bones_length(poses2D, poses3D, start_joints, end_joints=[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15])\n</code></pre> <p>Computes sum of bone lengths in 3D</p>"},{"location":"backend/keypoints/#arguments","title":"Arguments","text":"<p>poses3D: list of predicted poses in 3D (Nx16x3) poses2D: list of poses in 2D    (Nx32)</p>"},{"location":"backend/keypoints/#returns","title":"Returns","text":"<p>sum_bones2D: array of sum of length of all bones in the 2D skeleton sum_bones3D: array of sum of length of all bones in the 3D skeleton</p> <p>[source]</p>"},{"location":"backend/keypoints/#compute_reprojection_error","title":"compute_reprojection_error","text":"<pre><code>paz.backend.keypoints.compute_reprojection_error(initial_translation, keypoints3D, keypoints2D, camera_intrinsics)\n</code></pre> <p>compute distance between each person joints</p> <p>Arguments</p> <ul> <li>initial_translation: initial guess of position of joint</li> <li>keypoints3D: 3D keypoints to be optimized (Nx16x3)</li> <li>keypoints2D: 2D keypoints (Nx32)</li> <li>camera_inrinsics: camera intrinsic parameters</li> </ul> <p>Returns</p> <ul> <li>person_sum: sum of L2 distances between each joint per person</li> </ul> <p>[source]</p>"},{"location":"backend/keypoints/#merge_into_mean","title":"merge_into_mean","text":"<pre><code>paz.backend.keypoints.merge_into_mean(keypoints2D, args_to_mean)\n</code></pre> <p>merge keypoints and take the mean</p> <p>Arguments:</p> <pre><code> keypoints2D: keypoints2D (Nx17x2)\n args_to_mean: dict of joint indices\n</code></pre> <p>Returns:</p> <pre><code> keypoints2D: keypoints2D after merging\n</code></pre> <p>[source]</p>"},{"location":"backend/keypoints/#filter_keypoints","title":"filter_keypoints","text":"<pre><code>paz.backend.keypoints.filter_keypoints(keypoints, args_to_joints)\n</code></pre> <p>filter keypoints.</p> <p>Arguments</p> <ul> <li>keypoints: points in camera coordinates</li> <li>args_to_joints: Array of joints indices</li> </ul> <p>Returns</p> <p>filtered keypoints</p>"},{"location":"backend/keypoints/#_3","title":"Keypoints","text":"<p>[source]</p>"},{"location":"backend/keypoints/#filter_keypoints3d","title":"filter_keypoints3D","text":"<pre><code>paz.backend.keypoints.filter_keypoints3D(keypoints3D, args_to_joints3D)\n</code></pre> <p>Selects 16 moving joints (Neck/Nose excluded) from 32 predicted joints in 3D</p> <p>Arguments</p> <ul> <li>keypoints3D: Nx96 points in camera coordinates</li> <li>args_to_joints3D: list of indices</li> </ul> <p>Returns</p> <ul> <li>filtered_joints_3D: Nx48 points (moving joints)</li> </ul> <p>[source]</p>"},{"location":"backend/keypoints/#filter_keypoints2d","title":"filter_keypoints2D","text":"<pre><code>paz.backend.keypoints.filter_keypoints2D(keypoints2D, args_to_mean, h36m_to_coco_joints2D)\n</code></pre> <p>Selects 16 moving joints (Neck/Nose excluded) from 17 predicted joints in 2D</p> <p>Arguments</p> <ul> <li>keypoints3D: Nx17x2 points in camera coordinates</li> <li>args_to_mean: keypoints indices</li> <li>h36m_to_coco_joints2D: human36m dataset list of joints indices</li> </ul> <p>Returns</p> <ul> <li>joints2D: Nx32 points (moving joints)</li> </ul> <p>[source]</p>"},{"location":"backend/keypoints/#compute_optimized_pose3d","title":"compute_optimized_pose3D","text":"<pre><code>paz.backend.keypoints.compute_optimized_pose3D(keypoints3D, joint_translation, camera_intrinsics)\n</code></pre> <p>Compute the optimized 3D pose</p> <p>Arguments</p> <ul> <li>keypoints3D: 3D keypoints</li> <li>joint_translation: np array joints translation</li> <li>camera_intrinsics: camera intrinsics parameters</li> </ul> <p>Returns</p> <ul> <li>optimized_poses3D: np array of optimized posed3D</li> </ul> <p>[source]</p>"},{"location":"backend/keypoints/#human_pose3d_to_pose6d","title":"human_pose3D_to_pose6D","text":"<pre><code>paz.backend.keypoints.human_pose3D_to_pose6D(poses3D)\n</code></pre> <p>Estiate human pose 6D of the root joint from 3D pose of human joints.</p> <p>Arguments</p> <p>poses3D: numpy array       3D pose of human joint</p> <p>return</p> <p>rotation_matrix: numpy array              rotation of human root joint translation: list          translation of human root joint</p>"},{"location":"backend/quaternion/","title":"Quaternions","text":"<p>Backend functionality for quaternions</p> <p>{{autogenerated}}</p>"},{"location":"backend/render/","title":"Render","text":"<p>[source]</p>"},{"location":"backend/render/#compute_modelview_matrices","title":"compute_modelview_matrices","text":"<pre><code>paz.backend.render.compute_modelview_matrices(camera_origin, world_origin, roll=None, translate=None)\n</code></pre> <p>Compute model-view matrices from camera to origin and origin to camera.</p> <p>Arguments</p> <ul> <li>camera_origin: Numpy-array of length 3 determining the camera origin</li> <li>world_origin: Numpy-array of length 3 determining the world origin</li> <li>roll: <code>None</code> or float. If <code>None</code> no roll is performed. If float</li> </ul> <p>value should be between [0, 2*pi)</p> <p>Returns</p> <p>Transformation from camera to world and world to camera.</p> <p>[source]</p>"},{"location":"backend/render/#get_look_at_transform","title":"get_look_at_transform","text":"<pre><code>paz.backend.render.get_look_at_transform(camera_position, target_position)\n</code></pre> <p>Make transformation from target position to camera position with orientation looking at the target position.</p> <p>Arguments</p> <ul> <li>camera_position: Numpy-array of length 3. Camera position.</li> <li>target_position: Numpy-array of length 3. Target position.</li> </ul> <p>[source]</p>"},{"location":"backend/render/#random_perturbation","title":"random_perturbation","text":"<pre><code>paz.backend.render.random_perturbation(localization, shift)\n</code></pre> <p>Adds noise to 'localization' vector coordinates.</p> <p>Arguments</p> <ul> <li>localization: List of 3 floats.</li> <li>shift: Float indicating a uniform distribution [-shift, shift].</li> </ul> <p>Returns</p> <p>perturbed localization</p> <p>[source]</p>"},{"location":"backend/render/#random_translation","title":"random_translation","text":"<pre><code>paz.backend.render.random_translation(localization, shift)\n</code></pre> <p>Adds noise to 'localization' vector coordinates.</p> <p>Arguments</p> <ul> <li>localization: List of 3 floats.</li> <li>shift: Float indicating a uniform distribution [-shift, shift]. Returns</li> </ul> <p>perturbed localization</p> <p>[source]</p>"},{"location":"backend/render/#roll_camera","title":"roll_camera","text":"<pre><code>paz.backend.render.roll_camera(world_to_camera, angle)\n</code></pre> <p>Roll camera coordinate system.</p> <p>Arguments:</p> <ul> <li>world_to_camera: Numpy array containing the affine transformation.</li> <li>max_roll: 'None' or float. If None, the camera is not rolled.     If float it should be a value between [0, 2*pi)</li> </ul> <p>[source]</p>"},{"location":"backend/render/#sample_point_in_full_sphere","title":"sample_point_in_full_sphere","text":"<pre><code>paz.backend.render.sample_point_in_full_sphere(distance=1.0)\n</code></pre> <p>Get a point of the top of the unit sphere.</p> <p>Arguments</p> <ul> <li>distance: Float, indicating distance to origin.</li> </ul> <p>Returns</p> <ul> <li>sphere_point: List of spatial coordinates of a sphere.</li> </ul> <p>[source]</p>"},{"location":"backend/render/#sample_point_in_sphere","title":"sample_point_in_sphere","text":"<pre><code>paz.backend.render.sample_point_in_sphere(distance, top_only=False)\n</code></pre> <p>Samples random points from a sphere</p> <p>Arguments</p> <ul> <li>distance: Float, indicating distance to origin.</li> </ul> <p>Returns:</p> <p>List of spatial coordinates of a sphere.</p> <p>[source]</p>"},{"location":"backend/render/#sample_point_in_top_sphere","title":"sample_point_in_top_sphere","text":"<pre><code>paz.backend.render.sample_point_in_top_sphere(distance=1.0)\n</code></pre> <p>Get a point of the top of the unit sphere.</p> <p>Arguments</p> <ul> <li>distance: Float, indicating distance to origin.</li> </ul> <p>Returns</p> <ul> <li>sphere_point: List of spatial coordinates of a sphere.</li> </ul> <p>[source]</p>"},{"location":"backend/render/#sample_uniformly","title":"sample_uniformly","text":"<pre><code>paz.backend.render.sample_uniformly(value)\n</code></pre> <p>Samples from a uniform distribution.</p> <p>Arguments</p> <ul> <li>values: List or float. If list it must have [min_value, max_value].</li> </ul> <p>Returns</p> <p>Float</p> <p>[source]</p>"},{"location":"backend/render/#scale_translation","title":"scale_translation","text":"<pre><code>paz.backend.render.scale_translation(matrix, scale=10.0)\n</code></pre> <p>Changes the scale of the translation vector. Used for changing the regression problem to a bigger scale.</p> <p>Arguments:</p> <ul> <li>matrix: Numpy array of shape [4, 4]</li> <li>scale: Float used to multiple all the translation component.</li> </ul> <p>Returns:</p> <p>Numpy array of shape [4, 4]</p> <p>[source]</p>"},{"location":"backend/render/#split_alpha_channel","title":"split_alpha_channel","text":"<pre><code>paz.backend.render.split_alpha_channel(image)\n</code></pre> <p>Splits alpha channel from an RGBD image.</p> <p>Arguments</p> <ul> <li>image: Numpy array of shape [H, W, 4]</li> </ul> <p>Returns</p> <p>List of two numpy arrays of shape [H, W, 3] and [H, W]</p> <p>[source]</p>"},{"location":"backend/render/#translate_camera","title":"translate_camera","text":"<pre><code>paz.backend.render.translate_camera(world_to_camera, translation)\n</code></pre> <p>Translate camera coordinate system in its XY plane.</p> <p>Arguments:</p> <ul> <li>world_to_camera: Numpy array containing the affine transformation.</li> <li>translation: List or array with two inputs.</li> </ul>"},{"location":"backend/standard/","title":"Standard","text":"<p>[source]</p>"},{"location":"backend/standard/#append_lists","title":"append_lists","text":"<pre><code>paz.backend.standard.append_lists(intro_lists, outro_lists)\n</code></pre> <p>Appends multiple new values in intro lists to multiple outro lists</p> <p>Arguments</p> <ul> <li>intro_lists: List of lists</li> <li>outro_lists: List of lists</li> </ul> <p>Returns</p> <p>Lists with new values of intro lists</p> <p>[source]</p>"},{"location":"backend/standard/#append_values","title":"append_values","text":"<pre><code>paz.backend.standard.append_values(dictionary, lists, keys)\n</code></pre> <p>Append dictionary values to lists</p> <p>Arguments</p> <ul> <li>dictionary: dict</li> <li>lists: List of lists</li> <li>keys: Keys to dictionary values</li> </ul> <p>[source]</p>"},{"location":"backend/standard/#get_upper_multiple","title":"get_upper_multiple","text":"<pre><code>paz.backend.standard.get_upper_multiple(x, multiple=64)\n</code></pre> <p>Returns the upper multiple of 'multiple' to the x.</p> <p>Arguments</p> <ul> <li>x: Int.</li> <li>multiple: Int.</li> </ul> <p>Returns</p> <p>upper multiple. Int.</p> <p>[source]</p>"},{"location":"backend/standard/#resize_with_same_aspect_ratio","title":"resize_with_same_aspect_ratio","text":"<pre><code>paz.backend.standard.resize_with_same_aspect_ratio(image, input_size, multiple=64)\n</code></pre> <p>Resize the sort side of the input image to input_size and keep the aspect ratio.</p> <p>Arguments</p> <ul> <li>input_size: Dimension to be resized. Int.</li> <li>H: Int.</li> <li>W: Int.</li> </ul> <p>Returns</p> <p>resized H and W.</p> <p>[source]</p>"},{"location":"backend/standard/#get_transformation_scale","title":"get_transformation_scale","text":"<pre><code>paz.backend.standard.get_transformation_scale(image, size, scaling_factor)\n</code></pre> <p>Caluclte scale of resized H and W.</p> <p>Arguments</p> <ul> <li>H: Int.</li> <li>H_resized: Int.</li> <li>H_resized: Int.</li> <li>scaling_factor: Int.</li> </ul> <p>Returns</p> <p>scaled H and W</p> <p>[source]</p>"},{"location":"backend/standard/#compare_vertical_neighbours","title":"compare_vertical_neighbours","text":"<pre><code>paz.backend.standard.compare_vertical_neighbours(x, y, image, offset=0.25)\n</code></pre> <p>Compare two vertical neighbors and add an offset to the smaller one.</p> <p>Arguments</p> <ul> <li>x: Int. x coordinate of pixel to be compared.</li> <li>y: Int. y coordinate of pixel to be compared.</li> <li>image: Array.</li> <li>offset: Float.</li> </ul> <p>[source]</p>"},{"location":"backend/standard/#compare_horizontal_neighbours","title":"compare_horizontal_neighbours","text":"<pre><code>paz.backend.standard.compare_horizontal_neighbours(x, y, image, offset=0.25)\n</code></pre> <p>Compare two horizontal neighbors and add an offset to the smaller one.</p> <p>Arguments</p> <ul> <li>x: Int. x coordinate of pixel to be compared.</li> <li>y: Int. y coordinate of pixel to be compared.</li> <li>image: Array.</li> <li>offset: Float.</li> </ul> <p>[source]</p>"},{"location":"backend/standard/#get_all_indices_of_array","title":"get_all_indices_of_array","text":"<pre><code>paz.backend.standard.get_all_indices_of_array(array)\n</code></pre> <p>Get all the indices of an array.</p> <p>Arguments</p> <ul> <li>array: Array</li> </ul> <p>Returns</p> <p>Array. Array with the indices of the input array</p> <p>[source]</p>"},{"location":"backend/standard/#gather_nd","title":"gather_nd","text":"<pre><code>paz.backend.standard.gather_nd(array, indices, axis)\n</code></pre> <p>Take the value from the input array on the given indices along the given axis.</p> <p>Arguments</p> <ul> <li>array: Array</li> <li>indices: list/Array. values to be gathered from</li> <li>axis: Int. Axis along which to gather values.</li> </ul> <p>Returns</p> <p>Array. Gathered values from the input array</p> <p>[source]</p>"},{"location":"backend/standard/#calculate_norm","title":"calculate_norm","text":"<pre><code>paz.backend.standard.calculate_norm(vector, order=None, axis=None)\n</code></pre> <p>Calculates the norm of vector.</p> <p>Arguments</p> <ul> <li>x: List of spatial coordinates (x, y, z)</li> </ul> <p>[source]</p>"},{"location":"backend/standard/#tensor_to_numpy","title":"tensor_to_numpy","text":"<pre><code>paz.backend.standard.tensor_to_numpy(tensor)\n</code></pre> <p>Convert a tensor to a Array.</p> <p>Arguments</p> <ul> <li>tensor: multidimensional array of type tensor</li> </ul> <p>[source]</p>"},{"location":"backend/standard/#pad_matrix","title":"pad_matrix","text":"<pre><code>paz.backend.standard.pad_matrix(matrix, pool_size=(3, 3), strides=(1, 1), padding='valid', value=0)\n</code></pre> <p>Pad an array</p> <p>Arguments</p> <ul> <li>matrix: Array.</li> <li>padding: String. Type of padding</li> <li>value: Int. Value to be added in the padded area.</li> <li>poolsize: Int. How many rows and colums to be padded for 'same' padding</li> </ul> <p>[source]</p>"},{"location":"backend/standard/#max_pooling_2d","title":"max_pooling_2d","text":"<pre><code>paz.backend.standard.max_pooling_2d(image, pool_size=3, strides=1, padding='same')\n</code></pre> <p>Returns the maximum pooled value of an image.</p> <p>Arguments</p> <ul> <li>image: Array.</li> <li>poolsize: Int or list of len 2. Window size for each pool</li> <li>padding: String. Type of padding</li> </ul> <p>[source]</p>"},{"location":"backend/standard/#predict","title":"predict","text":"<pre><code>paz.backend.standard.predict(x, model, preprocess=None, postprocess=None)\n</code></pre> <p>Preprocess, predict and postprocess input. Arguments</p> <ul> <li>x: Input to model</li> <li>model: Callable i.e. Keras model.</li> <li>preprocess: Callable, used for preprocessing input x.</li> <li>postprocess: Callable, used for postprocessing output of model.</li> </ul> <p>Note</p> <p>If model outputs a tf.Tensor is converted automatically to numpy array.</p>"},{"location":"getting-started/bounding_boxes/","title":"Bounding boxes","text":"<p>In this tutorial we show you how to build bounding boxes for your own training pipeline:</p> <p>You can find the complete script of this tutorial here</p> <p>Let's do our basic imports</p> <pre><code>import os\nimport numpy as np\nfrom tensorflow.keras.utils import get_file\n\nimport paz.processors as pr\nfrom paz.abstract import SequentialProcessor\nfrom paz.backend.image import load_image\n</code></pre> <p>First we will download a test image and put it inside our PAZ directory</p> <pre><code>IMAGE_URL = ('https://github.com/oarriaga/altamira-data/releases/download'\n             '/v0.9/object_detection_augmentation.png')\nimage_filename = os.path.basename(IMAGE_URL)\nimage_fullpath = get_file(image_filename, IMAGE_URL, cache_subdir='paz/data')\n</code></pre> <p>Let's first build our labels: Keep in mind that the origin of our images is located at the top-left.</p> <p>The <code>x_min, y_min</code> are the normalized coordinates of top-left bounding-box corner.</p> <pre><code>height, width = load_image(image_fullpath).shape[:2]\nx_min_human, y_min_human = 200 / width, 60 / height\nx_min_horse, y_min_horse = 100 / width, 90 / height\n</code></pre> <p>The <code>x_max, y_max</code> are the normalized coordinates of bottom-right bounding-box corner.</p> <pre><code>x_max_human, y_max_human = 300 / width, 200 / height\nx_max_horse, y_max_horse = 400 / width, 300 / height\n</code></pre> <p>Our image has 1 + 2 classes. The first class is the background-class. The other 2 classes correspond to each object i.e. human, horse.</p> <pre><code>num_classes = 3\nbackground_class, human_class, horse_class = 0, 1, 2\nclass_names = ['background', 'human', 'horse']\n\nbox_data = np.array(\n    [[x_min_human, y_min_human, x_max_human, y_max_human, human_class],\n     [x_min_horse, y_min_horse, x_max_horse, y_max_horse, horse_class]])\n</code></pre> <p>Let's create a simple visualization pipeline. For an explanation of what control-map is doing please check our tutorial at: paz/examples/tutorials/controlmap_processor.py</p> <pre><code>draw_boxes = SequentialProcessor()\ndraw_boxes.add(pr.ControlMap(pr.ToBoxes2D(class_names), [1], [1]))\ndraw_boxes.add(pr.ControlMap(pr.LoadImage(), [0], [0]))\ndraw_boxes.add(pr.ControlMap(pr.DenormalizeBoxes2D(), [0, 1], [1], {0: 0}))\ndraw_boxes.add(pr.DrawBoxes2D(class_names))\ndraw_boxes.add(pr.ShowImage())\n</code></pre> <p>We can now look at our boxes!</p> <pre><code>draw_boxes(image_fullpath, box_data)\n</code></pre>"},{"location":"getting-started/controlmap/","title":"Control-map","text":"<p>In this tutorial we show you how to use one of our most useful processors <code>ControlMap</code>.</p> <p>You can find the complete script of this tutorial here</p> <pre><code>import os\nimport numpy as np\nfrom paz.abstract import SequentialProcessor\nfrom paz.backend.image import show_image, load_image\nimport paz.processors as pr\nfrom tensorflow.keras.utils import get_file\n</code></pre> <p>Let's download a test image and put it inside our PAZ directory</p> <pre><code>IMAGE_URL = ('https://github.com/oarriaga/altamira-data/releases/download'\n             '/v0.9/object_detection_augmentation.png')\nimage_filename = os.path.basename(IMAGE_URL)\nimage_fullpath = get_file(image_filename, IMAGE_URL, cache_subdir='paz/data')\n</code></pre> <p>The <code>x_min, y_min</code> are the normalized coordinates of top-left bounding-box corner.</p> <pre><code>H, W = load_image(image_fullpath).shape[:2]\n</code></pre> <p>The <code>x_max, y_max</code> are the normalized coordinates</p> <pre><code>class_names = ['background', 'human', 'horse']\nbox_data = np.array([[200 / W, 60 / H, 300 / W, 200 / H, 1],\n                     [100 / W, 90 / H, 400 / W, 300 / H, 2]])\n</code></pre> <p>Let's visualize our boxes! First we transform our numpy array into our built-in <code>Box2D</code> messages</p> <pre><code>to_boxes2D = pr.ToBoxes2D(class_names)\ndenormalize = pr.DenormalizeBoxes2D()\nboxes2D = to_boxes2D(box_data)\nimage = load_image(image_fullpath)\nboxes2D = denormalize(image, boxes2D)\ndraw_boxes2D = pr.DrawBoxes2D(class_names)\nshow_image(draw_boxes2D(image, boxes2D))\n</code></pre> <p>As you can see, we were not able to put everything as a <code>SequentialProcessor</code>. This is because we are dealing with 2 inputs: <code>box_data</code> and <code>image</code>. We can join them into a single processor using <code>pr.ControlMap</code> wrap. <code>pr.ControlMap</code> allows you to select which arguments (<code>intro_indices</code>) are passed to your processor, and also where you should put the output of your processor (<code>outro_indices</code>).</p> <pre><code>draw_boxes = SequentialProcessor()\ndraw_boxes.add(pr.ControlMap(to_boxes2D, intro_indices=[1], outro_indices=[1]))\ndraw_boxes.add(pr.ControlMap(pr.LoadImage(), [0], [0]))\ndraw_boxes.add(pr.ControlMap(denormalize, [0, 1], [1], keep={0: 0}))\ndraw_boxes.add(pr.DrawBoxes2D(class_names))\ndraw_boxes.add(pr.ShowImage())\n</code></pre> <p>Now you have everything in a single packed function that loads and draws!</p> <pre><code>draw_boxes(image_fullpath, box_data)\n</code></pre> <p>Also note if one of your function is <code>eating</code> away one input that you wish to keep in your pipeline, you can use the <code>keep</code> dictionary to explicitly say which of your inputs you wish to keep and where it should be located. This is represented respectively by the <code>key</code> and the <code>value</code> of the <code>keep</code> dictionary.</p>"},{"location":"getting-started/image_augmentation/","title":"Image augmentation","text":"<p>This tutorial explains the basic functionality of <code>SequentialProcessors</code> for data augmentation in a classification task.</p> <p>You can find the complete script of this tutorial here</p> <pre><code>import os\nfrom paz.abstract import SequentialProcessor\nfrom paz.backend.image import show_image, load_image\nimport paz.processors as pr\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.utils import get_file\n</code></pre> <p>Let's download a test image and put it inside our PAZ directory</p> <pre><code>IMAGE_URL = ('https://github.com/oarriaga/altamira-data/releases/download'\n             '/v0.9/image_augmentation.png')\nimage_filename = os.path.basename(IMAGE_URL)\nimage_fullpath = get_file(image_filename, IMAGE_URL, cache_subdir='paz/data')\n</code></pre> <p>We construct a data augmentation pipeline using the built-in PAZ processors:</p> <pre><code>augment = SequentialProcessor()\naugment.add(pr.RandomContrast())\naugment.add(pr.RandomBrightness())\naugment.add(pr.RandomSaturation())\n</code></pre> <p>We can now apply our pipeline as a normal function:</p> <pre><code>for _ in range(5):\n    image = load_image(image_fullpath)\n    # use it as a normal function\n    image = augment(image)\n    show_image(image)\n</code></pre> <p>We can add to our sequential pipeline other function anywhere i.e. arg 0:</p> <pre><code>augment.insert(0, pr.LoadImage())\nfor _ in range(5):\n    # now we don't load the image every time.\n    image = augment(image_fullpath)\n    show_image(image)\n</code></pre> <p>Adding new processor at the end to have a single function.</p> <pre><code>augment.add(pr.ShowImage())\nfor _ in range(5):\n    # everything compressed into a single function\n    image = augment(image_fullpath)\n</code></pre> <p>We can also pop the last processor added.</p> <pre><code>augment.pop()\n</code></pre> <p>We now create another processor for geometric augmentation. NOTE: We can instantiate a new <code>SequentialProcessor</code> using a list of processors</p> <pre><code>transform = SequentialProcessor([pr.RandomRotation(), pr.RandomTranslation()])\n</code></pre> <p>You should start getting now transformations similar to these ones:</p> <p> </p> <p>We can call both of our processors separately:</p> <pre><code>for _ in range(5):\n    image = transform(augment(image_fullpath))\n    show_image(image)\n</code></pre> <p>But since processors are just functions we can simply add it as a processor:</p> <pre><code>augment.add(transform)\nfor _ in range(5):\n    image = augment(image_fullpath)\n    show_image(image)\n</code></pre> <p>We can also use the Keras <code>ImageDataGenerator</code>:</p> <pre><code>generator = ImageDataGenerator(\n    rotation_range=30,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    zoom_range=0.1,\n    horizontal_flip=True)\n</code></pre> <p>We can add it by using our processor/wrapper <code>ImageDataProcessor</code>:</p> <pre><code>augment = SequentialProcessor()\naugment.add(pr.LoadImage())\naugment.add(pr.ImageDataProcessor(generator))\naugment.add(pr.ShowImage())\nfor _ in range(5):\n    image = augment(image_fullpath)\n</code></pre>"},{"location":"getting-started/introduction_to_processors/","title":"Introduction to processors","text":"<p>PAZ allows us to easily create preprocessing, data-augmentation and post-processing pipelines.</p> <p>In the example below we show how to create a simple data-augmentation pipeline:</p> <pre><code>from paz.abstract import SequentialProcessor\nfrom paz import processors as pr\n\naugment_image = SequentialProcessor()\naugment_image.add(pr.RandomContrast())\naugment_image.add(pr.RandomBrightness())\naugment_image.add(pr.RandomSaturation())\naugment_image.add(pr.RandomHue())\n</code></pre> <p>The final pipeline (<code>augment_image</code>) behaves as a Python function:</p> <pre><code>new_image = augment_image(image)\n</code></pre> <p>There exists plenty default <code>pipelines</code> already built in PAZ. For more information please consult <code>paz.pipelines</code>.</p> <p>Pipelines are built from <code>paz.processors</code>. There are plenty of processors implemented in PAZ; however, one can easily build a custom processor by inheriting from <code>paz.abstract.Processor</code>.</p> <p>In the example below we show how to build a <code>processor</code> for normalizing an image to a range from 0 to 1.</p> <pre><code>from paz.abstract import Processor\n\nclass NormalizeImage(Processor):\n\"\"\"Normalize image by diving all values by 255.0.\n    \"\"\"\n    def __init__(self):\n        super(NormalizeImage, self).__init__()\n\n    def call(self, image):\n        return image / 255.0\n</code></pre> <p>We can now use our processor to create a pipeline for loading an image and normalizing it:</p> <pre><code>from paz.abstract import SequentialProcessor\nfrom paz.processors import LoadImage\n\npreprocess_image = SequentialProcessor()\npreprocess_image.add(LoadImage())\npreprocess_image.add(NormalizeImage())\n</code></pre> <p>We can now use our new function/pipeline to load and normalize an image:</p> <pre><code>image = preprocess_image('images/cat.jpg')\n</code></pre>"},{"location":"getting-started/introduction_to_processors/#why-the-name-processor","title":"Why the name <code>Processor</code>?","text":"<p>Originally PAZ was only meant for pre-processing pipelines that included data-augmentation, normalization, etc. However, I found out that we could use the same API for post-processing; therefore, I thought at the time that <code>Processor</code> would be adequate to describe the capacity of both pre-processing and post-processing. Names that I also thought could have worked were: <code>Function</code>, <code>Functor</code> but I didn't want to use those since I thought they would be more confusing. Similarly, in Keras this abstraction is interpreted as a <code>Layer</code> but here I don't think that abstraction is adequate. A layer of computation maybe? So after having this thoughts swirling around I decided to go with <code>Processor</code> and be explicit about my mental jugglery hoping that this name doesn't cause much mental overhead in the future.</p>"},{"location":"getting-started/object_detection_pipeline/","title":"Data augmentation for object detection","text":"<p>This tutorial explains the basic functionality of <code>SequentialProcessors</code> for data augmentation in an object detection task.</p> <p>You can find the complete script of this tutorial here</p> <p>This script explains the basic functionality of <code>SequentialProcessors</code> for data augmentation in an object-detection task.</p> <pre><code>import os\nimport numpy as np\nfrom tensorflow.keras.utils import get_file\n\nfrom paz.abstract import SequentialProcessor, ProcessingSequence\nfrom paz.models.detection.utils import create_prior_boxes\nimport paz.processors as pr\nimport paz.backend as P\n</code></pre> <p>Let's download a test image and put it inside our PAZ directory</p> <pre><code>IMAGE_URL = ('https://github.com/oarriaga/altamira-data/releases/download'\n             '/v0.9/object_detection_augmentation.png')\nimage_filename = os.path.basename(IMAGE_URL)\nimage_fullpath = get_file(image_filename, IMAGE_URL, cache_subdir='paz/data')\n</code></pre>"},{"location":"getting-started/object_detection_pipeline/#image-augmentation-and-preprocessing-part","title":"Image augmentation and preprocessing part","text":"<p>We can also create sequential pipelines by inheriting <code>SequentialProcessor</code></p> <pre><code>class AugmentImage(SequentialProcessor):\n    def __init__(self):\n        super(AugmentImage, self).__init__()\n        self.add(pr.RandomContrast())\n        self.add(pr.RandomBrightness())\n        self.add(pr.RandomSaturation())\n        self.add(pr.RandomHue())\n\n\nclass PreprocessImage(SequentialProcessor):\n    def __init__(self, shape, mean=pr.BGR_IMAGENET_MEAN):\n        super(PreprocessImage, self).__init__()\n        self.add(pr.ResizeImage(shape))\n        self.add(pr.CastImage(float))\n        if mean is None:\n            self.add(pr.NormalizeImage())\n        else:\n            self.add(pr.SubtractMeanImage(mean))\n</code></pre> <p>Let's see who it works:</p> <pre><code>preprocess_image, augment_image = PreprocessImage((300, 300)), AugmentImage()\nprint('Image pre-processing examples')\nfor _ in range(10):\n    image = P.image.load_image(image_fullpath)\n    image = preprocess_image(augment_image(image))\n    P.image.show_image(image.astype('uint8'))\n</code></pre>"},{"location":"getting-started/object_detection_pipeline/#box-augmentation-and-preprocessing-part","title":"Box augmentation and preprocessing part","text":"<p>Let's first build our box labels: For a tutorial on how to build your box labels check here</p> <pre><code>H, W = P.image.load_image(image_fullpath).shape[:2]\nclass_names = ['background', 'human', 'horse']\nbox_data = np.array([[200 / W, 60 / H, 300 / W, 200 / H, 1],\n                     [100 / W, 90 / H, 400 / W, 300 / H, 2]])\n</code></pre>"},{"location":"getting-started/object_detection_pipeline/#data-augmentation-for-boxes","title":"Data augmentation for boxes","text":"<pre><code>class AugmentBoxes(SequentialProcessor):\n    def __init__(self, mean=pr.BGR_IMAGENET_MEAN):\n        super(AugmentBoxes, self).__init__()\n        self.add(pr.ToImageBoxCoordinates())\n        self.add(pr.Expand(mean=mean))\n        self.add(pr.RandomSampleCrop())\n        self.add(pr.RandomFlipBoxesLeftRight())\n        self.add(pr.ToNormalizedBoxCoordinates())\n</code></pre> <p>To visualize our current box augmentation we build a quick pipeline for drawing our boxes</p> <pre><code>draw_boxes = SequentialProcessor([\n    pr.ControlMap(pr.ToBoxes2D(class_names, False), [1], [1]),\n    pr.ControlMap(pr.DenormalizeBoxes2D(), [0, 1], [1], {0: 0}),\n    pr.DrawBoxes2D(class_names),\n    pr.ShowImage()])\n</code></pre> <p>Let's test our box data augmentation pipeline!</p> <pre><code>augment_boxes = AugmentBoxes()\nprint('Box augmentation examples')\nfor _ in range(10):\n    image = P.image.load_image(image_fullpath)\n    image, boxes = augment_boxes(image, box_data.copy())\n    draw_boxes(P.image.resize_image(image, (300, 300)), boxes)\n</code></pre>"},{"location":"getting-started/object_detection_pipeline/#data-preprocessing-for-boxes","title":"Data preprocessing for boxes","text":"<p>There is also some box-preprocessing that is required. Mostly we must match our boxes to a set of default (prior) boxes. Then we must encode them and expand the class label to a one-hot vector.</p> <pre><code>class PreprocessBoxes(SequentialProcessor):\n    def __init__(self, num_classes, prior_boxes, IOU, variances):\n        super(PreprocessBoxes, self).__init__()\n        self.add(pr.MatchBoxes(prior_boxes, IOU),)\n        self.add(pr.EncodeBoxes(prior_boxes, variances))\n        self.add(pr.BoxClassToOneHotVector(num_classes))\n</code></pre> <p>Putting everything together in a single processor:</p> <p>Note that these is the same processor we have internally in PAZ: <code>paz.pipelines.AugmentDetection</code>.</p> <pre><code>class AugmentDetection(SequentialProcessor):\n    def __init__(self, prior_boxes, split=pr.TRAIN, num_classes=21, size=300,\n                 mean=pr.BGR_IMAGENET_MEAN, IOU=.5, variances=[.1, .2]):\n        super(AugmentDetection, self).__init__()\n\n        # image processors\n        self.augment_image = AugmentImage()\n        self.augment_image.add(pr.ConvertColorSpace(pr.RGB2BGR))\n        self.preprocess_image = PreprocessImage((size, size), mean)\n\n        # box processors\n        self.augment_boxes = AugmentBoxes()\n        args = (num_classes, prior_boxes, IOU, variances)\n        self.preprocess_boxes = PreprocessBoxes(*args)\n\n        # pipeline\n        self.add(pr.UnpackDictionary(['image', 'boxes']))\n        self.add(pr.ControlMap(pr.LoadImage(), [0], [0]))\n        if split == pr.TRAIN:\n            self.add(pr.ControlMap(self.augment_image, [0], [0]))\n            self.add(pr.ControlMap(self.augment_boxes, [0, 1], [0, 1]))\n        self.add(pr.ControlMap(self.preprocess_image, [0], [0]))\n        self.add(pr.ControlMap(self.preprocess_boxes, [1], [1]))\n        self.add(pr.SequenceWrapper(\n            {0: {'image': [size, size, 3]}},\n            {1: {'boxes': [len(prior_boxes), 4 + num_classes]}}))\n</code></pre> <p>Here we just made some small modifications to our original drawing pipeline</p> <pre><code>prior_boxes = create_prior_boxes()\ndraw_boxes.processors[0].processor.one_hot_encoded = True\ndraw_boxes.insert(0, pr.ControlMap(pr.DecodeBoxes(prior_boxes), [1], [1]))\ndraw_boxes.insert(2, pr.ControlMap(\n    pr.FilterClassBoxes2D(class_names[1:]), [1], [1]))\n\ndef deprocess_image(image):\n    image = (image + pr.BGR_IMAGENET_MEAN).astype('uint8')\n    return P.image.convert_color_space(image, pr.BGR2RGB)\n</code></pre> <p>Finally we can apply our complete pipeline that preprocess and augments the images as well the bounding boxes!</p> <pre><code>augmentator = AugmentDetection(prior_boxes, num_classes=len(class_names))\nprint('Image and boxes augmentations')\nfor _ in range(10):\n    sample = {'image': image_fullpath, 'boxes': box_data.copy()}\n    data = augmentator(sample)\n    image, boxes = data['inputs']['image'], data['labels']['boxes']\n    image = deprocess_image(image)\n    draw_boxes(image, boxes)\n</code></pre> <p>Note that we change the input and output format from lists to a dictionaries. The input changed by adding the <code>pr.UnpackDictionary</code> processor, and the output changed by the <code>pr.SequenceWrapper</code> processor. The <code>pr.SequenceWrapper</code> method allows us to easily connect the complete pipeline to a Sequence Generator. Here we show you the final step such that you can wrap it in our custom generator and pass it directly to your model to do <code>model.fit</code>.</p> <pre><code>data = [{'image': image_fullpath, 'boxes': box_data}]\nprint('Image and boxes augmentations with generator')\nbatch_size = 1\nsequence = ProcessingSequence(augmentator, batch_size, data)\nfor _ in range(10):\n    batch = sequence.__getitem__(0)\n    batch_images, batch_boxes = batch[0]['image'], batch[1]['boxes']\n    image, boxes = batch_images[0], batch_boxes[0]\n    image = deprocess_image(image)\n    draw_boxes(image, boxes)\n</code></pre> <p>You should now be able to see transformations similar to these ones:</p> <p> </p>"},{"location":"models/classification/","title":"Classification","text":"<p>Models for object classification</p> <p>[source]</p>"},{"location":"models/classification/#minixception","title":"MiniXception","text":"<pre><code>paz.models.classification.xception.MiniXception(input_shape, num_classes, weights=None)\n</code></pre> <p>Build MiniXception (see references).</p> <p>Arguments</p> <ul> <li>input_shape: List of three integers e.g. <code>[H, W, 3]</code></li> <li>num_classes: Int.</li> <li>weights: <code>None</code> or string with pre-trained dataset. Valid datasets     include only <code>FER</code>.</li> </ul> <p>Returns</p> <p>Tensorflow-Keras model.</p> <p>References</p> <ul> <li>Real-time Convolutional Neural Networks for Emotion and     Gender Classification</li> </ul> <p>[source]</p>"},{"location":"models/classification/#protoembedding","title":"ProtoEmbedding","text":"<pre><code>paz.models.classification.protonet.ProtoEmbedding(image_shape, num_blocks)\n</code></pre> <p>Embedding convolutional network used for proto-typical networks</p> <p>Arguments:</p> <ul> <li>image_shape: List with image shape <code>(H, W, channels)</code>.</li> <li>num_blocks: Ints. Number of convolution blocks.</li> </ul> <p>Returns:</p> <p>Keras model.</p> <p>References:</p> <p>prototypical networks</p> <p>[source]</p>"},{"location":"models/classification/#protonet","title":"ProtoNet","text":"<pre><code>paz.models.classification.protonet.ProtoNet(embed, num_classes, num_support, num_queries, image_shape)\n</code></pre> <p>Prototypical networks used for few-shot classification Arguments:</p> <ul> <li>embed: Keras network for embedding images into metric space.</li> <li>num_classes: Number of <code>ways</code> for few-shot classification.</li> <li>num_support: Number of <code>shots</code> used for meta learning.</li> <li>num_queries: Number of test images to query.</li> <li>image_shape: List with image shape <code>(H, W, channels)</code>.</li> </ul> <p>Returns:</p> <p>Keras model.</p> <p>References:</p> <p>prototypical networks</p>"},{"location":"models/detection/","title":"Detection","text":"<p>Models for 2D object detection</p> <p>[source]</p>"},{"location":"models/detection/#ssd300","title":"SSD300","text":"<pre><code>paz.models.detection.ssd300.SSD300(num_classes=21, base_weights='VOC', head_weights='VOC', input_shape=(300, 300, 3), num_priors=[4, 6, 6, 6, 4, 4], l2_loss=0.0005, return_base=False, trainable_base=True)\n</code></pre> <p>Single-shot-multibox detector for 300x300x3 BGR input images. Arguments</p> <ul> <li>num_classes: Integer. Specifies the number of class labels.</li> <li>base_weights: String or None. If string should be a valid dataset name.     Current valid datasets include <code>VOC</code> <code>FAT</code> and <code>VGG</code>.</li> <li>head_weights: String or None. If string should be a valid dataset name.     Current valid datasets include <code>VOC</code> and <code>FAT</code>.</li> <li>input_shape: List of integers. Input shape to the model including only     spatial and channel resolution e.g. (300, 300, 3).</li> <li>num_priors: List of integers. Number of default box shapes     used in each detection layer.</li> <li>l2_loss: Float. l2 regularization loss for convolutional layers.</li> <li>return_base: Boolean. If <code>True</code> the model returned is just     the original base.</li> <li>trainable_base: Boolean. If <code>True</code> the base model     weights are also trained.</li> </ul> <p>Reference</p> <ul> <li>SSD: Single Shot MultiBox     Detector</li> </ul> <p>[source]</p>"},{"location":"models/detection/#ssd512","title":"SSD512","text":"<pre><code>paz.models.detection.ssd512.SSD512(num_classes=81, base_weights='COCO', head_weights='COCO', input_shape=(512, 512, 3), num_priors=[4, 6, 6, 6, 6, 4, 4], l2_loss=0.0005, return_base=False, trainable_base=True)\n</code></pre> <p>Single-shot-multibox detector for 512x512x3 BGR input images. Arguments</p> <ul> <li>num_classes: Integer. Specifies the number of class labels.</li> <li>base_weights: String or None. If string should be a valid dataset name.     Current valid datasets include <code>COCO</code> and <code>OIV6Hand</code>.</li> <li>head_weights: String or None. If string should be a valid dataset name.     Current valid datasets include <code>COCO</code>, <code>YCBVideo</code> and <code>OIV6Hand</code>.</li> <li>input_shape: List of integers. Input shape to the model including only     spatial and channel resolution e.g. (512, 512, 3).</li> <li>num_priors: List of integers. Number of default box shapes     used in each detection layer.</li> <li>l2_loss: Float. l2 regularization loss for convolutional layers.</li> <li>return_base: Boolean. If <code>True</code> the model returned is just     the original base.</li> <li>trainable_base: Boolean. If <code>True</code> the base model     weights are also trained.</li> </ul> <p>Reference</p> <ul> <li>SSD: Single Shot MultiBox     Detector</li> </ul> <p>[source]</p>"},{"location":"models/detection/#haarcascadedetector","title":"HaarCascadeDetector","text":"<pre><code>paz.models.detection.haar_cascade.HaarCascadeDetector(self, weights='frontalface_default', class_arg=None, scale=1.3, neighbors=5)\n</code></pre> <p>Haar cascade face detector.</p> <p>Arguments</p> <ul> <li>path: String. Postfix to default openCV haarcascades XML files, see [1]     e.g. <code>eye</code>, <code>frontalface_alt2</code>, <code>fullbody</code></li> <li>class_arg: Int. Class label argument.</li> </ul> <p>scale = Float. Scale for image reduction</p> <ul> <li>neighbors: Int. Minimum neighbors</li> </ul> <p>Reference</p> <ul> <li>Haar     Cascades</li> </ul> <p>[source]</p>"},{"location":"models/detection/#efficientdetd0","title":"EFFICIENTDETD0","text":"<pre><code>paz.models.detection.efficientdet.efficientdet.EFFICIENTDETD0(num_classes=90, base_weights='COCO', head_weights='COCO', input_shape=(512, 512, 3), FPN_num_filters=64, FPN_cell_repeats=3, box_class_repeats=3, anchor_scale=4.0, fusion='fast', return_base=False, model_name='efficientdet-d0', scaling_coefficients=(1.0, 1.0, 0.8))\n</code></pre> <p>Instantiates EfficientDet-D0 model.</p> <p>Arguments</p> <ul> <li>num_classes: Int, number of object classes.</li> <li>base_weights: Str, base weights name.</li> <li>head_weights: Str, head weights name.</li> <li>input_shape: Tuple, holding input image size.</li> <li>FPN_num_filters: Int, number of FPN filters.</li> <li>FPN_cell_repeats: Int, number of FPN blocks.</li> <li>box_class_repeats: Int, Number of regression     and classification blocks.</li> <li>anchor_scale: Int, number of anchor scales.</li> <li>fusion: Str, feature fusion weighting method.</li> <li>return_base: Bool, whether to return base or not.</li> <li>model_name: Str, EfficientDet model name.</li> <li>scaling_coefficients: Tuple, EfficientNet scaling coefficients.</li> </ul> <p>Returns</p> <ul> <li>model: EfficientDet-D0 model.</li> </ul> <p>[source]</p>"},{"location":"models/detection/#efficientdetd1","title":"EFFICIENTDETD1","text":"<pre><code>paz.models.detection.efficientdet.efficientdet.EFFICIENTDETD1(num_classes=90, base_weights='COCO', head_weights='COCO', input_shape=(640, 640, 3), FPN_num_filters=88, FPN_cell_repeats=4, box_class_repeats=3, anchor_scale=4.0, fusion='fast', return_base=False, model_name='efficientdet-d1', scaling_coefficients=(1.0, 1.1, 0.8))\n</code></pre> <p>Instantiates EfficientDet-D1 model.</p> <p>Arguments</p> <ul> <li>num_classes: Int, number of object classes.</li> <li>base_weights: Str, base weights name.</li> <li>head_weights: Str, head weights name.</li> <li>input_shape: Tuple, holding input image size.</li> <li>FPN_num_filters: Int, number of FPN filters.</li> <li>FPN_cell_repeats: Int, number of FPN blocks.</li> <li>box_class_repeats: Int, Number of regression     and classification blocks.</li> <li>anchor_scale: Int, number of anchor scales.</li> <li>fusion: Str, feature fusion weighting method.</li> <li>return_base: Bool, whether to return base or not.</li> <li>model_name: Str, EfficientDet model name.</li> <li>scaling_coefficients: Tuple, EfficientNet scaling coefficients.</li> </ul> <p>Returns</p> <ul> <li>model: EfficientDet-D1 model.</li> </ul> <p>[source]</p>"},{"location":"models/detection/#efficientdetd2","title":"EFFICIENTDETD2","text":"<pre><code>paz.models.detection.efficientdet.efficientdet.EFFICIENTDETD2(num_classes=90, base_weights='COCO', head_weights='COCO', input_shape=(768, 768, 3), FPN_num_filters=112, FPN_cell_repeats=5, box_class_repeats=3, anchor_scale=4.0, fusion='fast', return_base=False, model_name='efficientdet-d2', scaling_coefficients=(1.1, 1.2, 0.7))\n</code></pre> <p>Instantiate EfficientDet-D2 model.</p> <p>Arguments</p> <ul> <li>num_classes: Int, number of object classes.</li> <li>base_weights: Str, base weights name.</li> <li>head_weights: Str, head weights name.</li> <li>input_shape: Tuple, holding input image size.</li> <li>FPN_num_filters: Int, number of FPN filters.</li> <li>FPN_cell_repeats: Int, number of FPN blocks.</li> <li>box_class_repeats: Int, Number of regression     and classification blocks.</li> <li>anchor_scale: Int, number of anchor scales.</li> <li>fusion: Str, feature fusion weighting method.</li> <li>return_base: Bool, whether to return base or not.</li> <li>model_name: Str, EfficientDet model name.</li> <li>scaling_coefficients: Tuple, EfficientNet scaling coefficients.</li> </ul> <p>Returns</p> <ul> <li>model: EfficientDet-D2 model.</li> </ul> <p>[source]</p>"},{"location":"models/detection/#efficientdetd3","title":"EFFICIENTDETD3","text":"<pre><code>paz.models.detection.efficientdet.efficientdet.EFFICIENTDETD3(num_classes=90, base_weights='COCO', head_weights='COCO', input_shape=(896, 896, 3), FPN_num_filters=160, FPN_cell_repeats=6, box_class_repeats=4, anchor_scale=4.0, fusion='fast', return_base=False, model_name='efficientdet-d3', scaling_coefficients=(1.2, 1.4, 0.7))\n</code></pre> <p>Instantiates EfficientDet-D3 model.</p> <p>Arguments</p> <ul> <li>num_classes: Int, number of object classes.</li> <li>base_weights: Str, base weights name.</li> <li>head_weights: Str, head weights name.</li> <li>input_shape: Tuple, holding input image size.</li> <li>FPN_num_filters: Int, number of FPN filters.</li> <li>FPN_cell_repeats: Int, number of FPN blocks.</li> <li>box_class_repeats: Int, Number of regression     and classification blocks.</li> <li>anchor_scale: Int, number of anchor scales.</li> <li>fusion: Str, feature fusion weighting method.</li> <li>return_base: Bool, whether to return base or not.</li> <li>model_name: Str, EfficientDet model name.</li> <li>scaling_coefficients: Tuple, EfficientNet scaling coefficients.</li> </ul> <p>Returns</p> <ul> <li>model: EfficientDet-D3 model.</li> </ul> <p>[source]</p>"},{"location":"models/detection/#efficientdetd4","title":"EFFICIENTDETD4","text":"<pre><code>paz.models.detection.efficientdet.efficientdet.EFFICIENTDETD4(num_classes=90, base_weights='COCO', head_weights='COCO', input_shape=(1024, 1024, 3), FPN_num_filters=224, FPN_cell_repeats=7, box_class_repeats=4, anchor_scale=4.0, fusion='fast', return_base=False, model_name='efficientdet-d4', scaling_coefficients=(1.4, 1.8, 0.6))\n</code></pre> <p>Instantiates EfficientDet-D4 model.</p> <p>Arguments</p> <ul> <li>num_classes: Int, number of object classes.</li> <li>base_weights: Str, base weights name.</li> <li>head_weights: Str, head weights name.</li> <li>input_shape: Tuple, holding input image size.</li> <li>FPN_num_filters: Int, number of FPN filters.</li> <li>FPN_cell_repeats: Int, number of FPN blocks.</li> <li>box_class_repeats: Int, Number of regression     and classification blocks.</li> <li>anchor_scale: Int, number of anchor scales.</li> <li>fusion: Str, feature fusion weighting method.</li> <li>return_base: Bool, whether to return base or not.</li> <li>model_name: Str, EfficientDet model name.</li> <li>scaling_coefficients: Tuple, EfficientNet scaling coefficients.</li> </ul> <p>Returns</p> <ul> <li>model: EfficientDet-D4 model.</li> </ul> <p>[source]</p>"},{"location":"models/detection/#efficientdetd5","title":"EFFICIENTDETD5","text":"<pre><code>paz.models.detection.efficientdet.efficientdet.EFFICIENTDETD5(num_classes=90, base_weights='COCO', head_weights='COCO', input_shape=(1280, 1280, 3), FPN_num_filters=288, FPN_cell_repeats=7, box_class_repeats=4, anchor_scale=4.0, fusion='fast', return_base=False, model_name='efficientdet-d5', scaling_coefficients=(1.6, 2.2, 0.6))\n</code></pre> <p>Instantiates EfficientDet-D5 model.</p> <p>Arguments</p> <ul> <li>num_classes: Int, number of object classes.</li> <li>base_weights: Str, base weights name.</li> <li>head_weights: Str, head weights name.</li> <li>input_shape: Tuple, holding input image size.</li> <li>FPN_num_filters: Int, number of FPN filters.</li> <li>FPN_cell_repeats: Int, number of FPN blocks.</li> <li>box_class_repeats: Int, Number of regression     and classification blocks.</li> <li>anchor_scale: Int, number of anchor scales.</li> <li>fusion: Str, feature fusion weighting method.</li> <li>return_base: Bool, whether to return base or not.</li> <li>model_name: Str, EfficientDet model name.</li> <li>scaling_coefficients: Tuple, EfficientNet scaling coefficients.</li> </ul> <p>Returns</p> <ul> <li>model: EfficientDet-D5 model.</li> </ul> <p>[source]</p>"},{"location":"models/detection/#efficientdetd6","title":"EFFICIENTDETD6","text":"<pre><code>paz.models.detection.efficientdet.efficientdet.EFFICIENTDETD6(num_classes=90, base_weights='COCO', head_weights='COCO', input_shape=(1280, 1280, 3), FPN_num_filters=384, FPN_cell_repeats=8, box_class_repeats=5, anchor_scale=5.0, fusion='sum', return_base=False, model_name='efficientdet-d6', scaling_coefficients=(1.8, 2.6, 0.5))\n</code></pre> <p>Instantiates EfficientDet-D6 model.</p> <p>Arguments</p> <ul> <li>num_classes: Int, number of object classes.</li> <li>base_weights: Str, base weights name.</li> <li>head_weights: Str, head weights name.</li> <li>input_shape: Tuple, holding input image size.</li> <li>FPN_num_filters: Int, number of FPN filters.</li> <li>FPN_cell_repeats: Int, number of FPN blocks.</li> <li>box_class_repeats: Int, Number of regression     and classification blocks.</li> <li>anchor_scale: Int, number of anchor scales.</li> <li>fusion: Str, feature fusion weighting method.</li> <li>return_base: Bool, whether to return base or not.</li> <li>model_name: Str, EfficientDet model name.</li> <li>scaling_coefficients: Tuple, EfficientNet scaling coefficients.</li> </ul> <p>Returns</p> <ul> <li>model: EfficientDet-D6 model.</li> </ul> <p>[source]</p>"},{"location":"models/detection/#efficientdetd7","title":"EFFICIENTDETD7","text":"<pre><code>paz.models.detection.efficientdet.efficientdet.EFFICIENTDETD7(num_classes=90, base_weights='COCO', head_weights='COCO', input_shape=(1536, 1536, 3), FPN_num_filters=384, FPN_cell_repeats=8, box_class_repeats=5, anchor_scale=5.0, fusion='sum', return_base=False, model_name='efficientdet-d7', scaling_coefficients=(1.8, 2.6, 0.5))\n</code></pre> <p>Instantiates EfficientDet-D7 model.</p> <p>Arguments</p> <ul> <li>num_classes: Int, number of object classes.</li> <li>base_weights: Str, base weights name.</li> <li>head_weights: Str, head weights name.</li> <li>input_shape: Tuple, holding input image size.</li> <li>FPN_num_filters: Int, number of FPN filters.</li> <li>FPN_cell_repeats: Int, number of FPN blocks.</li> <li>box_class_repeats: Int, Number of regression     and classification blocks.</li> <li>anchor_scale: Int, number of anchor scales.</li> <li>fusion: Str, feature fusion weighting method.</li> <li>return_base: Bool, whether to return base or not.</li> <li>model_name: Str, EfficientDet model name.</li> <li>scaling_coefficients: Tuple, EfficientNet scaling coefficients.</li> </ul> <p>Returns</p> <ul> <li>model: EfficientDet-D7 model.</li> </ul>"},{"location":"models/keypoint/","title":"Keypoints","text":"<p>Models for 2D keypoint estimation</p> <p>[source]</p>"},{"location":"models/keypoint/#keypointnet","title":"KeypointNet","text":"<pre><code>paz.models.keypoint.keypointnet.KeypointNet(input_shape, num_keypoints, depth=0.2, filters=64, alpha=0.1)\n</code></pre> <p>Keypointnet model for discovering keypoint locations in 3D space</p> <p>Arguments</p> <ul> <li>input_shape: List of integers indicating <code>[H, W, num_channels)</code>.</li> <li>num_keypoints: Int. Number of keypoints to discover.</li> <li>depth: Float. Prior depth (centimeters) of keypoints.</li> <li>filters: Int. Number of filters used in convolutional layers.</li> <li>alpha: Float. Alpha parameter of leaky relu.</li> </ul> <p>Returns</p> <p>Keras/tensorflow model</p> <p>References</p> <ul> <li>Discovery of Latent 3D Keypoints via End-to-end     Geometric Reasoning</li> </ul> <p>[source]</p>"},{"location":"models/keypoint/#keypointnet2d","title":"KeypointNet2D","text":"<pre><code>paz.models.keypoint.keypointnet.KeypointNet2D(input_shape, num_keypoints, filters=64, alpha=0.1)\n</code></pre> <p>Model for discovering keypoint locations in 2D space, modified from</p> <p>Arguments</p> <ul> <li>input_shape: List of integers indicating <code>[H, W, num_channels]</code>.</li> <li>num_keypoints: Int. Number of keypoints to discover.</li> <li>filters: Int. Number of filters used in convolutional layers.</li> <li>alpha: Float. Alpha parameter of leaky relu.</li> </ul> <p>Returns</p> <p>Keras/tensorflow model</p> <p>References</p> <ul> <li>Discovery of Latent 3D Keypoints via End-to-end     Geometric Reasoning</li> </ul> <p>[source]</p>"},{"location":"models/keypoint/#projector","title":"Projector","text":"<pre><code>paz.models.keypoint.projector.Projector(self, focal_length, use_numpy=False)\n</code></pre> <p>Projects keypoints from image coordinates to 3D space and viceversa. This model uses the camera focal length and the depth estimation of a point to project it to image coordinates. It works with numpy matrices or tensorflow values. See <code>use_numpy</code>.</p> <p>Arguments</p> <ul> <li>focal_length: Float. Focal length of camera used to generate keypoints.</li> <li>use_numpy: Boolean. If <code>True</code> both unproject and project functions     take numpy arrays as inputs. If <code>False</code> takes tf.tensors as inputs.</li> </ul> <p>[source]</p>"},{"location":"models/keypoint/#detnet","title":"DetNet","text":"<pre><code>paz.models.keypoint.detnet.DetNet(input_shape=(128, 128, 3), num_keypoints=21)\n</code></pre> <p>DetNet: Estimate 3D keypoint positions of minimal hand from input color image.</p> <p>Arguments</p> <ul> <li>input_shape: Shape for 128x128 RGB image of left hand.              List of integers. Input shape to the model including only              spatial and channel resolution e.g. (128, 128, 3).</li> <li>num_keypoints: Int. Number of keypoints.</li> </ul> <p>Returns</p> <p>Tensorflow-Keras model. - xyz: Numpy array [num_keypoints, 3]. Normalized 3D keypoint locations. - uv: Numpy array [num_keypoints, 2]. The uv coordinates of the keypoints     on the heat map, whose resolution is 32x32.</p> <p>Reference</p> <p>-Monocular Real-time Hand Shape and Motion Capture using Multi-modal   Data</p> <p>[source]</p>"},{"location":"models/keypoint/#iknet","title":"IKNet","text":"<pre><code>paz.models.keypoint.iknet.IKNet(input_shape=(84, 3), num_keypoints=21, depth=6, width=1024)\n</code></pre> <p>IKNet: Estimate absolute joint angle for the minimal hand keypoints.</p> <p>Arguments</p> <ul> <li>input_shape: [num_keypoint x 4, 3]. Contains 3D keypoints, bone              orientation, refrence keypoint, refrence bone orientation.</li> <li>num_keypoints: Int. Number of keypoints.</li> </ul> <p>Returns</p> <p>Tensorflow-Keras model. absolute joint angle in quaternion representation.</p> <p>Reference</p> <ul> <li>Monocular Real-time Hand Shape and Motion Capture using Multi-modal    Data</li> </ul> <p>[source]</p>"},{"location":"models/keypoint/#simplebaseline","title":"SimpleBaseline","text":"<pre><code>paz.models.keypoint.simplebaselines.SimpleBaseline(input_shape=(32,), num_keypoints=16, keypoints_dim=3, hidden_dim=1024, num_layers=2, rate=1, weights='human36m')\n</code></pre> <p>Model that predicts 3D keypoints from 2D keypoints Arguments</p> <ul> <li>num_keypoints: numer of kepoints</li> <li>keypoints_dim: dimension of keypoints</li> <li>hidden_dim: size of hidden layers</li> <li>input_shape: size of the input</li> <li>num_layers: number of layers</li> <li>rate: dropout drop rate</li> </ul> <p>Returns</p> <p>keypoints3D estimation model</p>"},{"location":"models/layers/","title":"Layers","text":"<p>Custom layers used in our models</p> <p>[source]</p>"},{"location":"models/layers/#conv2dnormalization","title":"Conv2DNormalization","text":"<pre><code>paz.models.layers.Conv2DNormalization(scale, axis=3)\n</code></pre> <p>Normalization layer as described in ParseNet paper.</p> <p>Arguments</p> <ul> <li>scale: Float determining how much to scale the features.</li> <li>axis: Integer specifying axis of image channels.</li> </ul> <p>Returns</p> <p>Feature map tensor normalized with an L2 norm and then scaled.</p> <p>References</p> <ul> <li>ParseNet: Looking Wider to     See Better</li> </ul> <p>[source]</p>"},{"location":"models/layers/#subtractscalar","title":"SubtractScalar","text":"<pre><code>paz.models.layers.SubtractScalar(constant)\n</code></pre> <p>Subtracts scalar value to tensor.</p> <p>Arguments</p> <ul> <li>constant: Float. Value to be subtracted to all tensor values.</li> </ul> <p>[source]</p>"},{"location":"models/layers/#expectedvalue2d","title":"ExpectedValue2D","text":"<pre><code>paz.models.layers.ExpectedValue2D(axes=[2, 3])\n</code></pre> <p>Calculates the expected value along ''axes''.</p> <p>Arguments</p> <ul> <li>axes: List of integers. Axes for which the expected value     will be calculated.</li> </ul> <p>[source]</p>"},{"location":"models/layers/#expecteddepth","title":"ExpectedDepth","text":"<pre><code>paz.models.layers.ExpectedDepth(axes=[2, 3])\n</code></pre> <p>Calculates the expected depth along ''axes''. This layer takes two inputs. First input is a depth estimation tensor. Second input is a probability map of the keypoints. It multiplies both values and calculates the expected depth.</p> <p>Arguments</p> <ul> <li>axes: List of integers. Axes for which the expected value     will be calculated.</li> </ul>"},{"location":"models/pose_estimation/","title":"Pose estimation","text":"<p>[source]</p>"},{"location":"models/pose_estimation/#higherhrnet","title":"HigherHRNet","text":"<pre><code>paz.models.pose_estimation.higher_hrnet.HigherHRNet(weights='COCO', input_shape=(None, None, 3), num_keypoints=17, with_AE_loss=[True, False])\n</code></pre> <p>Human pose estimation detector for any input size of images. Arguments</p> <ul> <li>weights: String or None. If string should be a valid dataset name.     Current valid datasets include <code>COCO</code>.</li> <li>input_shape: List of integers. Input shape to the model including only     spatial and channel resolution e.g. (512, 512, 3).</li> <li>num_keypoints: Int. Number of joints.</li> <li>with_AE_loss: List of boolean.</li> </ul> <p>Reference</p> <ul> <li>HigherHRNet: Scale-Aware Representation Learning for Bottom-Up    Human Pose Estimation</li> </ul>"},{"location":"models/segmentation/","title":"Segmentation","text":"<p>[source]</p>"},{"location":"models/segmentation/#unet_vgg16","title":"UNET_VGG16","text":"<pre><code>paz.models.segmentation.unet.UNET_VGG16(num_classes=1, input_shape=(224, 224, 3), weights='imagenet', freeze_backbone=False, activation='sigmoid', decoder_type='upsample', decode_filters=[256, 128, 64, 32, 16])\n</code></pre> <p>Build a UNET model with a <code>VGG16</code> backbone.</p> <p>Arguments</p> <ul> <li>input_shape: List of integers: <code>(H, W, num_channels)</code>.</li> <li>num_classes: Integer used for output number of channels.</li> <li>branch_names: List of strings containing layer names of <code>BACKBONE()</code>.</li> <li>BACKBONE: Class for instantiating a backbone model</li> <li>weights: String indicating backbone weights e.g.     ''imagenet'', <code>None</code>.</li> <li>freeze_backbone: Boolean. If True <code>BACKBONE()</code> updates are frozen.</li> <li>decoder_type: String indicating decoding function e.g.     ''upsample ''transpose''.</li> <li>decoder_filters: List of integers used in each application of decoder.</li> <li>activation: Output activation of the model.</li> <li>input_tensor: Input tensor. If given <code>shape</code> is overwritten and this     tensor is used instead as input.</li> <li>name: String. indicating the name of the model.</li> </ul> <p>Returns</p> <p>A UNET-VGG16 Keras/tensorflow model.</p> <p>[source]</p>"},{"location":"models/segmentation/#unet_vgg19","title":"UNET_VGG19","text":"<pre><code>paz.models.segmentation.unet.UNET_VGG19(num_classes=1, input_shape=(224, 224, 3), weights='imagenet', freeze_backbone=False, activation='sigmoid', decoder_type='upsample', decode_filters=[256, 128, 64, 32, 16])\n</code></pre> <p>Build a UNET model with a <code>VGG19</code> backbone.</p> <p>Arguments</p> <ul> <li>input_shape: List of integers: <code>(H, W, num_channels)</code>.</li> <li>num_classes: Integer used for output number of channels.</li> <li>branch_names: List of strings containing layer names of <code>BACKBONE()</code>.</li> <li>BACKBONE: Class for instantiating a backbone model</li> <li>weights: String indicating backbone weights e.g.     ''imagenet'', <code>None</code>.</li> <li>freeze_backbone: Boolean. If True <code>BACKBONE()</code> updates are frozen.</li> <li>decoder_type: String indicating decoding function e.g.     ''upsample ''transpose''.</li> <li>decoder_filters: List of integers used in each application of decoder.</li> <li>activation: Output activation of the model.</li> <li>input_tensor: Input tensor. If given <code>shape</code> is overwritten and this     tensor is used instead as input.</li> <li>name: String. indicating the name of the model.</li> </ul> <p>Returns</p> <p>A UNET-VGG19 Keras/tensorflow model.</p> <p>[source]</p>"},{"location":"models/segmentation/#unet_resnet50","title":"UNET_RESNET50","text":"<pre><code>paz.models.segmentation.unet.UNET_RESNET50(num_classes=1, input_shape=(224, 224, 3), weights='imagenet', freeze_backbone=False, activation='sigmoid', decoder_type='upsample', decode_filters=[256, 128, 64, 32, 16])\n</code></pre> <p>Build a UNET model with a <code>RESNET50V2</code> backbone.</p> <p>Arguments</p> <ul> <li>input_shape: List of integers: <code>(H, W, num_channels)</code>.</li> <li>num_classes: Integer used for output number of channels.</li> <li>branch_names: List of strings containing layer names of <code>BACKBONE()</code>.</li> <li>BACKBONE: Class for instantiating a backbone model</li> <li>weights: String indicating backbone weights e.g.     ''imagenet'', <code>None</code>.</li> <li>freeze_backbone: Boolean. If True <code>BACKBONE()</code> updates are frozen.</li> <li>decoder_type: String indicating decoding function e.g.     ''upsample ''transpose''.</li> <li>decoder_filters: List of integers used in each application of decoder.</li> <li>activation: Output activation of the model.</li> <li>input_tensor: Input tensor. If given <code>shape</code> is overwritten and this     tensor is used instead as input.</li> <li>name: String. indicating the name of the model.</li> </ul> <p>Returns</p> <p>A UNET-RESNET50V2 Keras/tensorflow model.</p> <p>[source]</p>"},{"location":"models/segmentation/#unet","title":"UNET","text":"<pre><code>paz.models.segmentation.unet.UNET(input_shape, num_classes, branch_names, BACKBONE, weights, freeze_backbone=False, activation='sigmoid', decoder_type='upsample', decoder_filters=[256, 128, 64, 32, 16], input_tensor=None, name='UNET')\n</code></pre> <p>Build a generic UNET model with a given <code>BACKBONE</code> class.</p> <p>Arguments</p> <ul> <li>input_shape: List of integers: <code>(H, W, num_channels)</code>.</li> <li>num_classes: Integer used for output number of channels.</li> <li>branch_names: List of strings containing layer names of <code>BACKBONE()</code>.</li> <li>BACKBONE: Class for instantiating a backbone model</li> <li>weights: String indicating backbone weights e.g.     ''imagenet'', <code>None</code>.</li> <li>freeze_backbone: Boolean. If True <code>BACKBONE()</code> updates are frozen.</li> <li>decoder_type: String indicating decoding function e.g.     ''upsample ''transpose''.</li> <li>decoder_filters: List of integers used in each application of decoder.</li> <li>activation: Output activation of the model.</li> <li>input_tensor: Input tensor. If given <code>shape</code> is overwritten and this     tensor is used instead as input.</li> <li>name: String. indicating the name of the model.</li> </ul> <p>Returns</p> <p>A UNET Keras/tensorflow model.</p>"},{"location":"optimization/callbacks/","title":"Callbacks","text":"<p>[source]</p>"},{"location":"optimization/callbacks/#drawinferences","title":"DrawInferences","text":"<pre><code>paz.optimization.callbacks.DrawInferences(save_path, images, pipeline, topic='image', verbose=1)\n</code></pre> <p>Saves an image with its corresponding inferences</p> <p>Arguments</p> <ul> <li>save_path: String. Path in which the images will be saved.</li> <li>images: List of numpy arrays of shape.</li> <li>pipeline: Function that takes as input an element of ''images''     and outputs a ''Dict'' with inferences.</li> <li>topic: Key to the ''inferences'' dictionary containing as value the     drawn inferences.</li> <li>verbose: Integer. If is bigger than 1 messages would be displayed.</li> </ul> <p>[source]</p>"},{"location":"optimization/callbacks/#learningratescheduler","title":"LearningRateScheduler","text":"<pre><code>paz.optimization.callbacks.LearningRateScheduler(learning_rate, gamma_decay, scheduled_epochs, verbose=1)\n</code></pre> <p>Callback for reducing learning rate at specific epochs.</p> <p>Arguments</p> <ul> <li>learning_rate: float. Indicates the starting learning rate.</li> <li>gamma_decay: float. In an scheduled epoch the learning rate     is multiplied by this factor.</li> <li>scheduled_epochs: List of integers. Indicates in which epochs     the learning rate will be multiplied by the gamma decay factor.</li> <li>verbose: Integer. If is bigger than 1 messages would be displayed.</li> </ul> <p>[source]</p>"},{"location":"optimization/callbacks/#evaluatemap","title":"EvaluateMAP","text":"<pre><code>paz.optimization.callbacks.EvaluateMAP(data_manager, detector, period, save_path, iou_thresh=0.5)\n</code></pre> <p>Evaluates mean average precision (MAP) of an object detector.</p> <p>Arguments</p> <ul> <li>data_manager: Data manager and loader class. See ''paz.datasets''     for examples.</li> <li>detector: Tensorflow-Keras model.</li> <li>period: Int. Indicates how often the evaluation is performed.</li> <li>save_path: Str.</li> <li>iou_thresh: Float.</li> </ul>"},{"location":"optimization/losses/","title":"Losses","text":"<p>[source]</p>"},{"location":"optimization/losses/#multiboxloss","title":"MultiBoxLoss","text":"<pre><code>paz.optimization.losses.multi_box_loss.MultiBoxLoss(neg_pos_ratio=3, alpha=1.0, max_num_negatives=300)\n</code></pre> <p>Multi-box loss for a single-shot detection architecture.</p> <p>Arguments</p> <ul> <li>neg_pos_ratio: Int. Number of negatives used per positive box.</li> <li>alpha: Float. Weight parameter for localization loss.</li> <li>max_num_negatives: Int. Maximum number of negatives per batch.</li> </ul> <p>References</p> <ul> <li>SSD: Single Shot MultiBox     Detector</li> </ul> <p>[source]</p>"},{"location":"optimization/losses/#keypointnetloss","title":"KeypointNetLoss","text":"<pre><code>paz.optimization.losses.keypointnet_loss.KeypointNetLoss(num_keypoints, focal_length, rotation_noise=0.1, separation_delta=0.05, loss_weights={'consistency': 1.0, 'silhouette': 1.0, 'separation': 1.0, 'relative_pose': 0.2, 'variance': 0.5})\n</code></pre> <p>KeypointNet loss for discovering latent keypoints.</p> <p>Arguments</p> <ul> <li>num_keypints: Int. Number of keypoints to discover.</li> <li>focal_length: Float. Focal length of camera</li> <li>rotation_noise: Float. Noise added to the estimation of the rotation.</li> <li>separation_delta: Float. Delta used for the ''separation'' loss.</li> <li>loss_weights: Dict. having as keys strings with the different losses     names e.g. ''consistency'' and as value the weight used for that     loss.</li> </ul> <p>References</p> <ul> <li>Discovery of Latent 3D Keypoints via End-to-end     Geometric Reasoning</li> </ul> <p>[source]</p>"},{"location":"optimization/losses/#diceloss","title":"DiceLoss","text":"<pre><code>paz.optimization.losses.segmentation.dice_loss.DiceLoss(beta=1.0, class_weights=1.0)\n</code></pre> <p>Computes the F beta loss. The F beta score is the geometric mean of the precision and recall, where the recall is B times more important than the precision.</p> <p>Arguments</p> <ul> <li>beta: Float.</li> <li>class_weights: Float or list of floats of shape <code>(num_classes)</code>.</li> </ul> <p>[source]</p>"},{"location":"optimization/losses/#focalloss","title":"FocalLoss","text":"<pre><code>paz.optimization.losses.segmentation.focal_loss.FocalLoss(gamma=2.0, alpha=0.25)\n</code></pre> <p>Computes the Focal loss. The Focal loss down weights properly classified examples.</p> <p>Arguments</p> <ul> <li>gamma: Float.</li> <li>alpha: Float.</li> <li>class_weights: Float or list of floats of shape <code>(num_classes)</code>.</li> </ul> <p>[source]</p>"},{"location":"optimization/losses/#jaccardloss","title":"JaccardLoss","text":"<pre><code>paz.optimization.losses.segmentation.jaccard_loss.JaccardLoss(class_weights=1.0)\n</code></pre> <p>Computes the Jaccard loss. The Jaccard score is the intersection over union of the predicted with respect to real masks.</p> <p>Arguments</p> <ul> <li>class_weights: Float or list of floats of shape <code>(num_classes)</code>.</li> </ul> <p>[source]</p>"},{"location":"optimization/losses/#weightedreconstruction","title":"WeightedReconstruction","text":"<pre><code>paz.optimization.losses.segmentation.weighted_reconstruction.WeightedReconstruction(beta=3.0)\n</code></pre> <p>Computes L1 reconstruction loss by multiplying positive alpha mask by beta.</p> <p>Arguments</p> <ul> <li>beta: Float. Value used to multiple positive alpha mask values.</li> <li>RGBA_true: Tensor [batch, H, W, 4]. Color with alpha mask label values.</li> <li>RGB_pred: Tensor [batch, H, W, 3]. Predicted RGB values.</li> </ul> <p>Returns</p> <p>Tensor [batch, H, W] with weighted reconstruction loss values.</p> <p>[source]</p>"},{"location":"optimization/losses/#weightedreconstructionwitherror","title":"WeightedReconstructionWithError","text":"<pre><code>paz.optimization.losses.segmentation.weighted_reconstruction.WeightedReconstructionWithError(beta=3.0)\n</code></pre> <p>Computes L1 reconstruction loss by multiplying positive alpha mask by beta.</p> <p>Arguments</p> <ul> <li>RGBA_true: Tensor [batch, H, W, 4]. Color with alpha mask label values.</li> <li>RGBE_pred: Tensor [batch, H, W, 4]. Predicted RGB and error mask.</li> <li>beta: Float. Value used to multiple positive alpha mask values.</li> </ul> <p>Returns</p> <p>Tensor [batch, H, W] with weighted reconstruction loss values.</p>"},{"location":"pipelines/angles/","title":"Angles","text":"<p>[source]</p>"},{"location":"pipelines/angles/#iknethandjointangles","title":"IKNetHandJointAngles","text":"<pre><code>paz.pipelines.angles.IKNetHandJointAngles(links_origin=[[-0.99924976  0.01561216  0.0354427 ]\\n [-0.74495521 -0.14824392  0.30792697]\\n [-0.53770379 -0.13883537  0.558103  ]\\n [-0.30317002 -0.19618662  0.71142176]\\n [-0.02856714 -0.22446067  0.96352525]\\n [-0.06928206 -0.03928359  0.25380709]\\n [ 0.27568132  0.00303977  0.27721079]\\n [ 0.50956244 -0.01066658  0.27600616]\\n [ 0.77477083 -0.02060624  0.26214581]\\n [ 0.          0.          0.        ]\\n [ 0.32916895  0.01964846 -0.05905647]\\n [ 0.57095375  0.00643106 -0.10069535]\\n [ 0.84451634  0.01311267 -0.15696599]\\n [-0.13643704 -0.02616297 -0.24612351]\\n [ 0.16245268 -0.00434333 -0.29994444]\\n [ 0.41073873 -0.02216329 -0.38052812]\\n [ 0.67033013 -0.02615401 -0.45917176]\\n [-0.27312421 -0.08931944 -0.42068156]\\n [-0.09351757 -0.08866681 -0.5526205 ]\\n [ 0.07397581 -0.09596275 -0.66168566]\\n [ 0.26070401 -0.1101406  -0.76655779]], parents=[None, 0, 1, 2, 3, 0, 5, 6, 7, 0, 9, 10, 11, 0, 13, 14, 15, 0, 17, 18, 19], right_hand=False)\n</code></pre> <p>Estimate absolute and relative joint angle for the minimal hand joints using the 3D keypoint locations.</p> <p>Arguments</p> <ul> <li>links_origin: Array. Reference pose of the minimal hand joints.</li> <li>parent: List. Parents of the keypoints from kinematic chain</li> <li>right_hand: Boolean. If 'True', estimate angles for right hand, else             estimate angles for left hand.</li> <li>keypoints3D: Array [num_joints, 3]. 3D location of keypoints.</li> </ul> <p>Returns</p> <ul> <li>absolute_angles: Array [num_joints, 4]. quaternion repesentation</li> <li>relative_angles: Array [num_joints, 3]. axis-angle repesentation</li> </ul>"},{"location":"pipelines/applications/","title":"Applications","text":"<p>Out-of-the-box high-level pipelines for inference. All of these pipelines can be imported too from: <code>paz.pipelines</code></p> <p>[source]</p>"},{"location":"pipelines/applications/#ssd512coco","title":"SSD512COCO","text":"<pre><code>paz.pipelines.detection.SSD512COCO(score_thresh=0.6, nms_thresh=0.45, draw=True)\n</code></pre> <p>Single-shot inference pipeline with SSD512 trained on COCO.</p> <p>Arguments</p> <ul> <li>score_thresh: Float between [0, 1]</li> <li>nms_thresh: Float between [0, 1].</li> <li>draw: Boolean. If <code>True</code> prediction are drawn in the returned image.</li> </ul> <p>Example</p> <pre><code>from paz.pipelines import SSD512COCO\n\ndetect = SSD512COCO()\n\n# apply directly to an image (numpy-array)\ninferences = detect(image)\n</code></pre> <p>Returns</p> <p>A function that takes an RGB image and outputs the predictions    as a dictionary with <code>keys</code>: <code>image</code> and <code>boxes2D</code>.    The corresponding values of these keys contain the image with the drawn    inferences and a list of <code>paz.abstract.messages.Boxes2D</code>.</p> <p>Reference</p> <ul> <li>SSD: Single Shot MultiBox     Detector</li> </ul> <p>[source]</p>"},{"location":"pipelines/applications/#ssd300voc","title":"SSD300VOC","text":"<pre><code>paz.pipelines.detection.SSD300VOC(score_thresh=0.6, nms_thresh=0.45, draw=True)\n</code></pre> <p>Single-shot inference pipeline with SSD300 trained on VOC.</p> <p>Arguments</p> <ul> <li>score_thresh: Float between [0, 1]</li> <li>nms_thresh: Float between [0, 1].</li> <li>draw: Boolean. If <code>True</code> prediction are drawn in the returned image.</li> </ul> <p>Example</p> <pre><code>from paz.pipelines import SSD300VOC\n\ndetect = SSD300VOC()\n\n# apply directly to an image (numpy-array)\ninferences = detect(image)\n</code></pre> <p>Returns</p> <p>A function that takes an RGB image and outputs the predictions as a dictionary with <code>keys</code>: <code>image</code> and <code>boxes2D</code>. The corresponding values of these keys contain the image with the drawn inferences and a list of <code>paz.abstract.messages.Boxes2D</code>.</p> <p>Reference</p> <ul> <li>SSD: Single Shot MultiBox     Detector</li> </ul> <p>[source]</p>"},{"location":"pipelines/applications/#ssd512ycbvideo","title":"SSD512YCBVideo","text":"<pre><code>paz.pipelines.detection.SSD512YCBVideo(score_thresh=0.6, nms_thresh=0.45, draw=True)\n</code></pre> <p>Single-shot inference pipeline with SSD512 trained on YCBVideo.</p> <p>Arguments</p> <ul> <li>score_thresh: Float between [0, 1]</li> <li>nms_thresh: Float between [0, 1].</li> <li>draw: Boolean. If <code>True</code> prediction are drawn in the returned image.</li> </ul> <p>Example</p> <pre><code>from paz.pipelines import SSD512YCBVideo\n\ndetect = SSD512YCBVideo()\n\n# apply directly to an image (numpy-array)\ninferences = detect(image)\n</code></pre> <p>Returns</p> <p>A function that takes an RGB image and outputs the predictions as a dictionary with <code>keys</code>: <code>image</code> and <code>boxes2D</code>. The corresponding values of these keys contain the image with the drawn inferences and a list of <code>paz.abstract.messages.Boxes2D</code>.</p> <p>[source]</p>"},{"location":"pipelines/applications/#ssd300fat","title":"SSD300FAT","text":"<pre><code>paz.pipelines.detection.SSD300FAT(score_thresh=0.6, nms_thresh=0.45, draw=True)\n</code></pre> <p>Single-shot inference pipeline with SSD300 trained on FAT.</p> <p>Arguments</p> <ul> <li>score_thresh: Float between [0, 1]</li> <li>nms_thresh: Float between [0, 1].</li> <li>draw: Boolean. If <code>True</code> prediction are drawn in the returned image.</li> </ul> <p>Example</p> <pre><code>from paz.pipelines import SSD300FAT\n\ndetect = SSD300FAT()\n\n# apply directly to an image (numpy-array)\ninferences = detect(image)\n</code></pre> <p>Returns</p> <p>A function that takes an RGB image and outputs the predictions as a dictionary with <code>keys</code>: <code>image</code> and <code>boxes2D</code>. The corresponding values of these keys contain the image with the drawn inferences and a list of <code>paz.abstract.messages.Boxes2D</code>.</p> <p>[source]</p>"},{"location":"pipelines/applications/#higherhrnethumanpose2d","title":"HigherHRNetHumanPose2D","text":"<pre><code>paz.pipelines.keypoints.HigherHRNetHumanPose2D(dataset='COCO', data_with_center=False, max_num_people=30, with_flip=True, draw=True)\n</code></pre> <p>Estimate human pose 2D keypoints and draw a skeleton.</p> <p>Arguments</p> <ul> <li>model: Weights trained on HigherHRNet model.</li> <li>keypoint_order: List of length 17 (number of keypoints).     where the keypoints are listed order wise.</li> <li>flipped_keypoint_order: List of length 17 (number of keypoints).     Flipped list of keypoint order.</li> <li>dataset: String. Name of the dataset used for training the model.</li> <li>data_with_center: Boolean. True is the model is trained using the     center.</li> </ul> <p>Returns</p> <p>dictonary with the following keys:     image: contains the image with skeleton drawn on it.     keypoints: location of keypoints     score: score of detection</p> <p>[source]</p>"},{"location":"pipelines/applications/#detectminixceptionfer","title":"DetectMiniXceptionFER","text":"<pre><code>paz.pipelines.detection.DetectMiniXceptionFER(offsets=[0, 0], colors=[[255, 0, 0], [45, 90, 45], [255, 0, 255], [255, 255, 0], [0, 0, 255], [0, 255, 255], [0, 255, 0]])\n</code></pre> <p>Emotion classification and detection pipeline.</p> <p>Returns</p> <p>A function that takes an RGB image and outputs the predictions as a dictionary with <code>keys</code>: <code>image</code> and <code>boxes2D</code>. The corresponding values of these keys contain the image with the drawn inferences and a list of <code>paz.abstract.messages.Boxes2D</code>.</p> <p>Example</p> <pre><code>from paz.pipelines import DetectMiniXceptionFER\n\ndetect = DetectMiniXceptionFER()\n\n# apply directly to an image (numpy-array)\ninferences = detect(image)\n</code></pre> <p>Returns</p> <p>A function that takes an RGB image and outputs the predictions as a dictionary with <code>keys</code>: <code>image</code> and <code>boxes2D</code>. The corresponding values of these keys contain the image with the drawn inferences and a list of <code>paz.abstract.messages.Boxes2D</code>.</p> <p>References</p> <ul> <li>Real-time Convolutional Neural Networks for Emotion and     Gender Classification</li> </ul> <p>[source]</p>"},{"location":"pipelines/applications/#minixceptionfer","title":"MiniXceptionFER","text":"<pre><code>paz.pipelines.classification.MiniXceptionFER()\n</code></pre> <p>Mini Xception pipeline for classifying emotions from RGB faces.</p> <p>Example</p> <pre><code>from paz.pipelines import MiniXceptionFER\n\nclassify = MiniXceptionFER()\n\n# apply directly to an image (numpy-array)\ninference = classify(image)\n</code></pre> <p>Returns</p> <p>A function that takes an RGB image and outputs the predictions    as a dictionary with <code>keys</code>: <code>class_names</code> and <code>scores</code>.</p> <p>References</p> <ul> <li>Real-time Convolutional Neural Networks for Emotion and     Gender Classification</li> </ul> <p>[source]</p>"},{"location":"pipelines/applications/#facekeypointnet2d32","title":"FaceKeypointNet2D32","text":"<pre><code>paz.pipelines.keypoints.FaceKeypointNet2D32(draw=True, radius=3)\n</code></pre> <p>KeypointNet2D model trained with Kaggle Facial Detection challenge.</p> <p>Arguments</p> <ul> <li>draw: Boolean indicating if inferences should be drawn.</li> <li>radius: Int. used for drawing the predicted keypoints.</li> </ul> <p>Example</p> <pre><code>from paz.pipelines import FaceKeypointNet2D32\n\nestimate_keypoints= FaceKeypointNet2D32()\n\n# apply directly to an image (numpy-array)\ninference = estimate_keypoints(image)\n</code></pre> <p>Returns</p> <p>A function that takes an RGB image and outputs the predictions as a dictionary with <code>keys</code>: <code>image</code> and <code>keypoints</code>. The corresponding values of these keys contain the image with the drawn inferences and a numpy array representing the keypoints.</p> <p>[source]</p>"},{"location":"pipelines/applications/#headposekeypointnet2d32","title":"HeadPoseKeypointNet2D32","text":"<pre><code>paz.pipelines.pose.HeadPoseKeypointNet2D32(camera, offsets=[0, 0], radius=5, thickness=2)\n</code></pre> <p>Head pose estimation pipeline using a <code>HaarCascade</code> face detector and a pre-trained <code>KeypointNet2D</code> estimation model.</p> <p>Arguments</p> <ul> <li>camera: Instance of <code>paz.backend.camera.Camera</code> with     camera intrinsics.</li> <li>offsets: List of floats indicating the scaled offset to     be added to the <code>Box2D</code> coordinates.</li> <li>radius: Int. radius of keypoint to be drawn.</li> </ul> <p>Example</p> <pre><code>from paz.pipelines import HeadPoseKeypointNet2D32\n\nestimate_pose = HeadPoseKeypointNet2D32()\n\n# apply directly to an image (numpy-array)\ninferences = estimate_pose(image)\n</code></pre> <p>Returns</p> <p>A function that takes an RGB image and outputs the following inferences as keys of a dictionary:     <code>image</code>, <code>boxes2D</code>, <code>keypoints</code> and <code>poses6D</code>.</p> <p>[source]</p>"},{"location":"pipelines/applications/#haarcascadefrontalface","title":"HaarCascadeFrontalFace","text":"<pre><code>paz.pipelines.detection.HaarCascadeFrontalFace(class_name='Face', color=[0, 255, 0], draw=True)\n</code></pre> <p>HaarCascade pipeline for detecting frontal faces</p> <p>Arguments</p> <ul> <li>class_name: String indicating the class name.</li> <li>color: List indicating the RGB color e.g. <code>[0, 255, 0]</code>.</li> <li>draw: Boolean. If <code>False</code> the bounding boxes are not drawn.</li> </ul> <p>Example</p> <pre><code>from paz.pipelines import HaarCascadeFrontalFace\n\ndetect = HaarCascadeFrontalFace()\n\n# apply directly to an image (numpy-array)\ninferences = detect(image)\n</code></pre> <p>Returns</p> <p>A function that takes an RGB image and outputs the predictions as a dictionary with <code>keys</code>: <code>image</code> and <code>boxes2D</code>. The corresponding values of these keys contain the image with the drawn inferences and a list of <code>paz.abstract.messages.Boxes2D</code>.</p> <p>[source]</p>"},{"location":"pipelines/applications/#singlepowerdrillpix2pose6d","title":"SinglePowerDrillPIX2POSE6D","text":"<pre><code>paz.pipelines.pose.SinglePowerDrillPIX2POSE6D(camera, epsilon=0.15, resize=False, draw=True)\n</code></pre> <p>Predicts the pose6D of the YCB 035_power_drill object from an image. Optionally if a box2D message is given it translates the predicted points2D to new origin located at box2D top-left corner.</p> <p>Arguments</p> <ul> <li>camera: PAZ Camera with intrinsic matrix.</li> <li>epsilon: Float. Values below this value would be replaced by 0.</li> <li>resize: Boolean. If True RGB mask is resized before computing PnP.</li> <li>draw: Boolean. If True drawing functions are applied to output image.</li> </ul> <p>Returns</p> <p>Dictionary with inferred points2D, points3D, pose6D and image.</p> <p>[source]</p>"},{"location":"pipelines/applications/#multipowerdrillpix2pose6d","title":"MultiPowerDrillPIX2POSE6D","text":"<pre><code>paz.pipelines.pose.MultiPowerDrillPIX2POSE6D(camera, offsets, epsilon=0.15, resize=False, draw=True)\n</code></pre> <p>Predicts poses6D of multiple instances the YCB 035_power_drill object from an image.</p> <p>Arguments</p> <ul> <li>camera: PAZ Camera with intrinsic matrix.</li> <li>offsets: List of length two containing floats e.g. (x_scale, y_scale)</li> <li>epsilon: Float. Values below this value would be replaced by 0.</li> <li>resize: Boolean. If True RGB mask is resized before computing PnP.</li> <li>draw: Boolean. If True drawing functions are applied to output image.</li> </ul> <p>Returns</p> <p>Dictionary with inferred boxes2D, poses6D and image.</p> <p>[source]</p>"},{"location":"pipelines/applications/#pix2posepowerdrill","title":"PIX2POSEPowerDrill","text":"<pre><code>paz.pipelines.pose.PIX2POSEPowerDrill(camera, score_thresh=0.5, nms_thresh=0.45, offsets=[0.5, 0.5], epsilon=0.15, resize=False, draw=True)\n</code></pre> <p>PIX2POSE inference pipeline with SSD300 trained on FAT and UNET-VGG16 trained with domain randomization for the YCB object 035_power_drill.</p> <p>Arguments</p> <ul> <li>score_thresh: Float between [0, 1] for object detector.</li> <li>nms_thresh: Float between [0, 1] indicating the non-maximum supression.</li> <li>offsets: List of length two containing floats e.g. (x_scale, y_scale)</li> <li>epsilon: Float. Values below this value would be replaced by 0.</li> <li>draw: Boolean. If <code>True</code> prediction are drawn in the returned image.</li> </ul> <p>Returns</p> <p>Dictionary with inferred boxes2D, poses6D and image.</p> <p>[source]</p>"},{"location":"pipelines/applications/#pix2ycbtools6d","title":"PIX2YCBTools6D","text":"<pre><code>paz.pipelines.pose.PIX2YCBTools6D(camera, score_thresh=0.45, nms_thresh=0.15, offsets=[0.25, 0.25], epsilon=0.15, resize=False, draw=True)\n</code></pre> <p>Predicts poses6D of multiple instances of the YCB tools: '035_power_drill', '051_large_clamp', '037_scissors'</p> <p>Arguments</p> <ul> <li>camera: PAZ Camera with intrinsic matrix.</li> <li>score_thresh: Float between [0, 1] for filtering Boxes2D.</li> <li>nsm_thresh: Float between [0, 1] non-maximum-supression filtering.</li> <li>offsets: List of length two containing floats e.g. (x_scale, y_scale)</li> <li>epsilon: Float. Values below this value would be replaced by 0.</li> <li>resize: Boolean. If True RGB mask is resized before computing PnP.</li> <li>draw: Boolean. If True drawing functions are applied to output image.</li> </ul> <p>Returns</p> <p>Dictionary with inferred boxes2D, poses6D and image.</p> <p>[source]</p>"},{"location":"pipelines/applications/#detnethandkeypoints","title":"DetNetHandKeypoints","text":"<pre><code>paz.pipelines.keypoints.DetNetHandKeypoints(shape=(128, 128), draw=True, right_hand=False)\n</code></pre> <p>Estimate 2D and 3D keypoints from minimal hand and draw a skeleton.</p> <p>Arguments</p> <ul> <li>shape: List/tuple. Input image shape for DetNet model.</li> <li>draw: Boolean. Draw hand skeleton if true.</li> <li>right_hand: Boolean. If 'True', detect keypoints for right hand, else             detect keypoints for left hand.</li> <li>input_image: Array</li> </ul> <p>Returns</p> <ul> <li>image: contains the image with skeleton drawn on it.</li> <li>keypoints2D: Array [num_joints, 2]. 2D location of keypoints.</li> <li>keypoints3D: Array [num_joints, 3]. 3D location of keypoints.</li> </ul> <p>[source]</p>"},{"location":"pipelines/applications/#minimalhandposeestimation","title":"MinimalHandPoseEstimation","text":"<pre><code>paz.pipelines.keypoints.MinimalHandPoseEstimation(draw=True, right_hand=False)\n</code></pre> <p>Estimate 2D and 3D keypoints from minimal hand and draw a skeleton. Estimate absolute and relative joint angle for the minimal hand joints using the 3D keypoint locations.</p> <p>Arguments</p> <ul> <li>draw: Boolean. Draw hand skeleton if true.</li> <li>right_hand: Boolean. If 'True', detect keypoints for right hand, else             detect keypoints for left hand.</li> </ul> <p>Returns</p> <ul> <li>image: contains the image with skeleton drawn on it.</li> <li>keypoints2D: Array [num_joints, 2]. 2D location of keypoints.</li> <li>keypoints3D: Array [num_joints, 3]. 3D location of keypoints.</li> <li>absolute_angles: Array [num_joints, 4]. quaternion repesentation</li> <li>relative_angles: Array [num_joints, 3]. axis-angle repesentation</li> </ul> <p>[source]</p>"},{"location":"pipelines/applications/#detectminimalhand","title":"DetectMinimalHand","text":"<pre><code>paz.pipelines.keypoints.DetectMinimalHand(detect, estimate_keypoints, offsets=[0, 0], radius=3)\n</code></pre> <p>[source]</p>"},{"location":"pipelines/applications/#classifyhandclosure","title":"ClassifyHandClosure","text":"<pre><code>paz.pipelines.classification.ClassifyHandClosure(draw=True, right_hand=False)\n</code></pre> <p>Pipeline to classify minimal hand closure status.</p> <p>Example</p> <pre><code>from paz.pipelines import ClassifyHandClosure\n\nclassify = ClassifyHandClosure()\n\n# apply directly to an image (numpy-array)\ninference = classify(image)\n</code></pre> <p>Returns</p> <p>A function that takes an RGB image and outputs an image with class    status drawn on it.</p> <p>[source]</p>"},{"location":"pipelines/applications/#ssd512minimalhandpose","title":"SSD512MinimalHandPose","text":"<pre><code>paz.pipelines.detection.SSD512MinimalHandPose(right_hand=False, offsets=[0.25, 0.25])\n</code></pre> <p>Hand detection and minimal hand pose estimation pipeline.</p> <p>Arguments</p> <ul> <li>right_hand: Boolean. True for right hand inference.</li> <li>offsets: List of two elements. Each element must be between [0, 1].</li> </ul> <p>Example</p> <pre><code>from paz.pipelines import SSD512MinimalHandPose\n\ndetect = SSD512MinimalHandPose()\n\n# apply directly to an image (numpy-array)\ninferences = detect(image)\n</code></pre> <p>Returns</p> <p>A function that takes an RGB image and outputs the predictions as a dictionary with <code>keys</code>: <code>image</code>,  <code>boxes2D</code>, <code>Keypoints2D</code>, <code>Keypoints3D</code>. The corresponding values of these keys contain the image with the drawn inferences.</p>"},{"location":"pipelines/classification/","title":"Classification","text":"<p>[source]</p>"},{"location":"pipelines/classification/#minixceptionfer","title":"MiniXceptionFER","text":"<pre><code>paz.pipelines.classification.MiniXceptionFER()\n</code></pre> <p>Mini Xception pipeline for classifying emotions from RGB faces.</p> <p>Example</p> <pre><code>from paz.pipelines import MiniXceptionFER\n\nclassify = MiniXceptionFER()\n\n# apply directly to an image (numpy-array)\ninference = classify(image)\n</code></pre> <p>Returns</p> <p>A function that takes an RGB image and outputs the predictions    as a dictionary with <code>keys</code>: <code>class_names</code> and <code>scores</code>.</p> <p>References</p> <ul> <li>Real-time Convolutional Neural Networks for Emotion and     Gender Classification</li> </ul> <p>[source]</p>"},{"location":"pipelines/classification/#classifyhandclosure","title":"ClassifyHandClosure","text":"<pre><code>paz.pipelines.classification.ClassifyHandClosure(draw=True, right_hand=False)\n</code></pre> <p>Pipeline to classify minimal hand closure status.</p> <p>Example</p> <pre><code>from paz.pipelines import ClassifyHandClosure\n\nclassify = ClassifyHandClosure()\n\n# apply directly to an image (numpy-array)\ninference = classify(image)\n</code></pre> <p>Returns</p> <p>A function that takes an RGB image and outputs an image with class    status drawn on it.</p>"},{"location":"pipelines/detection/","title":"Detection","text":"<p>Built-in pipelines for preprocessing, agumentating and predicting.</p> <p>[source]</p>"},{"location":"pipelines/detection/#augmentboxes","title":"AugmentBoxes","text":"<pre><code>paz.pipelines.detection.AugmentBoxes(mean=(104, 117, 123))\n</code></pre> <p>Perform data augmentation with bounding boxes.</p> <p>Arguments</p> <ul> <li>mean: List of three elements used to fill empty image spaces.</li> </ul> <p>[source]</p>"},{"location":"pipelines/detection/#augmentdetection","title":"AugmentDetection","text":"<pre><code>paz.pipelines.detection.AugmentDetection(prior_boxes, split=0, num_classes=21, size=300, mean=(104, 117, 123), IOU=0.5, variances=[0.1, 0.1, 0.2, 0.2])\n</code></pre> <p>Augment boxes and images for object detection.</p> <p>Arguments</p> <ul> <li>prior_boxes: Numpy array of shape <code>[num_boxes, 4]</code> containing     prior/default bounding boxes.</li> <li>split: Flag from <code>paz.processors.TRAIN</code>, <code>paz.processors.VAL</code>     or <code>paz.processors.TEST</code>. Certain transformations would take     place depending on the flag.</li> <li>num_classes: Int.</li> <li>size: Int. Image size.</li> <li>mean: List of three elements indicating the per channel mean.</li> <li>IOU: Float. Intersection over union used to match boxes.</li> <li>variances: List of two floats indicating variances to be encoded     for encoding bounding boxes.</li> </ul> <p>[source]</p>"},{"location":"pipelines/detection/#preprocessboxes","title":"PreprocessBoxes","text":"<pre><code>paz.pipelines.detection.PreprocessBoxes(num_classes, prior_boxes, IOU, variances)\n</code></pre> <p>Preprocess bounding boxes</p> <p>Arguments</p> <ul> <li>num_classes: Int.</li> <li>prior_boxes: Numpy array of shape <code>[num_boxes, 4]</code> containing     prior/default bounding boxes.</li> <li>IOU: Float. Intersection over union used to match boxes.</li> <li>variances: List of two floats indicating variances to be encoded     for encoding bounding boxes.</li> </ul> <p>[source]</p>"},{"location":"pipelines/detection/#postprocessboxes2d","title":"PostprocessBoxes2D","text":"<pre><code>paz.pipelines.detection.PostprocessBoxes2D(offsets, valid_names=None)\n</code></pre> <p>Filters, squares and offsets 2D bounding boxes</p> <p>Arguments</p> <ul> <li>valid_names: List of strings containing class names to keep.</li> <li>offsets: List of length two containing floats e.g. (x_scale, y_scale)</li> </ul> <p>[source]</p>"},{"location":"pipelines/detection/#detectsingleshot","title":"DetectSingleShot","text":"<pre><code>paz.pipelines.detection.DetectSingleShot(model, class_names, score_thresh, nms_thresh, preprocess=None, postprocess=None, variances=[0.1, 0.1, 0.2, 0.2], draw=True)\n</code></pre> <p>Single-shot object detection prediction.</p> <p>Arguments</p> <ul> <li>model: Keras model.</li> <li>class_names: List of strings indicating the class names.</li> <li>preprocess: Callable, pre-processing pipeline.</li> <li>postprocess: Callable, post-processing pipeline.</li> <li>score_thresh: Float between [0, 1]</li> <li>nms_thresh: Float between [0, 1].</li> <li>variances: List, of floats.</li> <li>draw: Boolean. If <code>True</code> prediction are drawn in the     returned image.</li> </ul> <p>[source]</p>"},{"location":"pipelines/detection/#detecthaarcascade","title":"DetectHaarCascade","text":"<pre><code>paz.pipelines.detection.DetectHaarCascade(detector, class_names=None, colors=None, draw=True)\n</code></pre> <p>HaarCascade prediction pipeline/function from RGB-image.</p> <p>Arguments</p> <ul> <li>detector: An instantiated <code>HaarCascadeDetector</code> model.</li> <li>offsets: List of two elements. Each element must be between [0, 1].</li> <li>class_names: List of strings.</li> <li>draw: Boolean. If <code>True</code> prediction are drawn in the returned image.</li> </ul> <p>Returns</p> <p>A function for predicting bounding box detections.</p> <p>[source]</p>"},{"location":"pipelines/detection/#ssd512handdetection","title":"SSD512HandDetection","text":"<pre><code>paz.pipelines.detection.SSD512HandDetection(score_thresh=0.4, nms_thresh=0.45, draw=True)\n</code></pre> <p>Minimal hand detection with SSD512Custom trained on OPenImageV6.</p> <p>Arguments</p> <ul> <li>score_thresh: Float between [0, 1]</li> <li>nms_thresh: Float between [0, 1].</li> <li>draw: Boolean. If <code>True</code> prediction are drawn in the returned image.</li> </ul> <p>Example</p> <pre><code>from paz.pipelines import SSD512HandDetection\n\ndetect = SSD512HandDetection()\n\n# apply directly to an image (numpy-array)\ninferences = detect(image)\n</code></pre> <p>Returns</p> <p>A function that takes an RGB image and outputs the predictions    as a dictionary with <code>keys</code>: <code>image</code> and <code>boxes2D</code>.    The corresponding values of these keys contain the image with the drawn    inferences and a list of <code>paz.abstract.messages.Boxes2D</code>.</p> <p>Reference</p> <ul> <li>SSD: Single Shot MultiBox     Detector</li> </ul> <p>[source]</p>"},{"location":"pipelines/detection/#ssd512minimalhandpose","title":"SSD512MinimalHandPose","text":"<pre><code>paz.pipelines.detection.SSD512MinimalHandPose(right_hand=False, offsets=[0.25, 0.25])\n</code></pre> <p>Hand detection and minimal hand pose estimation pipeline.</p> <p>Arguments</p> <ul> <li>right_hand: Boolean. True for right hand inference.</li> <li>offsets: List of two elements. Each element must be between [0, 1].</li> </ul> <p>Example</p> <pre><code>from paz.pipelines import SSD512MinimalHandPose\n\ndetect = SSD512MinimalHandPose()\n\n# apply directly to an image (numpy-array)\ninferences = detect(image)\n</code></pre> <p>Returns</p> <p>A function that takes an RGB image and outputs the predictions as a dictionary with <code>keys</code>: <code>image</code>,  <code>boxes2D</code>, <code>Keypoints2D</code>, <code>Keypoints3D</code>. The corresponding values of these keys contain the image with the drawn inferences.</p> <p>[source]</p>"},{"location":"pipelines/detection/#ssdpreprocess","title":"SSDPreprocess","text":"<pre><code>paz.pipelines.detection.SSDPreprocess(model, mean=(104, 117, 123), color_space=4)\n</code></pre> <p>Preprocessing pipeline for SSD.</p> <p>Arguments</p> <ul> <li>model: Keras model.</li> <li>mean: List, of three elements indicating the per channel mean.</li> <li>color_space: Int, specifying the color space to transform.</li> </ul> <p>[source]</p>"},{"location":"pipelines/detection/#ssdpostprocess","title":"SSDPostprocess","text":"<pre><code>paz.pipelines.detection.SSDPostprocess(model, class_names, score_thresh, nms_thresh, variances=[0.1, 0.1, 0.2, 0.2], class_arg=0, box_method=0)\n</code></pre> <p>Postprocessing pipeline for SSD.</p> <p>Arguments</p> <ul> <li>model: Keras model.</li> <li>class_names: List, of strings indicating the class names.</li> <li>score_thresh: Float, between [0, 1]</li> <li>nms_thresh: Float, between [0, 1].</li> <li>variances: List, of floats.</li> <li>class_arg: Int, index of class to be removed.</li> <li>box_method: Int, type of boxes to boxes2D conversion method.</li> </ul> <p>[source]</p>"},{"location":"pipelines/detection/#detectsingleshotefficientdet","title":"DetectSingleShotEfficientDet","text":"<pre><code>paz.pipelines.detection.DetectSingleShotEfficientDet(model, class_names, score_thresh, nms_thresh, preprocess=None, postprocess=None, draw=True)\n</code></pre> <p>Single-shot object detection prediction for EfficientDet models.</p> <p>Arguments</p> <ul> <li>model: Keras model.</li> <li>class_names: List of strings indicating class names.</li> <li>preprocess: Callable, preprocessing pipeline.</li> <li>postprocess: Callable, postprocessing pipeline.</li> <li>draw: Bool. If <code>True</code> prediction are drawn on the     returned image.</li> </ul> <p>Properties</p> <ul> <li>model: Keras model.</li> <li>draw: Bool.</li> <li>preprocess: Callable.</li> <li>postprocess: Callable.</li> <li>draw_boxes2D: Callable.</li> <li>wrap: Callable.</li> </ul> <p>Methods</p> <p>call()</p> <p>[source]</p>"},{"location":"pipelines/detection/#efficientdetpreprocess","title":"EfficientDetPreprocess","text":"<pre><code>paz.pipelines.detection.EfficientDetPreprocess(model, mean=(123, 117, 104), standard_deviation=(58.4, 57.1, 57.3))\n</code></pre> <p>Preprocessing pipeline for EfficientDet.</p> <p>Arguments</p> <ul> <li>model: Keras model.</li> <li>mean: Tuple, containing mean per channel on ImageNet.</li> <li>standard_deviation: Tuple, containing standard deviations     per channel on ImageNet.</li> </ul> <p>[source]</p>"},{"location":"pipelines/detection/#efficientdetpostprocess","title":"EfficientDetPostprocess","text":"<pre><code>paz.pipelines.detection.EfficientDetPostprocess(model, class_names, score_thresh, nms_thresh, variances=[1.0, 1.0, 1.0, 1.0], class_arg=None)\n</code></pre> <p>Postprocessing pipeline for EfficientDet.</p> <p>Arguments</p> <ul> <li>model: Keras model.</li> <li>class_names: List of strings indicating class names.</li> <li>score_thresh: Float between [0, 1].</li> <li>nms_thresh: Float between [0, 1].</li> <li>variances: List of float values.</li> <li>class_arg: Int, index of the class to be removed.</li> <li>renormalize: Bool, if true scores are renormalized.</li> <li>method: Int, method to convert boxes to <code>Boxes2D</code>.</li> </ul> <p>[source]</p>"},{"location":"pipelines/detection/#efficientdetd0coco","title":"EFFICIENTDETD0COCO","text":"<pre><code>paz.pipelines.detection.EFFICIENTDETD0COCO(score_thresh=0.6, nms_thresh=0.45, draw=True)\n</code></pre> <p>Single-shot inference pipeline with EFFICIENTDETD0 trained on COCO.</p> <p>Arguments</p> <ul> <li>score_thresh: Float between [0, 1]</li> <li>nms_thresh: Float between [0, 1].</li> <li>draw: Boolean. If <code>True</code> prediction are drawn in the     returned image.</li> </ul> <p>References</p> <p>Google AutoML repository implementation of EfficientDet</p> <p>[source]</p>"},{"location":"pipelines/detection/#efficientdetd1coco","title":"EFFICIENTDETD1COCO","text":"<pre><code>paz.pipelines.detection.EFFICIENTDETD1COCO(score_thresh=0.6, nms_thresh=0.45, draw=True)\n</code></pre> <p>Single-shot inference pipeline with EFFICIENTDETD1 trained on COCO.</p> <p>Arguments</p> <ul> <li>score_thresh: Float between [0, 1]</li> <li>nms_thresh: Float between [0, 1].</li> <li>draw: Boolean. If <code>True</code> prediction are drawn in the     returned image.</li> </ul> <p>References</p> <p>Google AutoML repository implementation of EfficientDet</p> <p>[source]</p>"},{"location":"pipelines/detection/#efficientdetd2coco","title":"EFFICIENTDETD2COCO","text":"<pre><code>paz.pipelines.detection.EFFICIENTDETD2COCO(score_thresh=0.6, nms_thresh=0.45, draw=True)\n</code></pre> <p>Single-shot inference pipeline with EFFICIENTDETD2 trained on COCO.</p> <p>Arguments</p> <ul> <li>score_thresh: Float between [0, 1]</li> <li>nms_thresh: Float between [0, 1].</li> <li>draw: Boolean. If <code>True</code> prediction are drawn in the     returned image.</li> </ul> <p>References</p> <p>Google AutoML repository implementation of EfficientDet</p> <p>[source]</p>"},{"location":"pipelines/detection/#efficientdetd3coco","title":"EFFICIENTDETD3COCO","text":"<pre><code>paz.pipelines.detection.EFFICIENTDETD3COCO(score_thresh=0.6, nms_thresh=0.45, draw=True)\n</code></pre> <p>Single-shot inference pipeline with EFFICIENTDETD3 trained on COCO.</p> <p>Arguments</p> <ul> <li>score_thresh: Float between [0, 1]</li> <li>nms_thresh: Float between [0, 1].</li> <li>draw: Boolean. If <code>True</code> prediction are drawn in the     returned image.</li> </ul> <p>References</p> <p>Google AutoML repository implementation of EfficientDet</p> <p>[source]</p>"},{"location":"pipelines/detection/#efficientdetd4coco","title":"EFFICIENTDETD4COCO","text":"<pre><code>paz.pipelines.detection.EFFICIENTDETD4COCO(score_thresh=0.6, nms_thresh=0.45, draw=True)\n</code></pre> <p>Single-shot inference pipeline with EFFICIENTDETD4 trained on COCO.</p> <p>Arguments</p> <ul> <li>score_thresh: Float between [0, 1]</li> <li>nms_thresh: Float between [0, 1].</li> <li>draw: Boolean. If <code>True</code> prediction are drawn in the     returned image.</li> </ul> <p>References</p> <p>Google AutoML repository implementation of EfficientDet</p> <p>[source]</p>"},{"location":"pipelines/detection/#efficientdetd5coco","title":"EFFICIENTDETD5COCO","text":"<pre><code>paz.pipelines.detection.EFFICIENTDETD5COCO(score_thresh=0.6, nms_thresh=0.45, draw=True)\n</code></pre> <p>Single-shot inference pipeline with EFFICIENTDETD5 trained on COCO.</p> <p>Arguments</p> <ul> <li>score_thresh: Float between [0, 1]</li> <li>nms_thresh: Float between [0, 1].</li> <li>draw: Boolean. If <code>True</code> prediction are drawn in the     returned image.</li> </ul> <p>References</p> <p>Google AutoML repository implementation of EfficientDet</p> <p>[source]</p>"},{"location":"pipelines/detection/#efficientdetd6coco","title":"EFFICIENTDETD6COCO","text":"<pre><code>paz.pipelines.detection.EFFICIENTDETD6COCO(score_thresh=0.6, nms_thresh=0.45, draw=True)\n</code></pre> <p>Single-shot inference pipeline with EFFICIENTDETD6 trained on COCO.</p> <p>Arguments</p> <ul> <li>score_thresh: Float between [0, 1]</li> <li>nms_thresh: Float between [0, 1].</li> <li>draw: Boolean. If <code>True</code> prediction are drawn in the     returned image.</li> </ul> <p>References</p> <p>Google AutoML repository implementation of EfficientDet</p> <p>[source]</p>"},{"location":"pipelines/detection/#efficientdetd7coco","title":"EFFICIENTDETD7COCO","text":"<pre><code>paz.pipelines.detection.EFFICIENTDETD7COCO(score_thresh=0.6, nms_thresh=0.45, draw=True)\n</code></pre> <p>Single-shot inference pipeline with EFFICIENTDETD7 trained on COCO.</p> <p>Arguments</p> <ul> <li>score_thresh: Float between [0, 1]</li> <li>nms_thresh: Float between [0, 1].</li> <li>draw: Boolean. If <code>True</code> prediction are drawn in the     returned image.</li> </ul> <p>References</p> <p>Google AutoML repository implementation of EfficientDet</p> <p>[source]</p>"},{"location":"pipelines/detection/#efficientdetd0voc","title":"EFFICIENTDETD0VOC","text":"<pre><code>paz.pipelines.detection.EFFICIENTDETD0VOC(score_thresh=0.6, nms_thresh=0.45, draw=True)\n</code></pre> <p>Single-shot inference pipeline with EFFICIENTDETD0 trained on VOC.</p> <p>Arguments</p> <ul> <li>score_thresh: Float between [0, 1]</li> <li>nms_thresh: Float between [0, 1].</li> <li>draw: Boolean. If <code>True</code> prediction are drawn in the     returned image.</li> </ul> <p>References</p> <p>Google AutoML repository implementation of EfficientDet</p>"},{"location":"pipelines/heatmaps/","title":"Heatmaps","text":"<p>[source]</p>"},{"location":"pipelines/heatmaps/#getheatmapsandtags","title":"GetHeatmapsAndTags","text":"<pre><code>paz.pipelines.heatmaps.GetHeatmapsAndTags(model, flipped_keypoint_order, with_flip, data_with_center, scale_output=True, axes=[0, 3, 1, 2])\n</code></pre> <p>Get Heatmaps and Tags from the model output. Arguments</p> <ul> <li>model: Model weights trained on HigherHRNet model.</li> <li>flipped_keypoint_order: List of length 17 (number of keypoints).     Flipped list of keypoint order.</li> <li>data_with_center: Boolean. True is the model is trained using the     center.</li> <li>image: Numpy array. Input image of shape (H, W)</li> </ul> <p>Returns</p> <ul> <li>heatmaps: Numpy array of shape (1, num_keypoints, H, W)</li> <li>Tags: Numpy array of shape (1, num_keypoints, H, W)</li> </ul>"},{"location":"pipelines/image/","title":"Image","text":"<p>Built-in pipelines for preprocessing, agumentating and predicting.</p> <p>[source]</p>"},{"location":"pipelines/image/#augmentimage","title":"AugmentImage","text":"<pre><code>paz.pipelines.image.AugmentImage()\n</code></pre> <p>Augments an RGB image by randomly changing contrast, brightness saturation and hue.</p> <p>[source]</p>"},{"location":"pipelines/image/#preprocessimage","title":"PreprocessImage","text":"<pre><code>paz.pipelines.image.PreprocessImage(shape, mean=(104, 117, 123))\n</code></pre> <p>Preprocess RGB image by resizing it to the given <code>shape</code>. If a <code>mean</code> is given it is substracted from image and it not the image gets normalized.</p> <p>Arguments</p> <ul> <li>shape: List of two Ints.</li> <li>mean: List of three Ints indicating the per-channel mean to be     subtracted.</li> </ul> <p>[source]</p>"},{"location":"pipelines/image/#decoderpredictor","title":"DecoderPredictor","text":"<pre><code>paz.pipelines.image.DecoderPredictor(decoder)\n</code></pre> <p>Pipeline for predicting decoded image from a latent vector.</p> <p>Arguments</p> <ul> <li>model: Keras model.</li> </ul> <p>[source]</p>"},{"location":"pipelines/image/#encoderpredictor","title":"EncoderPredictor","text":"<pre><code>paz.pipelines.image.EncoderPredictor(encoder)\n</code></pre> <p>Pipeline for predicting latent vector of an encoder.</p> <p>Arguments</p> <ul> <li>model: Keras model.</li> </ul> <p>[source]</p>"},{"location":"pipelines/image/#preprocessimagehigherhrnet","title":"PreprocessImageHigherHRNet","text":"<pre><code>paz.pipelines.image.PreprocessImageHigherHRNet(scaling_factor=200, input_size=512, multiple=64)\n</code></pre> <p>Transform the image according to the HigherHRNet model requirement. Arguments</p> <ul> <li>scaling_factor: Int. scale factor for image dimensions.</li> <li>input_size: Int. resize the first dimension of image to input size.</li> <li>inverse: Boolean. Reverse the affine transform input.</li> <li>image: Numpy array. Input image</li> </ul> <p>Returns</p> <ul> <li>image: resized and transformed image</li> <li>center: center of the image</li> <li>scale: scaled image dimensions</li> </ul>"},{"location":"pipelines/keypoints/","title":"Keypoints","text":"<p>Built-in pipelines for preprocessing, agumentating and predicting.</p> <p>[source]</p>"},{"location":"pipelines/keypoints/#keypointnetinference","title":"KeypointNetInference","text":"<pre><code>paz.pipelines.keypoints.KeypointNetInference(model, num_keypoints=None, radius=5)\n</code></pre> <p>Performs inference from a <code>KeypointNetShared</code> model.</p> <p>Arguments</p> <ul> <li>model: Keras model for predicting keypoints.</li> <li>num_keypoints: Int or None. If None <code>num_keypoints</code> is     tried to be inferred from <code>model.output_shape</code></li> <li>radius: Int. used for drawing the predicted keypoints.</li> </ul> <p>[source]</p>"},{"location":"pipelines/keypoints/#keypointnetsharedaugmentation","title":"KeypointNetSharedAugmentation","text":"<pre><code>paz.pipelines.keypoints.KeypointNetSharedAugmentation(renderer, size)\n</code></pre> <p>Wraps <code>RenderTwoViews</code> as a sequential processor for using it directly with a <code>paz.GeneratingSequence</code>.</p> <p>Arguments</p> <ul> <li>renderer: <code>RenderTwoViews</code> processor.</li> <li>size: Image size.</li> </ul> <p>[source]</p>"},{"location":"pipelines/keypoints/#estimatekeypoints2d","title":"EstimateKeypoints2D","text":"<pre><code>paz.pipelines.keypoints.EstimateKeypoints2D(model, num_keypoints, draw=True, radius=3, color=4)\n</code></pre> <p>Basic 2D keypoint prediction pipeline.</p> <p>Arguments</p> <ul> <li>model: Keras model for predicting keypoints.</li> <li>num_keypoints: Int or None. If None <code>num_keypoints</code> is     tried to be inferred from <code>model.output_shape</code></li> <li>draw: Boolean indicating if inferences should be drawn.</li> <li>radius: Int. used for drawing the predicted keypoints.</li> </ul> <p>[source]</p>"},{"location":"pipelines/keypoints/#detectkeypoints2d","title":"DetectKeypoints2D","text":"<pre><code>paz.pipelines.detection.DetectKeypoints2D(detect, estimate_keypoints, offsets=[0, 0], radius=3)\n</code></pre> <p>[source]</p>"},{"location":"pipelines/keypoints/#getkeypoints","title":"GetKeypoints","text":"<pre><code>paz.pipelines.keypoints.GetKeypoints(max_num_instance, keypoint_order, detection_thresh=0.2, tag_thresh=1)\n</code></pre> <p>Extract out the top k keypoints heatmaps and group the keypoints with their respective tags value. Adjust and refine the keypoint locations by removing the margins. Arguments</p> <ul> <li>max_num_instance: Int. Maximum number of instances to be detected.</li> <li>keypoint_order: List of length 17 (number of keypoints).</li> <li>heatmaps: Numpy array of shape (1, num_keypoints, H, W)</li> <li>Tags: Numpy array of shape (1, num_keypoints, H, W, 2)</li> </ul> <p>Returns</p> <ul> <li>grouped_keypoints: numpy array. keypoints grouped by tag</li> <li>scores: int: score for the keypoint</li> </ul> <p>[source]</p>"},{"location":"pipelines/keypoints/#transformkeypoints","title":"TransformKeypoints","text":"<pre><code>paz.pipelines.keypoints.TransformKeypoints(inverse=False)\n</code></pre> <p>Transform the keypoint coordinates. Arguments</p> <ul> <li>grouped_keypoints: Numpy array. keypoints grouped by tag</li> <li>center: Tuple. center of the imput image</li> <li>scale: Float. scaled imput image dimension</li> <li>shape: Tuple/List</li> </ul> <p>Returns</p> <ul> <li>transformed_keypoints: keypoint location with respect to the                        input image</li> </ul> <p>[source]</p>"},{"location":"pipelines/keypoints/#higherhrnethumanpose2d","title":"HigherHRNetHumanPose2D","text":"<pre><code>paz.pipelines.keypoints.HigherHRNetHumanPose2D(dataset='COCO', data_with_center=False, max_num_people=30, with_flip=True, draw=True)\n</code></pre> <p>Estimate human pose 2D keypoints and draw a skeleton.</p> <p>Arguments</p> <ul> <li>model: Weights trained on HigherHRNet model.</li> <li>keypoint_order: List of length 17 (number of keypoints).     where the keypoints are listed order wise.</li> <li>flipped_keypoint_order: List of length 17 (number of keypoints).     Flipped list of keypoint order.</li> <li>dataset: String. Name of the dataset used for training the model.</li> <li>data_with_center: Boolean. True is the model is trained using the     center.</li> </ul> <p>Returns</p> <p>dictonary with the following keys:     image: contains the image with skeleton drawn on it.     keypoints: location of keypoints     score: score of detection</p> <p>[source]</p>"},{"location":"pipelines/keypoints/#detnethandkeypoints","title":"DetNetHandKeypoints","text":"<pre><code>paz.pipelines.keypoints.DetNetHandKeypoints(shape=(128, 128), draw=True, right_hand=False)\n</code></pre> <p>Estimate 2D and 3D keypoints from minimal hand and draw a skeleton.</p> <p>Arguments</p> <ul> <li>shape: List/tuple. Input image shape for DetNet model.</li> <li>draw: Boolean. Draw hand skeleton if true.</li> <li>right_hand: Boolean. If 'True', detect keypoints for right hand, else             detect keypoints for left hand.</li> <li>input_image: Array</li> </ul> <p>Returns</p> <ul> <li>image: contains the image with skeleton drawn on it.</li> <li>keypoints2D: Array [num_joints, 2]. 2D location of keypoints.</li> <li>keypoints3D: Array [num_joints, 3]. 3D location of keypoints.</li> </ul> <p>[source]</p>"},{"location":"pipelines/keypoints/#minimalhandposeestimation","title":"MinimalHandPoseEstimation","text":"<pre><code>paz.pipelines.keypoints.MinimalHandPoseEstimation(draw=True, right_hand=False)\n</code></pre> <p>Estimate 2D and 3D keypoints from minimal hand and draw a skeleton. Estimate absolute and relative joint angle for the minimal hand joints using the 3D keypoint locations.</p> <p>Arguments</p> <ul> <li>draw: Boolean. Draw hand skeleton if true.</li> <li>right_hand: Boolean. If 'True', detect keypoints for right hand, else             detect keypoints for left hand.</li> </ul> <p>Returns</p> <ul> <li>image: contains the image with skeleton drawn on it.</li> <li>keypoints2D: Array [num_joints, 2]. 2D location of keypoints.</li> <li>keypoints3D: Array [num_joints, 3]. 3D location of keypoints.</li> <li>absolute_angles: Array [num_joints, 4]. quaternion repesentation</li> <li>relative_angles: Array [num_joints, 3]. axis-angle repesentation</li> </ul> <p>[source]</p>"},{"location":"pipelines/keypoints/#detectminimalhand","title":"DetectMinimalHand","text":"<pre><code>paz.pipelines.keypoints.DetectMinimalHand(detect, estimate_keypoints, offsets=[0, 0], radius=3)\n</code></pre> <p>[source]</p>"},{"location":"pipelines/keypoints/#estimatehumanpose3d","title":"EstimateHumanPose3D","text":"<pre><code>paz.pipelines.keypoints.EstimateHumanPose3D(input_shape=(32,), num_keypoints=16)\n</code></pre> <p>Estimate human pose 3D from 2D human pose.</p> <p>Arguments</p> <p>input_shape: tuple num_keypoints: Int. Number of keypoints.</p> <p>Return</p> <ul> <li>keypoints3D: human pose 3D</li> </ul> <p>[source]</p>"},{"location":"pipelines/keypoints/#estimatehumanpose","title":"EstimateHumanPose","text":"<pre><code>paz.pipelines.keypoints.EstimateHumanPose(solver, camera_intrinsics, args_to_joints3D=[0, 1, 2, 3, 6, 7, 8, 12, 13, 15, 17, 18, 19, 25, 26, 27], filter=True, draw=True, draw_pose=True)\n</code></pre> <p>Estimates 2D and 3D keypoints of human from an image</p> <p>Arguments</p> <ul> <li>estimate_keypoints_3D: 3D simple baseline model</li> <li>args_to_mean: keypoints indices</li> <li>h36m_to_coco_joints2D: h36m joints indices</li> </ul> <p>Returns</p> <p>keypoints2D, keypoints3D</p>"},{"location":"pipelines/masks/","title":"Masks","text":"<p>[source]</p>"},{"location":"pipelines/masks/#rgbmasktoimagepoints2d","title":"RGBMaskToImagePoints2D","text":"<pre><code>paz.pipelines.masks.RGBMaskToImagePoints2D()\n</code></pre> <p>Predicts 2D image keypoints from an RGB mask.</p> <p>[source]</p>"},{"location":"pipelines/masks/#rgbmasktoobjectpoints3d","title":"RGBMaskToObjectPoints3D","text":"<pre><code>paz.pipelines.masks.RGBMaskToObjectPoints3D(object_sizes)\n</code></pre> <p>Predicts 3D keypoints from an RGB mask. Arguments</p> <ul> <li>object_sizes: Array (3) determining the (width, height, depth)</li> </ul> <p>[source]</p>"},{"location":"pipelines/masks/#predictrgbmask","title":"PredictRGBMask","text":"<pre><code>paz.pipelines.masks.PredictRGBMask(model, epsilon=0.15)\n</code></pre> <p>Predicts RGB mask from a segmentation model Arguments</p> <ul> <li>model: Keras segmentation model.</li> <li>epsilon: Float. Values below this value would be replaced by 0.</li> </ul> <p>[source]</p>"},{"location":"pipelines/masks/#pix2points","title":"Pix2Points","text":"<pre><code>paz.pipelines.masks.Pix2Points(model, object_sizes, epsilon=0.15, resize=False, method=1)\n</code></pre> <p>Predicts RGB_mask and corresponding points2D and points3D.</p> <p>Arguments</p> <ul> <li>model: Keras segmentation model.</li> <li>object_sizes: Array (3) determining the (width, height, depth)</li> <li>epsilon: Float. Values below this value would be replaced by 0.</li> <li>resize: Boolean. If True RGB mask is resized to original shape.</li> <li>method: Interpolation method to use if resize is True.</li> </ul> <p>Note</p> <p>Compare with and without RGB interpolation.</p>"},{"location":"pipelines/pose/","title":"Pose","text":"<p>Built-in pipelines for preprocessing, agumentating and predicting.</p> <p>[source]</p>"},{"location":"pipelines/pose/#estimateposekeypoints","title":"EstimatePoseKeypoints","text":"<pre><code>paz.pipelines.pose.EstimatePoseKeypoints(detect, estimate_keypoints, camera, offsets, model_points, class_to_dimensions, radius=3, thickness=1)\n</code></pre> <p>[source]</p>"},{"location":"pipelines/pose/#headposekeypointnet2d32","title":"HeadPoseKeypointNet2D32","text":"<pre><code>paz.pipelines.pose.HeadPoseKeypointNet2D32(camera, offsets=[0, 0], radius=5, thickness=2)\n</code></pre> <p>Head pose estimation pipeline using a <code>HaarCascade</code> face detector and a pre-trained <code>KeypointNet2D</code> estimation model.</p> <p>Arguments</p> <ul> <li>camera: Instance of <code>paz.backend.camera.Camera</code> with     camera intrinsics.</li> <li>offsets: List of floats indicating the scaled offset to     be added to the <code>Box2D</code> coordinates.</li> <li>radius: Int. radius of keypoint to be drawn.</li> </ul> <p>Example</p> <pre><code>from paz.pipelines import HeadPoseKeypointNet2D32\n\nestimate_pose = HeadPoseKeypointNet2D32()\n\n# apply directly to an image (numpy-array)\ninferences = estimate_pose(image)\n</code></pre> <p>Returns</p> <p>A function that takes an RGB image and outputs the following inferences as keys of a dictionary:     <code>image</code>, <code>boxes2D</code>, <code>keypoints</code> and <code>poses6D</code>.</p> <p>[source]</p>"},{"location":"pipelines/pose/#singleinstancepix2pose6d","title":"SingleInstancePIX2POSE6D","text":"<pre><code>paz.pipelines.pose.SingleInstancePIX2POSE6D(model, object_sizes, camera, epsilon=0.15, resize=False, class_name=None, draw=True)\n</code></pre> <p>Predicts a single pose6D from an image. Optionally if a box2D message is given it translates the predicted points2D to new origin located at box2D top-left corner.</p> <p>Arguments</p> <ul> <li>model: Keras segmentation model.</li> <li>object_sizes: Array (3) determining the (width, height, depth)</li> <li>camera: PAZ Camera with intrinsic matrix.</li> <li>epsilon: Float. Values below this value would be replaced by 0.</li> <li>resize: Boolean. If True RGB mask is resized before computing PnP.</li> <li>class_name: Str indicating object name.</li> <li>draw: Boolean. If True drawing functions are applied to output image.</li> </ul> <p>Returns</p> <p>Dictionary with inferred points2D, points3D, pose6D and image.</p> <p>[source]</p>"},{"location":"pipelines/pose/#multiinstancepix2pose6d","title":"MultiInstancePIX2POSE6D","text":"<pre><code>paz.pipelines.pose.MultiInstancePIX2POSE6D(estimate_pose, offsets, camera=None, draw=True)\n</code></pre> <p>Predicts poses6D of multiple instances the same object from an image.</p> <p>Arguments</p> <ul> <li>estimate_pose: Function that takes as input an image and outputs a     dictionary with points2D, points3D and pose6D messages e.g     SingleInstancePIX2POSE6D</li> <li>offsets: List of length two containing floats e.g. (x_scale, y_scale)</li> <li>camera: PAZ Camera with intrinsic matrix.</li> <li>draw: Boolean. If True drawing functions are applied to output image.</li> </ul> <p>Returns</p> <p>Dictionary with inferred boxes2D, poses6D and image.</p> <p>[source]</p>"},{"location":"pipelines/pose/#multiinstancemulticlasspix2pose6d","title":"MultiInstanceMultiClassPIX2POSE6D","text":"<pre><code>paz.pipelines.pose.MultiInstanceMultiClassPIX2POSE6D(detect, name_to_model, name_to_size, camera, offsets, epsilon=0.15, resize=False, draw=True)\n</code></pre> <p>Predicts poses6D of multiple instances of multiple objects from an image</p> <p>Arguments</p> <ul> <li>detect: Function that takes as input an image and outputs a dictionary     containing Boxes2D messages.</li> <li>name_to_model: Dictionary with class name as key and as value a     Keras segmentation model.</li> <li>name_to_size: Dictionary with class name as key and as value the     object sizes.</li> <li>camera: PAZ Camera with intrinsic matrix.</li> <li>offsets: List of length two containing floats e.g. (x_scale, y_scale)</li> <li>epsilon: Float. Values below this value would be replaced by 0.</li> <li>resize: Boolean. If True RGB mask is resized before computing PnP.</li> <li>draw: Boolean. If True drawing functions are applied to output image.</li> </ul> <p>Returns</p> <p>Dictionary with inferred boxes2D, poses6D and image.</p>"},{"location":"pipelines/renderer/","title":"Renderer","text":"<p>[source]</p>"},{"location":"pipelines/renderer/#randomizerenderedimage","title":"RandomizeRenderedImage","text":"<pre><code>paz.pipelines.renderer.RandomizeRenderedImage(image_paths, num_occlusions=1, max_radius_scale=0.5)\n</code></pre> <p>Performs alpha blending and data-augmentation to an image and it's alpha channel. image_paths: List of strings indicating the paths to the images used for the background. num_occlusions: Int. number of occlusions to be added to the image. max_radius_scale: Float between [0, 1] indicating the maximum radius in scale of the image size.</p> <p>[source]</p>"},{"location":"pipelines/renderer/#rendertwoviews","title":"RenderTwoViews","text":"<pre><code>paz.pipelines.renderer.RenderTwoViews(renderer)\n</code></pre> <p>Renders two views along with their transformations.</p> <p>Arguments</p> <ul> <li>renderer: A class with a method <code>render</code> that outputs     two lists. The first list contains two numpy arrays     representing the images e.g. <code>(image_A, image_B)</code> each     of shape <code>[H, W, 3]</code>.     The other list contains three numpy arrays representing the     transformations from the origin to the cameras and the     two alpha channels of both images e.g.     <code>[matrices, alpha_channel_A, alpha_channel_B]</code>.     <code>matrices</code> is a numpy array of shape <code>(4, 4 * 4)</code>.     Each row is a matrix of <code>4 x 4</code> representing the following     transformations respectively: <code>world_to_A</code>, <code>world_to_B</code>,     <code>A_to_world</code> and  <code>B_to_world</code>.     The shape of each <code>alpha_channel</code> should be <code>[H, W]</code>.</li> </ul>"},{"location":"processors/angles/","title":"Angles","text":"<p>[source]</p>"},{"location":"processors/angles/#changelinkorder","title":"ChangeLinkOrder","text":"<pre><code>paz.processors.angles.ChangeLinkOrder(config1_labels, config2_labels)\n</code></pre> <p>Map data from one config to another.</p> <p>Arguments</p> <ul> <li>joints: Array</li> <li>config1_labels: input joint configuration</li> <li>config2_labels: output joint configuration</li> </ul> <p>Returns</p> <ul> <li>Array: joints maped to the config2_labels</li> </ul> <p>[source]</p>"},{"location":"processors/angles/#calculaterelativeangles","title":"CalculateRelativeAngles","text":"<pre><code>paz.processors.angles.CalculateRelativeAngles(right_hand=False, input_config=&lt;class 'paz.datasets.CMU_poanoptic.MANOHandJoints'&gt;, output_config=&lt;class 'paz.datasets.CMU_poanoptic.MPIIHandJoints'&gt;)\n</code></pre> <p>Compute the realtive joint rotation for the minimal hand joints and map it to the output_config kinematic chain form.</p> <p>Arguments</p> <p>absolute_quaternions : Array [num_joints, 4]. Absolute joint angle rotation for the minimal hand joints in quaternion representation [q1, q2, q3, w0].</p> <p>Returns</p> <ul> <li>relative_angles: Array [num_joints, 3].</li> </ul> <p>Relative joint rotation of the minimal hand joints in compact axis angle representation.</p> <p>[source]</p>"},{"location":"processors/angles/#ishandopen","title":"IsHandOpen","text":"<pre><code>paz.processors.angles.IsHandOpen(joint_name_to_arg={'wrist': 0, 'thumb_cmc': 1, 'thumb_mcp': 2, 'thumb_ip': 3, 'thumb_tip': 4, 'index_finger_mcp': 5, 'index_finger_pip': 6, 'index_finger_dip': 7, 'index_finger_tip': 8, 'middle_finger_mcp': 9, 'middle_finger_pip': 10, 'middle_finger_dip': 11, 'middle_finger_tip': 12, 'ring_finger_mcp': 13, 'ring_finger_pip': 14, 'ring_finger_dip': 15, 'ring_finger_tip': 16, 'pinky_mcp': 17, 'pinky_pip': 18, 'pinky_dip': 19, 'pinky_tip': 20}, thresh=0.4)\n</code></pre> <p>Check is the hand is open by by using the relative angles of the joint. Arguments</p> <ul> <li>joint_name_to_arg: Dictionary for the joints</li> <li>thresh: Float. Threshold value for theta</li> <li>relative_angle: Array</li> </ul> <p>Returns</p> <ul> <li>String: Hand is open or closed.</li> </ul>"},{"location":"processors/detection/","title":"Detection","text":"<p>Processors for object detection</p> <p>[source]</p>"},{"location":"processors/detection/#squareboxes2d","title":"SquareBoxes2D","text":"<pre><code>paz.processors.detection.SquareBoxes2D()\n</code></pre> <p>Transforms bounding rectangular boxes into square bounding boxes.</p> <p>[source]</p>"},{"location":"processors/detection/#denormalizeboxes2d","title":"DenormalizeBoxes2D","text":"<pre><code>paz.processors.detection.DenormalizeBoxes2D()\n</code></pre> <p>Denormalizes boxes shapes to be in accordance to the original image size.</p> <p>Arguments:</p> <ul> <li>image_size: List containing height and width of an image.</li> </ul> <p>[source]</p>"},{"location":"processors/detection/#roundboxes2d","title":"RoundBoxes2D","text":"<pre><code>paz.processors.detection.RoundBoxes2D()\n</code></pre> <p>Round to integer box coordinates.</p> <p>[source]</p>"},{"location":"processors/detection/#clipboxes2d","title":"ClipBoxes2D","text":"<pre><code>paz.processors.detection.ClipBoxes2D()\n</code></pre>"},{"location":"processors/detection/#clips-boxes-coordinates-into-the-image-dimensions","title":"Clips boxes coordinates into the image dimensions","text":"<p>[source]</p>"},{"location":"processors/detection/#filterclassboxes2d","title":"FilterClassBoxes2D","text":"<pre><code>paz.processors.detection.FilterClassBoxes2D(valid_class_names)\n</code></pre> <p>Filters boxes with valid class names.</p> <p>Arguments</p> <ul> <li>valid_class_names: List of strings indicating class names to be kept.</li> </ul> <p>[source]</p>"},{"location":"processors/detection/#cropboxes2d","title":"CropBoxes2D","text":"<pre><code>paz.processors.detection.CropBoxes2D()\n</code></pre> <p>Creates a list of images cropped from the bounding boxes.</p> <p>Arguments</p> <ul> <li>offset_scales: List of floats having x and y scales respectively.</li> </ul> <p>[source]</p>"},{"location":"processors/detection/#toboxes2d","title":"ToBoxes2D","text":"<pre><code>paz.processors.detection.ToBoxes2D(class_names=None, one_hot_encoded=False, default_score=1.0, default_class=None, box_method=0)\n</code></pre> <p>Transforms boxes from dataset into <code>Boxes2D</code> messages.</p> <p>Arguments</p> <ul> <li>class_names: List of class names ordered with respect to the     class indices from the dataset <code>boxes</code>.</li> <li>one_hot_encoded: Bool, indicating if scores are one hot vectors.</li> <li>default_score: Float, score to set.</li> <li>default_class: Str, class to set.</li> <li>box_method: Int, method to convert boxes to <code>Boxes2D</code>.</li> </ul> <p>Properties</p> <ul> <li>one_hot_encoded: Bool.</li> <li>box_processor: Callable.</li> </ul> <p>Methods</p> <p>call()</p> <p>[source]</p>"},{"location":"processors/detection/#matchboxes","title":"MatchBoxes","text":"<pre><code>paz.processors.detection.MatchBoxes(prior_boxes, iou=0.5)\n</code></pre> <p>Match prior boxes with ground truth boxes.</p> <p>Arguments</p> <ul> <li>prior_boxes: Numpy array of shape (num_boxes, 4).</li> <li>iou: Float in [0, 1]. Intersection over union in which prior boxes     will be considered positive. A positive box is box with a class     different than <code>background</code>.</li> <li>variance: List of two floats.</li> </ul> <p>[source]</p>"},{"location":"processors/detection/#encodeboxes","title":"EncodeBoxes","text":"<pre><code>paz.processors.detection.EncodeBoxes(prior_boxes, variances=[0.1, 0.1, 0.2, 0.2])\n</code></pre> <p>Encodes bounding boxes.</p> <p>Arguments</p> <ul> <li>prior_boxes: Numpy array of shape (num_boxes, 4).</li> <li>variances: List of two float values.</li> </ul> <p>[source]</p>"},{"location":"processors/detection/#decodeboxes","title":"DecodeBoxes","text":"<pre><code>paz.processors.detection.DecodeBoxes(prior_boxes, variances=[0.1, 0.1, 0.2, 0.2])\n</code></pre> <p>Decodes bounding boxes.</p> <p>Arguments</p> <ul> <li>prior_boxes: Numpy array of shape (num_boxes, 4).</li> <li>variances: List of two float values.</li> </ul> <p>[source]</p>"},{"location":"processors/detection/#nonmaximumsuppressionperclass","title":"NonMaximumSuppressionPerClass","text":"<pre><code>paz.processors.detection.NonMaximumSuppressionPerClass(nms_thresh=0.45, epsilon=0.01)\n</code></pre> <p>Applies non maximum suppression per class.</p> <p>Arguments</p> <ul> <li>nms_thresh: Float between [0, 1].</li> <li>epsilon: Float between [0, 1].</li> </ul> <p>[source]</p>"},{"location":"processors/detection/#mergenmsboxwithclass","title":"MergeNMSBoxWithClass","text":"<pre><code>paz.processors.detection.MergeNMSBoxWithClass()\n</code></pre> <p>Merges box coordinates with their corresponding class defined by <code>class_labels</code> which is decided by best box geometry by non maximum suppression (and not by the best scoring class) into a single output.</p> <p>[source]</p>"},{"location":"processors/detection/#filterboxes","title":"FilterBoxes","text":"<pre><code>paz.processors.detection.FilterBoxes(class_names, conf_thresh=0.5)\n</code></pre> <p>Filters boxes outputted from function <code>detect</code> as <code>Box2D</code> messages.</p> <p>Arguments</p> <ul> <li>class_names: List of class names.</li> <li>conf_thresh: Float between [0, 1].</li> </ul> <p>[source]</p>"},{"location":"processors/detection/#offsetboxes2d","title":"OffsetBoxes2D","text":"<pre><code>paz.processors.detection.OffsetBoxes2D(offsets)\n</code></pre> <p>Offsets the height and widht of a list of <code>Boxes2D</code>.</p> <p>Arguments</p> <ul> <li>offsets: Float between [0, 1].</li> </ul> <p>[source]</p>"},{"location":"processors/detection/#cropimage","title":"CropImage","text":"<pre><code>paz.processors.detection.CropImage()\n</code></pre> <p>Crop images using a list of <code>box2D</code>.</p> <p>[source]</p>"},{"location":"processors/detection/#boxestoboxes2d","title":"BoxesToBoxes2D","text":"<pre><code>paz.processors.detection.BoxesToBoxes2D(default_score=1.0, default_class=None)\n</code></pre> <p>Transforms boxes from dataset into <code>Boxes2D</code> messages given no class names and score.</p> <p>Arguments</p> <ul> <li>default_score: Float, score to set.</li> <li>default_class: Str, class to set.</li> </ul> <p>Properties</p> <ul> <li>default_score: Float.</li> <li>default_class: Str.</li> </ul> <p>Methods</p> <p>call()</p> <p>[source]</p>"},{"location":"processors/detection/#boxeswithonehotvectorstoboxes2d","title":"BoxesWithOneHotVectorsToBoxes2D","text":"<pre><code>paz.processors.detection.BoxesWithOneHotVectorsToBoxes2D(arg_to_class)\n</code></pre> <p>Transforms boxes from dataset into <code>Boxes2D</code> messages given boxes with scores as one hot vectors.</p> <p>Arguments</p> <ul> <li>arg_to_class: List, of classes.</li> </ul> <p>Properties</p> <ul> <li>arg_to_class: List.</li> </ul> <p>Methods</p> <p>call()</p> <p>[source]</p>"},{"location":"processors/detection/#boxeswithclassargtoboxes2d","title":"BoxesWithClassArgToBoxes2D","text":"<pre><code>paz.processors.detection.BoxesWithClassArgToBoxes2D(arg_to_class, default_score=1.0)\n</code></pre> <p>Transforms boxes from dataset into <code>Boxes2D</code> messages given boxes with class argument.</p> <p>Arguments</p> <ul> <li>default_score: Float, score to set.</li> <li>arg_to_class: List, of classes.</li> </ul> <p>Properties</p> <ul> <li>default_score: Float.</li> <li>arg_to_class: List.</li> </ul> <p>Methods</p> <p>call()</p> <p>[source]</p>"},{"location":"processors/detection/#roundboxes","title":"RoundBoxes","text":"<pre><code>paz.processors.detection.RoundBoxes()\n</code></pre> <p>Rounds the floating value coordinates of the box coordinates into integer type.</p> <p>Methods</p> <p>call()</p> <p>[source]</p>"},{"location":"processors/detection/#removeclass","title":"RemoveClass","text":"<pre><code>paz.processors.detection.RemoveClass(class_names, class_arg=None, renormalize=False)\n</code></pre> <p>Remove a particular class from the pipeline.</p> <p>Arguments</p> <ul> <li>class_names: List, indicating given class names.</li> <li>class_arg: Int, index of the class to be removed.</li> <li>renormalize: Bool, if true scores are renormalized.</li> </ul> <p>Properties</p> <ul> <li>class_arg: Int.</li> <li>renormalize: Bool</li> </ul> <p>Methods</p> <p>call()</p> <p>[source]</p>"},{"location":"processors/detection/#scalebox","title":"ScaleBox","text":"<pre><code>paz.processors.detection.ScaleBox()\n</code></pre> <p>Scale box coordinates of the prediction.</p> <p>Arguments</p> <ul> <li>scales: Array of shape <code>()</code>, value to scale boxes.</li> </ul> <p>Properties</p> <ul> <li>scales: Int.</li> </ul> <p>Methods</p> <p>call()</p>"},{"location":"processors/draw/","title":"Draw","text":"<p>Processors for drawing</p> <p>[source]</p>"},{"location":"processors/draw/#drawboxes2d","title":"DrawBoxes2D","text":"<pre><code>paz.processors.draw.DrawBoxes2D(class_names=None, colors=None, weighted=False, scale=0.7, with_score=True)\n</code></pre> <p>Draws bounding boxes from Boxes2D messages.</p> <p>Arguments</p> <ul> <li>class_names: List of strings.</li> <li>colors: List of lists containing the color values</li> <li>weighted: Boolean. If <code>True</code> the colors are weighted with the     score of the bounding box.</li> <li>scale: Float. Scale of drawn text.</li> </ul> <p>[source]</p>"},{"location":"processors/draw/#drawkeypoints2d","title":"DrawKeypoints2D","text":"<pre><code>paz.processors.draw.DrawKeypoints2D(num_keypoints, radius=3, normalized=False)\n</code></pre> <p>Draws keypoints into image.</p> <p>Arguments</p> <ul> <li>num_keypoints: Int. Used initialize colors for each keypoint</li> <li>radius: Float. Approximate radius of the circle in pixel coordinates.</li> </ul> <p>[source]</p>"},{"location":"processors/draw/#drawboxes3d","title":"DrawBoxes3D","text":"<pre><code>paz.processors.draw.DrawBoxes3D(camera, class_to_dimensions, color=(0, 255, 0), thickness=5, radius=2)\n</code></pre> <p>[source]</p>"},{"location":"processors/draw/#drawrandompolygon","title":"DrawRandomPolygon","text":"<pre><code>paz.processors.draw.DrawRandomPolygon(max_radius_scale=0.5)\n</code></pre> <p>Adds occlusion to image</p> <p>Arguments</p> <ul> <li>max_radius_scale: Maximum radius in scale with respect to image i.e.         each vertex radius from the polygon is sampled         from <code>[0, max_radius_scale]</code>. This radius is later         multiplied by the image dimensions.</li> </ul> <p>[source]</p>"},{"location":"processors/draw/#drawpose6d","title":"DrawPose6D","text":"<pre><code>paz.processors.draw.DrawPose6D(object_sizes, camera_intrinsics, thickness=2)\n</code></pre> <p>Draws a single cube in image by projecting points3D.</p> <p>Arguments</p> <ul> <li>object_sizes: Array (3) indicating (x, y, z) sizes of object.</li> <li>camera_intrinsics: Array (3, 3).     Camera intrinsics for projecting 3D rays into 2D image.</li> <li>thickness: Positive integer indicating line thickness.</li> </ul> <p>Returns</p> <p>Image array (H, W) with drawn inferences.</p> <p>[source]</p>"},{"location":"processors/draw/#drawposes6d","title":"DrawPoses6D","text":"<pre><code>paz.processors.draw.DrawPoses6D(object_sizes, camera_intrinsics, thickness=2)\n</code></pre> <p>Draws multiple cubes in image by projecting points3D.</p> <p>Arguments</p> <ul> <li>object_sizes: Array (3) indicating (x, y, z) sizes of object.</li> <li>camera_intrinsics: Array (3, 3).     Camera intrinsics for projecting 3D rays into 2D image.</li> <li>thickness: Positive integer indicating line thickness.</li> </ul> <p>Returns</p> <p>Image array (H, W) with drawn inferences.</p> <p>[source]</p>"},{"location":"processors/draw/#drawhumanskeleton","title":"DrawHumanSkeleton","text":"<pre><code>paz.processors.draw.DrawHumanSkeleton(dataset, check_scores, link_width=2, keypoint_radius=4)\n</code></pre> <p>Draw human pose skeleton on image.</p> <p>Arguments</p> <ul> <li>images: Numpy array.</li> <li>grouped_joints: Joint locations of all the person model detected                 in the image. List of numpy array.</li> <li>dataset: String.</li> <li>check_scores: Boolean. Flag to check score before drawing.</li> </ul> <p>Returns</p> <p>A numpy array containing pose skeleton.</p> <p>[source]</p>"},{"location":"processors/draw/#drawhandskeleton","title":"DrawHandSkeleton","text":"<pre><code>paz.processors.draw.DrawHandSkeleton(check_scores=False, link_width=2, keypoint_radius=4)\n</code></pre> <p>Draw hand pose skeleton on image.</p> <p>Arguments</p> <ul> <li>image: Array (H, W, 3)</li> <li>keypoints: Array. All the joint locations detected by model                 in the image. Returns</li> </ul> <p>A numpy array containing pose skeleton.</p> <p>[source]</p>"},{"location":"processors/draw/#drawrgbmask","title":"DrawRGBMask","text":"<pre><code>paz.processors.draw.DrawRGBMask(object_sizes)\n</code></pre> <p>Draws RGB mask by transforming points3D to RGB space and putting in them in their 2D coordinates (points2D)</p> <p>Arguments</p> <ul> <li>object_sizes: Array (x_size, y_size, z_size)</li> </ul> <p>[source]</p>"},{"location":"processors/draw/#drawrgbmasks","title":"DrawRGBMasks","text":"<pre><code>paz.processors.draw.DrawRGBMasks(object_sizes)\n</code></pre> <p>Draws RGB masks by transforming points3D to RGB space and putting in them in their 2D coordinates (points2D)</p> <p>Arguments</p> <ul> <li>object_sizes: Array (x_size, y_size, z_size)</li> </ul> <p>[source]</p>"},{"location":"processors/draw/#drawtext","title":"DrawText","text":"<pre><code>paz.processors.draw.DrawText(color=(0, 255, 0), thickness=2, scale=1)\n</code></pre> <p>Draws text to image.</p> <p>Arguments</p> <ul> <li>color: List. Color of text to</li> <li>thickness: Int. Thickness of text.</li> <li>scale: Int. Size scale for text.</li> <li>message: Str. Text to be added on the image.</li> <li>location: List/tuple of int. Pixel corordinte in image to add text.</li> </ul> <p>[source]</p>"},{"location":"processors/draw/#drawhumanpose6d","title":"DrawHumanPose6D","text":"<pre><code>paz.processors.draw.DrawHumanPose6D(camera_intrinsics)\n</code></pre> <p>Draw basis vectors for human pose 6D</p> <p>Arguments</p> <ul> <li>image: numpy array</li> <li>rotation: numpy array of size (3 x 3)</li> <li>translations: list of length 3</li> </ul> <p>Returns</p> <ul> <li>image: numpy array</li> </ul>"},{"location":"processors/geometric/","title":"Geometric","text":"<p>Processors for geometric image transformations</p> <p>[source]</p>"},{"location":"processors/geometric/#randomflipboxesleftright","title":"RandomFlipBoxesLeftRight","text":"<pre><code>paz.processors.geometric.RandomFlipBoxesLeftRight()\n</code></pre> <p>Flips image and implemented labels horizontally.</p> <p>[source]</p>"},{"location":"processors/geometric/#toimageboxcoordinates","title":"ToImageBoxCoordinates","text":"<pre><code>paz.processors.geometric.ToImageBoxCoordinates()\n</code></pre> <p>Convert normalized box coordinates to image-size box coordinates.</p> <p>[source]</p>"},{"location":"processors/geometric/#tonormalizedboxcoordinates","title":"ToNormalizedBoxCoordinates","text":"<pre><code>paz.processors.geometric.ToNormalizedBoxCoordinates()\n</code></pre> <p>Convert image-size box coordinates to normalized box coordinates.</p> <p>[source]</p>"},{"location":"processors/geometric/#randomsamplecrop","title":"RandomSampleCrop","text":"<pre><code>paz.processors.geometric.RandomSampleCrop(probability=0.5, max_trials=50)\n</code></pre> <p>Crops image while adjusting the normalized corner form bounding boxes.</p> <p>Arguments</p> <ul> <li>probability: Float between ''[0, 1]''.</li> </ul> <p>[source]</p>"},{"location":"processors/geometric/#randomtranslation","title":"RandomTranslation","text":"<pre><code>paz.processors.geometric.RandomTranslation(delta_scale=[0.25, 0.25], fill_color=None)\n</code></pre> <p>Applies a random translation to image and labels</p> <p>Arguments</p> <ul> <li>delta_scale: List with two elements having the normalized deltas.     e.g. ''[.25, .25]''.</li> </ul> <p>fill_color: List of three integers indicating the color values e.g. ''[0, 0, 0]''.</p> <p>[source]</p>"},{"location":"processors/geometric/#randomrotation","title":"RandomRotation","text":"<pre><code>paz.processors.geometric.RandomRotation(rotation_range=30, fill_color=None, probability=0.5)\n</code></pre> <p>Randomly rotate an images</p> <p>Arguments</p> <ul> <li>rotation_range: Int. indicating the max and min values in degrees     of the uniform distribution <code>[-range, range]</code> from which the     angles are sampled.</li> <li>fill_color: ''None'' or List of three integers indicating the     color values e.g. <code>[0, 0, 0]</code>. If <code>None</code> mean channel values of     the image will be calculated as fill values.</li> <li>probability: Float between 0 and 1.</li> </ul> <p>[source]</p>"},{"location":"processors/geometric/#randomkeypointtranslation","title":"RandomKeypointTranslation","text":"<pre><code>paz.processors.geometric.RandomKeypointTranslation(delta_scale=[0.2, 0.2], fill_color=None, probability=0.5)\n</code></pre> <p>Applies a random translation to image and keypoints.</p> <p>Arguments</p> <ul> <li>delta_scale: List with two elements having the normalized deltas.     e.g. ''[.25, .25]''.</li> <li>fill_color: ''None'' or List of three integers indicating the     color values e.g. ''[0, 0, 0]''. If ''None'' mean channel values of     the image will be calculated as fill values.</li> <li>probability: Float between ''[0, 1]''.</li> </ul> <p>[source]</p>"},{"location":"processors/geometric/#randomkeypointrotation","title":"RandomKeypointRotation","text":"<pre><code>paz.processors.geometric.RandomKeypointRotation(rotation_range=30, fill_color=None, probability=0.5)\n</code></pre> <p>Randomly rotate an images with its corresponding keypoints.</p> <p>Arguments</p> <ul> <li>rotation_range: Int. indicating the max and min values in degrees     of the uniform distribution ''[-range, range]'' from which the     angles are sampled.</li> <li>fill_color: ''None'' or List of three integers indicating the     color values e.g. ''[0, 0, 0]''. If ''None'' mean channel values of     the image will be calculated as fill values.</li> </ul> <p>[source]</p>"},{"location":"processors/geometric/#gettransformationsize","title":"GetTransformationSize","text":"<pre><code>paz.processors.geometric.GetTransformationSize(input_size, multiple)\n</code></pre> <p>Calculate the transformation size for the imgae. The size is tuple of length two indicating the x, y values.</p> <p>Arguments</p> <ul> <li>image: Numpy array</li> </ul> <p>[source]</p>"},{"location":"processors/geometric/#gettransformationscale","title":"GetTransformationScale","text":"<pre><code>paz.processors.geometric.GetTransformationScale(scaling_factor)\n</code></pre> <p>Calculate the transformation scale for the imgae. The scale is a numpy array of size two indicating the width and height scale.</p> <p>Arguments</p> <ul> <li>image: Numpy array</li> <li>size: Numpy array of length 2</li> </ul> <p>[source]</p>"},{"location":"processors/geometric/#getsourcedestinationpoints","title":"GetSourceDestinationPoints","text":"<pre><code>paz.processors.geometric.GetSourceDestinationPoints(scaling_factor)\n</code></pre> <p>Returns the source and destination points for affine transformation.</p> <p>Arguments</p> <ul> <li>center: Numpy array of shape (2,). Center coordinates of image</li> <li>scale: Numpy array of shape (2,). Scale of width and height of image</li> <li>size: List of length 2. Size of image</li> </ul> <p>[source]</p>"},{"location":"processors/geometric/#getimagecenter","title":"GetImageCenter","text":"<pre><code>paz.processors.geometric.GetImageCenter(offset=0.5)\n</code></pre> <p>Calculate the center of the image and add an offset to the center.</p> <p>Arguments</p> <ul> <li>image: Numpy array</li> <li>offset: Float</li> </ul> <p>[source]</p>"},{"location":"processors/geometric/#warpaffine","title":"WarpAffine","text":"<pre><code>paz.processors.geometric.WarpAffine()\n</code></pre> <p>Applies an affine transformation to an image</p> <p>Arguments</p> <ul> <li>image: Numpy array</li> <li>transform: Numpy array. Transformation matrix</li> <li>size: Numpy array. Transformation size</li> </ul>"},{"location":"processors/groups/","title":"Groups","text":"<p>[source]</p>"},{"location":"processors/groups/#toaffinematrix","title":"ToAffineMatrix","text":"<pre><code>paz.processors.groups.ToAffineMatrix()\n</code></pre> <p>Builds affine matrix from a rotation matrix and a translation vector.</p> <p>[source]</p>"},{"location":"processors/groups/#rotationvectortoquaternion","title":"RotationVectorToQuaternion","text":"<pre><code>paz.processors.groups.RotationVectorToQuaternion()\n</code></pre> <p>Transforms rotation vector into quaternion.</p> <p>[source]</p>"},{"location":"processors/groups/#rotationvectortorotationmatrix","title":"RotationVectorToRotationMatrix","text":"<pre><code>paz.processors.groups.RotationVectorToRotationMatrix()\n</code></pre> <p>Transforms rotation vector into a rotation matrix.</p>"},{"location":"processors/heatmaps/","title":"Heatmaps","text":"<p>[source]</p>"},{"location":"processors/heatmaps/#transposeoutput","title":"TransposeOutput","text":"<pre><code>paz.processors.heatmaps.TransposeOutput(axes)\n</code></pre> <p>Transpose the output of the HigherHRNet model Arguments</p> <ul> <li>axes: List or tuple</li> <li>Output: List of numpy array</li> </ul> <p>[source]</p>"},{"location":"processors/heatmaps/#scaleoutput","title":"ScaleOutput","text":"<pre><code>paz.processors.heatmaps.ScaleOutput(scale_factor, full_scaling=False)\n</code></pre> <p>Scale the output of the HigherHRNet model Arguments</p> <ul> <li>scaling_factor: Int.</li> <li>full_scaling: Boolean. If all the array of array are to be scaled.</li> <li>Output: List of numpy array</li> </ul> <p>[source]</p>"},{"location":"processors/heatmaps/#getheatmaps","title":"GetHeatmaps","text":"<pre><code>paz.processors.heatmaps.GetHeatmaps(flipped_keypoint_order)\n</code></pre> <p>Get Heatmaps from the model output. Arguments</p> <ul> <li>flipped_keypoint_order: List of length 17 (number of keypoints).     Flipped list of keypoint order.</li> <li>outputs: List of numpy arrays. Output of HigherHRNet model</li> <li>with_flip: Boolean. indicates whether to flip the output</li> </ul> <p>Returns</p> <ul> <li>heatmaps: Numpy array of shape (1, num_keypoints, H, W)</li> </ul> <p>[source]</p>"},{"location":"processors/heatmaps/#gettags","title":"GetTags","text":"<pre><code>paz.processors.heatmaps.GetTags(flipped_keypoint_order)\n</code></pre> <p>Get Tags from the model output. Arguments</p> <ul> <li>flipped_keypoint_order: List of length 17 (number of keypoints).     Flipped list of keypoint order.</li> <li>outputs: List of numpy arrays. Output of HigherHRNet model</li> <li>with_flip: Boolean. indicates whether to flip the output</li> </ul> <p>Returns</p> <ul> <li>Tags: Numpy array of shape (1, num_keypoints, H, W)</li> </ul> <p>[source]</p>"},{"location":"processors/heatmaps/#removelastelement","title":"RemoveLastElement","text":"<pre><code>paz.processors.heatmaps.RemoveLastElement()\n</code></pre> <p>Remove last element of array Arguments</p> <ul> <li>x: array or list of arrays</li> </ul> <p>[source]</p>"},{"location":"processors/heatmaps/#aggregateresults","title":"AggregateResults","text":"<pre><code>paz.processors.heatmaps.AggregateResults(with_flip=False)\n</code></pre> <p>Aggregate heatmaps and tags to get final heatmaps and tags for processing. Arguments</p> <ul> <li>heatmaps: Numpy array of shape (1, num_keypoints, H, W)</li> <li>Tags: Numpy array of shape (1, num_keypoints, H, W)</li> </ul> <p>Returns</p> <ul> <li>heatmaps: Numpy array of shape (1, num_keypoints, H, W)</li> <li>Tags: Numpy array of shape (1, num_keypoints, H, W, 2)</li> </ul> <p>[source]</p>"},{"location":"processors/heatmaps/#topkdetections","title":"TopKDetections","text":"<pre><code>paz.processors.heatmaps.TopKDetections(k, use_numpy=False)\n</code></pre> <p>Extract out the top k detections Arguments</p> <ul> <li>k: Int. Maximum number of instances to be detected.</li> <li>use_numpy: Boolean. Whether to use numpy functions or tf functions.</li> <li>heatmaps: Numpy array of shape (1, num_joints, H, W)</li> <li>Tags: Numpy array of shape (1, num_joints, H, W, 2)</li> </ul> <p>Returns</p> <ul> <li>top_k_detections: Numpy array. Contains the top k keypoints locations                   of the detection with their value and tags.</li> </ul> <p>[source]</p>"},{"location":"processors/heatmaps/#groupkeypointsbytag","title":"GroupKeypointsByTag","text":"<pre><code>paz.processors.heatmaps.GroupKeypointsByTag(keypoint_order, tag_thresh, detection_thresh)\n</code></pre> <p>Group the keypoints with their respective tags value. Arguments</p> <ul> <li>keypoint_order: List of length 17 (number of keypoints).</li> <li>tag_thresh: Float.</li> <li>detection_thresh: Float.</li> <li>Detection: Numpy array containing the location, value and tags            of top k keypoints</li> </ul> <p>Returns</p> <ul> <li>grouped_keypoints: Numpy array. keypoints grouped by tag</li> </ul> <p>[source]</p>"},{"location":"processors/heatmaps/#adjustkeypointslocations","title":"AdjustKeypointsLocations","text":"<pre><code>paz.processors.heatmaps.AdjustKeypointsLocations()\n</code></pre> <p>Adjust the keypoint locations by removing the margins. Arguments</p> <ul> <li>heatmaps: Numpy array.</li> <li>grouped_keypoints: numpy array. keypoints grouped by tag</li> </ul> <p>[source]</p>"},{"location":"processors/heatmaps/#getscores","title":"GetScores","text":"<pre><code>paz.processors.heatmaps.GetScores()\n</code></pre> <p>Calculate the score of the detection results. Arguments</p> <ul> <li>grouped_keypoints: numpy array. keypoints grouped by tag</li> </ul> <p>[source]</p>"},{"location":"processors/heatmaps/#refinekeypointslocations","title":"RefineKeypointsLocations","text":"<pre><code>paz.processors.heatmaps.RefineKeypointsLocations()\n</code></pre> <p>Refine the keypoint locations by removing the margins. Arguments</p> <ul> <li>heatmaps: Numpy array.</li> <li>Tgas: Numpy array.</li> <li>grouped_keypoints: numpy array. keypoints grouped by tag</li> </ul> <p>[source]</p>"},{"location":"processors/heatmaps/#transformkeypoints","title":"TransformKeypoints","text":"<pre><code>paz.processors.heatmaps.TransformKeypoints()\n</code></pre> <p>Transform keypoint.</p> <p>Arguments</p> <ul> <li>grouped_keypoints: numpy array. keypoints grouped by tag</li> <li>transform: Numpy array. Transformation matrix</li> </ul> <p>[source]</p>"},{"location":"processors/heatmaps/#extractkeypointslocations","title":"ExtractKeypointsLocations","text":"<pre><code>paz.processors.heatmaps.ExtractKeypointsLocations()\n</code></pre> <p>Extract keypoint location.</p> <p>Arguments</p> <ul> <li>keypoints: numpy array</li> </ul>"},{"location":"processors/image/","title":"Image","text":"<p>Processors for image transformations</p> <p>[source]</p>"},{"location":"processors/image/#castimage","title":"CastImage","text":"<pre><code>paz.processors.image.CastImage(dtype)\n</code></pre> <p>Cast image to given dtype.</p> <p>Arguments</p> <ul> <li>dtype: Str or np.dtype</li> </ul> <p>[source]</p>"},{"location":"processors/image/#subtractmeanimage","title":"SubtractMeanImage","text":"<pre><code>paz.processors.image.SubtractMeanImage(mean)\n</code></pre> <p>Subtract channel-wise mean to image.</p> <p>Arguments</p> <ul> <li>mean: List of length 3, containing the channel-wise mean.</li> </ul> <p>[source]</p>"},{"location":"processors/image/#addmeanimage","title":"AddMeanImage","text":"<pre><code>paz.processors.image.AddMeanImage(mean)\n</code></pre> <p>Adds channel-wise mean to image.</p> <p>Arguments</p> <ul> <li>mean: List of length 3, containing the channel-wise mean.</li> </ul> <p>[source]</p>"},{"location":"processors/image/#normalizeimage","title":"NormalizeImage","text":"<pre><code>paz.processors.image.NormalizeImage()\n</code></pre> <p>Normalize image by diving all values by 255.0.</p> <p>[source]</p>"},{"location":"processors/image/#denormalizeimage","title":"DenormalizeImage","text":"<pre><code>paz.processors.image.DenormalizeImage()\n</code></pre> <p>Denormalize image by multiplying all values by 255.0.</p> <p>[source]</p>"},{"location":"processors/image/#loadimage","title":"LoadImage","text":"<pre><code>paz.processors.image.LoadImage(num_channels=3)\n</code></pre> <p>Loads image.</p> <p>Arguments</p> <ul> <li>num_channels: Integer, valid integers are: 1, 3 and 4.</li> </ul> <p>[source]</p>"},{"location":"processors/image/#randomsaturation","title":"RandomSaturation","text":"<pre><code>paz.processors.image.RandomSaturation(lower=0.3, upper=1.5)\n</code></pre> <p>Applies random saturation to an image in RGB space.</p> <p>Arguments</p> <ul> <li>lower: Float, lower bound for saturation factor.</li> <li>upper: Float, upper bound for saturation factor.</li> </ul> <p>[source]</p>"},{"location":"processors/image/#randombrightness","title":"RandomBrightness","text":"<pre><code>paz.processors.image.RandomBrightness(delta=32)\n</code></pre> <p>Adjust random brightness to an image in RGB space.</p> <p>Arguments</p> <ul> <li>max_delta: Float.</li> </ul> <p>[source]</p>"},{"location":"processors/image/#randomcontrast","title":"RandomContrast","text":"<pre><code>paz.processors.image.RandomContrast(lower=0.5, upper=1.5)\n</code></pre> <p>Applies random contrast to an image in RGB</p> <p>Arguments</p> <ul> <li>lower: Float, indicating the lower bound of the random number     to be multiplied with the BGR/RGB image.</li> <li>upper: Float, indicating the upper bound of the random number</li> </ul> <p>to be multiplied with the BGR/RGB image.</p> <p>[source]</p>"},{"location":"processors/image/#randomhue","title":"RandomHue","text":"<pre><code>paz.processors.image.RandomHue(delta=18)\n</code></pre> <p>Applies random hue to an image in RGB space.</p> <p>Arguments</p> <ul> <li>delta: Int, indicating the range (-delta, delta ) of possible     hue values.</li> </ul> <p>[source]</p>"},{"location":"processors/image/#resizeimages","title":"ResizeImages","text":"<pre><code>paz.processors.image.ResizeImages(shape)\n</code></pre> <p>Resize list of images.</p> <p>Arguments</p> <ul> <li>size: List of two ints.</li> </ul> <p>[source]</p>"},{"location":"processors/image/#resizeimages_1","title":"ResizeImages","text":"<pre><code>paz.processors.image.ResizeImages(shape)\n</code></pre> <p>Resize list of images.</p> <p>Arguments</p> <ul> <li>size: List of two ints.</li> </ul> <p>[source]</p>"},{"location":"processors/image/#randomimageblur","title":"RandomImageBlur","text":"<pre><code>paz.processors.image.RandomImageBlur(probability=0.5)\n</code></pre> <p>Randomizes image quality</p> <p>Arguments</p> <ul> <li>probability: Float between [0, 1]. Assigns probability of how     often a random image blur is applied.</li> </ul> <p>[source]</p>"},{"location":"processors/image/#randomgaussianblur","title":"RandomGaussianBlur","text":"<pre><code>paz.processors.image.RandomGaussianBlur(kernel_size=(5, 5), probability=0.5)\n</code></pre> <p>Randomizes image quality</p> <p>Arguments</p> <ul> <li>probability: Float between [0, 1]. Assigns probability of how     often a random image blur is applied.</li> </ul> <p>[source]</p>"},{"location":"processors/image/#randomflipimageleftright","title":"RandomFlipImageLeftRight","text":"<pre><code>paz.processors.image.RandomFlipImageLeftRight()\n</code></pre> <p>Randomly flip the image left or right</p> <p>[source]</p>"},{"location":"processors/image/#convertcolorspace","title":"ConvertColorSpace","text":"<pre><code>paz.processors.image.ConvertColorSpace(flag)\n</code></pre> <p>Converts image to a different color space.</p> <p>Arguments</p> <ul> <li>flag: Flag found in <code>processors</code>indicating transform e.g.     <code>pr.BGR2RGB</code></li> </ul> <p>[source]</p>"},{"location":"processors/image/#showimage","title":"ShowImage","text":"<pre><code>paz.processors.image.ShowImage(window_name='image', wait=True)\n</code></pre> <p>Shows image in a separate window.</p> <p>Arguments</p> <ul> <li>window_name: String. Window name.</li> <li>wait: Boolean</li> </ul> <p>[source]</p>"},{"location":"processors/image/#imagedataprocessor","title":"ImageDataProcessor","text":"<pre><code>paz.processors.image.ImageDataProcessor(generator)\n</code></pre> <p>Wrapper for Keras ImageDataGenerator</p> <p>Arguments</p> <ul> <li>generator: An instantiated Keras ImageDataGenerator</li> </ul> <p>[source]</p>"},{"location":"processors/image/#alphablending","title":"AlphaBlending","text":"<pre><code>paz.processors.image.AlphaBlending()\n</code></pre> <p>Blends image to background using the image's alpha channel.</p> <p>[source]</p>"},{"location":"processors/image/#randomimagecrop","title":"RandomImageCrop","text":"<pre><code>paz.processors.image.RandomImageCrop(crop_factor=0.3, probability=0.5)\n</code></pre> <p>Crops randomly a rectangle from an image.</p> <p>Arguments</p> <ul> <li>crop_factor: Float between <code>[0, 1]</code>.</li> <li>probability: Float between <code>[0, 1]</code>.</li> </ul> <p>[source]</p>"},{"location":"processors/image/#randomshapecrop","title":"RandomShapeCrop","text":"<pre><code>paz.processors.image.RandomShapeCrop(shape)\n</code></pre> <p>Randomly crops a part of an image of always the same given <code>shape</code>.</p> <p>Arguments</p> <ul> <li>shape: List of two ints [height, width].     Dimensions of image to be cropped.</li> </ul> <p>[source]</p>"},{"location":"processors/image/#makerandomplainimage","title":"MakeRandomPlainImage","text":"<pre><code>paz.processors.image.MakeRandomPlainImage(shape)\n</code></pre> <p>Makes random plain image by randomly sampling an RGB color.</p> <p>Arguments</p> <ul> <li>shape: List of two ints [height, width].     Dimensions of plain image to be generated.</li> </ul> <p>[source]</p>"},{"location":"processors/image/#concatenatealphamask","title":"ConcatenateAlphaMask","text":"<pre><code>paz.processors.image.ConcatenateAlphaMask()\n</code></pre> <p>Concatenates alpha mask to original image.</p> <p>[source]</p>"},{"location":"processors/image/#blendrandomcroppedbackground","title":"BlendRandomCroppedBackground","text":"<pre><code>paz.processors.image.BlendRandomCroppedBackground(background_paths)\n</code></pre> <p>Blends image with a randomly cropped background.</p> <p>Arguments</p> <ul> <li>background_paths: List of strings. Each element of the list is a     full-path to an image used for cropping a background.</li> </ul> <p>[source]</p>"},{"location":"processors/image/#addocclusion","title":"AddOcclusion","text":"<pre><code>paz.processors.image.AddOcclusion(max_radius_scale=0.5, probability=0.5)\n</code></pre> <p>Adds a random occlusion to image by generating random vertices and drawing a polygon.</p> <p>Arguments</p> <ul> <li>max_radius_scale: Float between [0, 1].     Value multiplied with largest image dimension to obtain the maximum         radius possible of a vertex in the occlusion polygon.</li> <li>probability: Float between [0, 1]. Assigns probability of how     often an occlusion to an image is generated.</li> </ul> <p>[source]</p>"},{"location":"processors/image/#translateimage","title":"TranslateImage","text":"<pre><code>paz.processors.geometric.TranslateImage(fill_color=None)\n</code></pre> <p>Applies a translation of image. The translation is a list of length two indicating the x, y values.</p> <p>Arguments</p> <ul> <li>fill_color: List of three integers indicating the     color values e.g. <code>[0, 0, 0]</code></li> </ul> <p>[source]</p>"},{"location":"processors/image/#imagetonormalizeddevicecoordinates","title":"ImageToNormalizedDeviceCoordinates","text":"<pre><code>paz.processors.image.ImageToNormalizedDeviceCoordinates()\n</code></pre> <p>Map image value from [0, 255] -&gt; [-1, 1].</p> <p>[source]</p>"},{"location":"processors/image/#normalizeddevicecoordinatestoimage","title":"NormalizedDeviceCoordinatesToImage","text":"<pre><code>paz.processors.image.NormalizedDeviceCoordinatesToImage()\n</code></pre> <p>Map normalized value from [-1, 1] -&gt; [0, 255].</p> <p>[source]</p>"},{"location":"processors/image/#replacelowerthanthreshold","title":"ReplaceLowerThanThreshold","text":"<pre><code>paz.processors.image.ReplaceLowerThanThreshold(threshold=1e-08, replacement=0.0)\n</code></pre> <p>[source]</p>"},{"location":"processors/image/#getnonzerovalues","title":"GetNonZeroValues","text":"<pre><code>paz.processors.image.GetNonZeroValues()\n</code></pre> <p>[source]</p>"},{"location":"processors/image/#getnonzeroarguments","title":"GetNonZeroArguments","text":"<pre><code>paz.processors.image.GetNonZeroArguments()\n</code></pre> <p>[source]</p>"},{"location":"processors/image/#flipleftrightimage","title":"FlipLeftRightImage","text":"<pre><code>paz.processors.image.FlipLeftRightImage()\n</code></pre> <p>Flips an image left and right.</p> <p>Arguments</p> <ul> <li>image: Numpy array.</li> </ul> <p>[source]</p>"},{"location":"processors/image/#dividestandarddeviationimage","title":"DivideStandardDeviationImage","text":"<pre><code>paz.processors.image.DivideStandardDeviationImage(standard_deviation)\n</code></pre> <p>Divide channel-wise standard deviation to image.</p> <p>Arguments</p> <ul> <li>standard_deviation: List of length 3, containing the     channel-wise standard deviation.</li> </ul> <p>Properties</p> <ul> <li>standard_deviation: List.</li> </ul> <p>Methods</p> <p>call()</p> <p>[source]</p>"},{"location":"processors/image/#scaledresize","title":"ScaledResize","text":"<pre><code>paz.processors.image.ScaledResize(image_size)\n</code></pre> <p>Resizes image by returning the scales to original image.</p> <p>Arguments</p> <ul> <li>image_size: Int, desired size of the model input.</li> </ul> <p>Properties</p> <ul> <li>image_size: Int.</li> </ul> <p>Methods</p> <p>call()</p>"},{"location":"processors/keypoints/","title":"Keypoints","text":"<p>Processors for keypoints</p> <p>[source]</p>"},{"location":"processors/keypoints/#changekeypointscoordinatesystem","title":"ChangeKeypointsCoordinateSystem","text":"<pre><code>paz.processors.keypoints.ChangeKeypointsCoordinateSystem()\n</code></pre> <p>Changes <code>keypoints</code> 2D coordinate system using <code>box2D</code> coordinates to locate the new origin at the openCV image origin (top-left).</p> <p>[source]</p>"},{"location":"processors/keypoints/#denormalizekeypoints","title":"DenormalizeKeypoints","text":"<pre><code>paz.processors.keypoints.DenormalizeKeypoints()\n</code></pre> <p>Transform normalized keypoints coordinates into image-size coordinates.</p> <p>Arguments</p> <ul> <li>image_size: List of two floats having height and width of image.</li> </ul> <p>[source]</p>"},{"location":"processors/keypoints/#normalizekeypoints","title":"NormalizeKeypoints","text":"<pre><code>paz.processors.keypoints.NormalizeKeypoints(image_size)\n</code></pre> <p>Transform keypoints in image-size coordinates to normalized coordinates.</p> <p>Arguments</p> <ul> <li>image_size: List of two ints indicating ''(height, width)''</li> </ul> <p>[source]</p>"},{"location":"processors/keypoints/#partitionkeypoints","title":"PartitionKeypoints","text":"<pre><code>paz.processors.keypoints.PartitionKeypoints()\n</code></pre> <p>Partitions keypoints from shape [num_keypoints, 2] into a list of the form ((2), (2), ....) and length equal to num_of_keypoints.</p> <p>[source]</p>"},{"location":"processors/keypoints/#projectkeypoints","title":"ProjectKeypoints","text":"<pre><code>paz.processors.keypoints.ProjectKeypoints(projector, keypoints)\n</code></pre> <p>Projects homogenous keypoints (4D) in the camera coordinates system into image coordinates using a projective transformation.</p> <p>Arguments</p> <ul> <li>projector: Instance of ''paz.models.Project''.</li> <li>keypoints: Numpy array of shape ''(num_keypoints, 3)''</li> </ul> <p>[source]</p>"},{"location":"processors/keypoints/#removekeypointsdepth","title":"RemoveKeypointsDepth","text":"<pre><code>paz.processors.keypoints.RemoveKeypointsDepth()\n</code></pre> <p>Removes Z component from keypoints.</p> <p>[source]</p>"},{"location":"processors/keypoints/#translatekeypoints","title":"TranslateKeypoints","text":"<pre><code>paz.processors.keypoints.TranslateKeypoints()\n</code></pre> <p>Applies a translation to keypoints. The translation is a list of length two indicating the x, y values.</p> <p>[source]</p>"},{"location":"processors/keypoints/#denormalizekeypoints2d","title":"DenormalizeKeypoints2D","text":"<pre><code>paz.processors.keypoints.DenormalizeKeypoints2D()\n</code></pre> <p>Transform normalized keypoints coordinates into image-size coordinates.</p> <p>Arguments</p> <ul> <li>image_size: List of two floats having height and width of image.</li> </ul> <p>[source]</p>"},{"location":"processors/keypoints/#normalizekeypoints2d","title":"NormalizeKeypoints2D","text":"<pre><code>paz.processors.keypoints.NormalizeKeypoints2D(image_size)\n</code></pre> <p>Transform keypoints in image-size coordinates to normalized coordinates.</p> <p>Arguments</p> <ul> <li>image_size: List of two ints indicating ''(height, width)''</li> </ul> <p>[source]</p>"},{"location":"processors/keypoints/#argumentstoimagekeypoints2d","title":"ArgumentsToImageKeypoints2D","text":"<pre><code>paz.processors.keypoints.ArgumentsToImageKeypoints2D()\n</code></pre> <p>Convert array arguments into UV coordinates.</p> <p>Image plane</p> <p>(0,0)--------&gt;  (U) | | | v</p> <p>(V)</p> <p>Arguments</p> <ul> <li>row_args: Array (num_rows).</li> <li>col_args: Array (num_cols).</li> </ul> <p>Returns</p> <p>Array (num_cols, num_rows) representing points2D in UV space.</p> <p>Notes</p> <p>Arguments are row args (V) and col args (U). Image points are in UV     coordinates; thus, we concatenate them in that order     i.e. [col_args, row_args]</p> <p>[source]</p>"},{"location":"processors/keypoints/#scalekeypoints","title":"ScaleKeypoints","text":"<pre><code>paz.processors.keypoints.ScaleKeypoints(scale=1, shape=(128, 128))\n</code></pre> <p>Scale keypoints to input image shape.</p> <p>Arguments</p> <ul> <li>keypoints: Array. Detected keypoints by the model</li> <li>image: Array. Input image.</li> </ul> <p>Returns</p> <p>Scaled keypoints: Array. keypoints scaled to input image shape.</p> <p>[source]</p>"},{"location":"processors/keypoints/#computeorientationvector","title":"ComputeOrientationVector","text":"<pre><code>paz.processors.keypoints.ComputeOrientationVector(parents)\n</code></pre> <p>Calculate the orientation of keypoints links with 3D keypoints.</p> <p>Arguments</p> <ul> <li>keypoints: Array. 3D keypoints</li> </ul> <p>Returns</p> <ul> <li>orientation: Array. Orientation of keypoint links</li> </ul> <p>[source]</p>"},{"location":"processors/keypoints/#mergekeypoints2d","title":"MergeKeypoints2D","text":"<pre><code>paz.processors.keypoints.MergeKeypoints2D(args_to_mean)\n</code></pre> <p>[source]</p>"},{"location":"processors/keypoints/#filterkeypoints2d","title":"FilterKeypoints2D","text":"<pre><code>paz.processors.keypoints.FilterKeypoints2D(args_to_mean, h36m_to_coco_joints2D)\n</code></pre> <p>[source]</p>"},{"location":"processors/keypoints/#standardizekeypoints2d","title":"StandardizeKeypoints2D","text":"<pre><code>paz.processors.keypoints.StandardizeKeypoints2D(data_mean2D, data_stdev2D)\n</code></pre> <p>[source]</p>"},{"location":"processors/keypoints/#destandardizekeypoints2d","title":"DestandardizeKeypoints2D","text":"<pre><code>paz.processors.keypoints.DestandardizeKeypoints2D(data_mean3D, data_stdev3D, dim_to_use)\n</code></pre> <p>[source]</p>"},{"location":"processors/keypoints/#optimizehumanpose3d","title":"OptimizeHumanPose3D","text":"<pre><code>paz.processors.keypoints.OptimizeHumanPose3D(args_to_joints3D, solver, camera_intrinsics)\n</code></pre> <p>Optimize human 3D pose</p>"},{"location":"processors/keypoints/#arguments","title":"Arguments","text":"<p>solver: library solver camera_intrinsics: camera intrinsic parameters</p>"},{"location":"processors/keypoints/#returns","title":"Returns","text":"<p>keypoints3D, optimized keypoints3D</p>"},{"location":"processors/munkres/","title":"Munkres","text":"<p>[source]</p>"},{"location":"processors/munkres/#munkres","title":"Munkres","text":"<pre><code>paz.processors.munkres.Munkres()\n</code></pre> <p>Provides an implementation of the Munkres algorithm.</p> <p>References</p> <p>https://brc2.com/the-algorithm-workshop/ https://software.clapper.org/munkres/ https://github.com/bmc/munkres</p>"},{"location":"processors/pose/","title":"Pose","text":"<p>Processors for pose estimation</p> <p>[source]</p>"},{"location":"processors/pose/#solvepnp","title":"SolvePNP","text":"<pre><code>paz.processors.pose.SolvePNP(points3D, camera, solver=0)\n</code></pre> <p>Calculates 6D pose from 3D points and 2D keypoints correspondences.</p> <p>Arguments</p> <ul> <li>model_points: Numpy array of shape <code>[num_points, 3]</code>.     Model 3D points known in advance.</li> <li>camera: Instance of ''paz.backend.Camera'' containing as properties     the <code>camera_intrinsics</code> a Numpy array of shape <code>[3, 3]</code>     usually calculated from the openCV <code>calibrateCamera</code> function,     and the <code>distortion</code> a Numpy array of shape <code>[5]</code> in which the     elements are usually obtained from the openCV     <code>calibrateCamera</code> function.</li> <li>solver: Flag specifying solvers. Current solvers are:     <code>paz.processors.LEVENBERG_MARQUARDT</code> and <code>paz.processors.UPNP</code>.</li> </ul> <p>Returns</p> <p>Instance from <code>Pose6D</code> message.</p> <p>[source]</p>"},{"location":"processors/pose/#solvechangingobjectpnpransac","title":"SolveChangingObjectPnPRANSAC","text":"<pre><code>paz.processors.pose.SolveChangingObjectPnPRANSAC(camera_intrinsics, inlier_thresh=5, num_iterations=100)\n</code></pre> <p>Returns rotation (Roc) and translation (Toc) vectors that transform 3D points in object frame to camera frame.</p> <p>O------------O /|           /| / |          / | O------------O  | |  |    z    |  | |  O_|_|__O |  /    |___y|  /   object | /    /     | /  coordinates |/    x      |/ O------------O</p> <p>Z                | /                 | Rco, Tco /_____X     &lt;------| | |    camera Y  coordinates</p> <p>Arguments</p> <ul> <li>object_points3D: Array (num_points, 3). Points 3D in object reference     frame. Represented as (0) in image above.</li> <li>image_points2D: Array (num_points, 2). Points in 2D in camera UV space.</li> <li>camera_intrinsics: Array of shape (3, 3). Diagonal elements represent     focal lenghts and last column the image center translation.</li> <li>inlier_threshold: Number of inliers for RANSAC method.</li> <li>num_iterations: Maximum number of iterations.</li> </ul> <p>Returns</p> <p>Boolean indicating success, rotation vector in axis-angle form (3)     and translation vector (3).</p> <p>[source]</p>"},{"location":"processors/pose/#translation3dfromboxwidth","title":"Translation3DFromBoxWidth","text":"<pre><code>paz.processors.pose.Translation3DFromBoxWidth(camera, real_width=0.3)\n</code></pre> <p>Computes 3D translation from box width and real width ratio.</p> <p>Arguments</p> <ul> <li>camera: Instance of ''paz.backend.Camera'' containing as properties     the <code>camera_intrinsics</code> a Numpy array of shape <code>[3, 3]</code>     usually calculated from the openCV <code>calibrateCamera</code> function,     and the <code>distortion</code> a Numpy array of shape <code>[5]</code> in which the     elements are usually obtained from the openCV     <code>calibrateCamera</code> function.</li> <li>real_width: Real width of the predicted box2D.</li> </ul> <p>Returns</p> <p>Array (num_boxes, 3) containing all 3D translations.</p>"},{"location":"processors/renderer/","title":"Renderer","text":"<p>Processors used for rendering</p> <p>[source]</p>"},{"location":"processors/renderer/#render","title":"Render","text":"<pre><code>paz.processors.renderer.Render(renderer)\n</code></pre> <p>Render images and labels.</p> <p>Arguments</p> <ul> <li>renderer: Object that renders images and labels using a method     ''render_sample()''.</li> </ul>"},{"location":"processors/standard/","title":"Standard","text":"<p>Standard processors</p> <p>[source]</p>"},{"location":"processors/standard/#controlmap","title":"ControlMap","text":"<pre><code>paz.processors.standard.ControlMap(processor, intro_indices=[0], outro_indices=[0], keep=None)\n</code></pre> <p>Controls which inputs are passed ''processor'' and the order of its outputs.</p> <p>Arguments</p> <ul> <li>processor: Function e.g. a ''paz.processor''</li> <li>intro_indices: List of Ints.</li> <li>outro_indices: List of Ints.</li> <li>keep: ''None'' or dictionary. If <code>None</code> control maps operates     without explicitly retaining an input. If dict it must contain     as keys the input args to be kept and as values where they should     be located at the end.</li> </ul> <p>[source]</p>"},{"location":"processors/standard/#expanddomain","title":"ExpandDomain","text":"<pre><code>paz.processors.standard.ExpandDomain(processor)\n</code></pre> <p>Extends number of inputs a function can take applying the identity function to all new/extended inputs. e.g. For a given function f(x) = y. If g = ExtendInputs(f), we can now have g(x, x1, x2, ..., xn) = y, x1, x2, ..., xn.</p> <p>Arguments</p> <ul> <li>processor: Function e.g. any procesor in ''paz.processors''.</li> </ul> <p>[source]</p>"},{"location":"processors/standard/#copydomain","title":"CopyDomain","text":"<pre><code>paz.processors.standard.CopyDomain(intro_indices, outro_indices)\n</code></pre> <p>Copies ''intro_indices'' and places it ''outro_indices''.</p> <p>Arguments</p> <ul> <li>intro_indices: List of Ints.</li> <li>outro_indices: List of Ints.</li> </ul> <p>[source]</p>"},{"location":"processors/standard/#extendinputs","title":"ExtendInputs","text":"<pre><code>paz.processors.standard.ExtendInputs(processor)\n</code></pre> <p>Extends number of inputs a function can take applying the identity function to all new/extended inputs. e.g. For a given function f(x) = y. If g = ExtendInputs(f), we can now have g(x, x1, x2, ..., xn) = y, x1, x2, ..., xn.</p> <p>Arguments</p> <ul> <li>processor: Function e.g. any procesor in ''paz.processors''.</li> </ul> <p>[source]</p>"},{"location":"processors/standard/#sequencewrapper","title":"SequenceWrapper","text":"<pre><code>paz.processors.standard.SequenceWrapper(inputs_info, labels_info)\n</code></pre> <p>Wraps arguments to directly use ''paz.abstract.ProcessingSequence'' or ''paz.abstract.GeneratingSequence''.</p> <p>Arguments</p> <ul> <li>inputs_info: Dictionary containing an integer per key representing     the argument to grab, and as value a dictionary containing the     tensor name as key and the tensor shape of a single sample as value     e.g. {0: {'input_image': [300, 300, 3]}, 1: {'depth': [300, 300]}}.     The values given here are for the inputs of the model.</li> <li>labels_info: Dictionary containing an integer per key representing     the argument to grab, and as value a dictionary containing the     tensor name as key and the tensor shape of a single sample as value     e.g. {2: {'classes': [10]}}.     The values given here are for the labels of the model.</li> </ul> <p>[source]</p>"},{"location":"processors/standard/#predict","title":"Predict","text":"<pre><code>paz.processors.standard.Predict(model, preprocess=None, postprocess=None)\n</code></pre> <p>Perform input preprocessing, model prediction and output postprocessing.</p> <p>Arguments</p> <ul> <li>model: Class with a ''predict'' method e.g. a Keras model.</li> <li>preprocess: Function applied to given inputs.</li> <li>postprocess: Function applied to outputted predictions from model.</li> </ul> <p>[source]</p>"},{"location":"processors/standard/#toclassname","title":"ToClassName","text":"<pre><code>paz.processors.standard.ToClassName(labels)\n</code></pre> <p>[source]</p>"},{"location":"processors/standard/#expanddims","title":"ExpandDims","text":"<pre><code>paz.processors.standard.ExpandDims(axis)\n</code></pre> <p>Expand dimension of given array.</p> <p>Arguments</p> <ul> <li>axis: Int.</li> </ul> <p>[source]</p>"},{"location":"processors/standard/#boxclasstoonehotvector","title":"BoxClassToOneHotVector","text":"<pre><code>paz.processors.standard.BoxClassToOneHotVector(num_classes)\n</code></pre> <p>Transform box data with class index to a one-hot encoded vector.</p> <p>Arguments</p> <ul> <li>num_classes: Integer. Total number of classes.</li> </ul> <p>[source]</p>"},{"location":"processors/standard/#squeeze","title":"Squeeze","text":"<pre><code>paz.processors.standard.Squeeze(axis)\n</code></pre> <p>Wrap around numpy <code>squeeze</code> due to common use before model predict. Arguments</p> <ul> <li>expand_dims: Int or list of Ints.</li> <li>topic: String.</li> </ul> <p>[source]</p>"},{"location":"processors/standard/#copy","title":"Copy","text":"<pre><code>paz.processors.standard.Copy()\n</code></pre> <p>Copies value passed to function.</p> <p>[source]</p>"},{"location":"processors/standard/#lambda","title":"Lambda","text":"<pre><code>paz.processors.standard.Lambda(function)\n</code></pre> <p>Applies a lambda function as a processor transformation.</p> <p>Arguments</p> <ul> <li>function: Function.</li> </ul> <p>[source]</p>"},{"location":"processors/standard/#unpackdictionary","title":"UnpackDictionary","text":"<pre><code>paz.processors.standard.UnpackDictionary(order)\n</code></pre> <p>Unpacks dictionary into a tuple. Arguments</p> <ul> <li>order: List of strings containing the keys of the dictionary.     The order of the list is the order in which the tuple     would be ordered.</li> </ul> <p>[source]</p>"},{"location":"processors/standard/#wrapoutput","title":"WrapOutput","text":"<pre><code>paz.processors.standard.WrapOutput(keys)\n</code></pre> <p>Wraps arguments in dictionary</p> <p>Arguments</p> <ul> <li>keys: List of strings representing the keys used to wrap the inputs.     The order of the list must correspond to the same order of     inputs (''args'').</li> </ul> <p>[source]</p>"},{"location":"processors/standard/#concatenate","title":"Concatenate","text":"<pre><code>paz.processors.standard.Concatenate(axis)\n</code></pre> <p>Concatenates a list of arrays in given ''axis''.</p> <p>Arguments</p> <ul> <li>axis: Int.</li> </ul> <p>[source]</p>"},{"location":"processors/standard/#selectelement","title":"SelectElement","text":"<pre><code>paz.processors.standard.SelectElement(index)\n</code></pre> <p>Selects element of input value.</p> <p>Arguments</p> <ul> <li>index: Int. argument to select from ''inputs''.</li> </ul> <p>[source]</p>"},{"location":"processors/standard/#stochasticprocessor","title":"StochasticProcessor","text":"<pre><code>paz.processors.standard.StochasticProcessor(probability=0.5, name=None)\n</code></pre> <p>[source]</p>"},{"location":"processors/standard/#stochastic","title":"Stochastic","text":"<pre><code>paz.processors.standard.Stochastic(function, probability=0.5, name=None)\n</code></pre> <p>[source]</p>"},{"location":"processors/standard/#unwrapdictionary","title":"UnwrapDictionary","text":"<pre><code>paz.processors.standard.UnwrapDictionary(keys)\n</code></pre> <p>Unwraps a dictionry into a list given the key order.</p> <p>[source]</p>"},{"location":"processors/standard/#scale","title":"Scale","text":"<pre><code>paz.processors.standard.Scale(scales)\n</code></pre> <p>Scales an input.</p> <p>[source]</p>"},{"location":"processors/standard/#appendvalues","title":"AppendValues","text":"<pre><code>paz.processors.standard.AppendValues(keys)\n</code></pre> <p>Append dictionary values to lists</p> <p>Arguments</p> <ul> <li>keys: Keys to dictionary values</li> </ul> <p>[source]</p>"},{"location":"processors/standard/#booleantotextmessage","title":"BooleanToTextMessage","text":"<pre><code>paz.processors.standard.BooleanToTextMessage(true_message, false_message)\n</code></pre> <p>Convert a boolean to text message. Arguments</p> <ul> <li>true_message: String. Message for true case.</li> <li>false_message: String. Message for false case.</li> <li>Flag: Boolean.</li> </ul> <p>Returns</p> <ul> <li>message: String.</li> </ul> <p>[source]</p>"},{"location":"processors/standard/#printtopics","title":"PrintTopics","text":"<pre><code>paz.processors.standard.PrintTopics(topics)\n</code></pre> <p>Prints topics Arguments</p> <ul> <li>topics: List of keys to the inputted dictionary</li> </ul> <p>Returns</p> <p>Returns same dictionary but outputs to terminal topic values.</p>"},{"location":"utils/documentation/","title":"Documentation","text":"<p>[source]</p>"},{"location":"utils/documentation/#docstring","title":"docstring","text":"<pre><code>paz.utils.documentation.docstring(original)\n</code></pre> <p>Doctors (documents) <code>target</code> <code>Callable</code> with <code>original</code> docstring.</p> <p>Arguments:</p> <ul> <li>original: Object with documentation string.</li> </ul> <p>Returns</p> <p>Function that replaces <code>target</code> docstring with <code>original</code> docstring.</p>"},{"location":"utils/logger/","title":"Logger","text":"<p>[source]</p>"},{"location":"utils/logger/#build_directory","title":"build_directory","text":"<pre><code>paz.utils.logger.build_directory(root='experiments', label=None)\n</code></pre> <p>Builds and makes directory with time date and user given label.</p> <p>Arguments:</p> <ul> <li>root: String with partial or full path.</li> <li>label: String user label.</li> </ul> <p>Returns</p> <p>Full directory path</p> <p>[source]</p>"},{"location":"utils/logger/#make_directory","title":"make_directory","text":"<pre><code>paz.utils.logger.make_directory(directory_name)\n</code></pre> <p>Makes directory.</p> <p>Arguments:</p> <ul> <li>directory_name: String. Directory name.</li> </ul> <p>[source]</p>"},{"location":"utils/logger/#write_dictionary","title":"write_dictionary","text":"<pre><code>paz.utils.logger.write_dictionary(dictionary, directory, filename, indent=4)\n</code></pre> <p>Writes dictionary as json file.</p> <p>Arguments:</p> <ul> <li>dictionary: Dictionary to write in memory.</li> <li>directory: String. Directory name.</li> <li>filename: String. Filename.</li> <li>indent: Number of spaces between keys.</li> </ul> <p>[source]</p>"},{"location":"utils/logger/#write_weights","title":"write_weights","text":"<pre><code>paz.utils.logger.write_weights(model, directory, name=None)\n</code></pre> <p>Writes Keras weights in memory.</p> <p>Arguments:</p> <ul> <li>model: Keras model.</li> <li>directory: String. Directory name.</li> <li>name: String or <code>None</code>. Weights filename.</li> </ul>"}]}